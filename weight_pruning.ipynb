{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 17:30:21.492286 140008901007168 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:9: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0819 17:30:21.493005 140008901007168 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:13: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "import pdb\n",
    "import re\n",
    "from ddm.run_split import SplitMnistGenerator\n",
    "from ddm.alg.cla_models_multihead import MFVI_IBP_NN, Vanilla_NN\n",
    "from ddm.alg.utils import get_scores, concatenate_results\n",
    "from ddm.alg.vcl import run_vcl\n",
    "from copy import deepcopy\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 10:52:12.129490 140686037821248 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0817 10:52:12.132290 140686037821248 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:167: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0817 10:52:12.188359 140686037821248 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:61: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0817 10:52:12.303390 140686037821248 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:65: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.049326868\n",
      "Epoch: 0006 cost= 0.000994026\n",
      "Epoch: 0011 cost= 0.000234237\n",
      "Epoch: 0016 cost= 0.000100588\n",
      "Epoch: 0021 cost= 0.000055076\n",
      "Epoch: 0026 cost= 0.000033257\n",
      "Epoch: 0031 cost= 0.000021853\n",
      "Epoch: 0036 cost= 0.000014344\n",
      "Epoch: 0041 cost= 0.000010315\n",
      "Epoch: 0046 cost= 0.000007250\n",
      "Epoch: 0051 cost= 0.000005240\n",
      "Epoch: 0056 cost= 0.000003922\n",
      "Epoch: 0061 cost= 0.000002916\n",
      "Epoch: 0066 cost= 0.000002214\n",
      "Epoch: 0071 cost= 0.000001703\n",
      "Epoch: 0076 cost= 0.000001282\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9d3b4377aeb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mml_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVanilla_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mmf_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmf_variances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, task_idx, no_epochs, batch_size, display_epoch)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mperm_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mcur_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mcur_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 100\n",
    "alpha0 = 1.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "# Run vanilla VCL\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = True\n",
    "data_gen = SplitMnistGenerator(val)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, temp=tau0, temp_prior=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "    acc = get_scores(mf_model, x_valsets, y_valsets, single_head)\n",
    "    ibp_acc = concatenate_results(acc, ibp_acc)\n",
    "    \n",
    "    mf_model.close_session()\n",
    "    \n",
    "ibp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Vanilla VCL\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "hidden_size = [10]\n",
    "coreset_size = 0\n",
    "\n",
    "data_gen = SplitMnistGenerator()\n",
    "vcl_result = run_vcl(hidden_size, no_epochs, data_gen, \n",
    "                              lambda a: a, coreset_size, batch_size, single_head)\n",
    "print(vcl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run IBP VCL\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(12)\n",
    "# np.random.seed(1)\n",
    "# coreset_size = 0\n",
    "\n",
    "# hidden_size = [50]\n",
    "# batch_size = 128\n",
    "# no_epochs = 100\n",
    "# alpha0 = 5.0\n",
    "# tau0=0.1 # initial temperature\n",
    "# temp_prior=1.0\n",
    "# ANNEAL_RATE=0.000\n",
    "# MIN_TEMP=0.1\n",
    "# single_head=False\n",
    "\n",
    "# # data_gen = SplitMnistGenerator()\n",
    "# # vcl_ibp_result = vcl.run_vcl_ibp(hidden_size=hidden_size, no_epochs=no_epochs, data_gen=data_gen,\n",
    "# #                                   batch_size=batch_size, single_head=single_head, alpha0=alpha0,\n",
    "# #                                   learning_rate=0.01, temp_prior=temp_prior, no_pred_samples=100,\n",
    "# #                                   tau0=tau0, tau_anneal_rate=ANNEAL_RATE, tau_min=MIN_TEMP)\n",
    "# # print(vcl_ibp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ibp_acc = np.nanmean(ibp_acc, 1)\n",
    "_vcl_result = np.nanmean(vcl_result, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vcl_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = plt.gca()\n",
    "plt.plot(np.arange(len(_ibp_acc))+1, _ibp_acc, label='VCL + IBP', marker='o')\n",
    "plt.plot(np.arange(len(_vcl_result))+1, _vcl_result, label='VCL', marker='o')\n",
    "ax.set_xticks(range(1, len(_ibp_acc)+1))\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('\\# tasks')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFVI_IBP_NN_prune(MFVI_IBP_NN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, training_size,\n",
    "                 no_train_samples=10, no_pred_samples=100, prev_means=None, prev_log_variances=None,\n",
    "                 prev_betas=None, learning_rate=0.001,\n",
    "                 prior_mean=0, prior_var=1, alpha0=5., beta0=1., lambda_1=1., lambda_2=1.,\n",
    "                 tensorboard_dir='logs', name='ibp_wp', min_temp=0.5, tb_logging=True,\n",
    "                output_tb_gradients=True):\n",
    "\n",
    "        super(MFVI_IBP_NN_prune, self).__init__(input_size, hidden_size, output_size, training_size,\n",
    "                 no_train_samples, no_pred_samples, prev_means, prev_log_variances,\n",
    "                 prev_betas, learning_rate,\n",
    "                 prior_mean, prior_var, alpha0, beta0, lambda_1, lambda_2,\n",
    "                 tensorboard_dir, name, min_temp, tb_logging, output_tb_gradients=True)\n",
    "\n",
    "\n",
    "    def prune_weights(self, X_test, Y_test, task_id):\n",
    "        \"\"\" Performs weight pruning\n",
    "        Args:\n",
    "            X_test: numpy array\n",
    "            Y_test: numpy array\n",
    "            task_id: int\n",
    "        :return: cutoffs, accs via naive pruning, accs via snr pruning,\n",
    "        weight values, sigma values of network\n",
    "        \"\"\"\n",
    "\n",
    "        def reset_weights(pr_mus, pr_sigmas, _mus, _sigmas):\n",
    "            \"\"\" Reset weights of graph to original values\n",
    "            Args:\n",
    "                pr_mus: list of tf variables which have been pruned\n",
    "                pr_sigmas: list of tf variables which have been pruned\n",
    "                _mus: list of cached mus in numpy\n",
    "                _sigmas: list of cached sigmas in numpy\n",
    "            \"\"\"\n",
    "\n",
    "            for v, _v in zip(pr_mus, _mus):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "            for v, _v in zip(pr_sigmas, _sigmas):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "        def pruning(remove_pct, weightvalues, sigmavalues,\n",
    "                    weights, sigmas, uncert_pruning=True):\n",
    "            \"\"\" Performs weight pruning experiment\n",
    "            Args:\n",
    "                weightvalues: np array of weights\n",
    "                sigmavalues: np array of sigmas\n",
    "                weights: list of tf weight variable\n",
    "                sigmas: list of tf sigma variables\n",
    "                uncert_pruning: bool pruning by snr\n",
    "            \"\"\"\n",
    "            if uncert_pruning:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues) / sigmavalues)\n",
    "                \n",
    "            else:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues))\n",
    "            cutoff = sorted_STN[int(remove_pct * len(sorted_STN))]\n",
    "            \n",
    "            # Weights, biases and head weights\n",
    "            for v, s in zip(weights, sigmas):\n",
    "                if uncert_pruning:\n",
    "                    snr = tf.abs(v) / tf.exp(0.5*s)\n",
    "                    mask = tf.greater_equal(snr, cutoff)\n",
    "                else:\n",
    "                    mask = tf.greater_equal(tf.abs(v), cutoff)\n",
    "                self.sess.run(tf.assign(v, tf.multiply(v, tf.cast(mask, v.dtype))))\n",
    "                self.sess.run(tf.assign(s, tf.multiply(s, tf.cast(mask, s.dtype))))  # also apply zero std to weight!!!\n",
    "                \n",
    "            \n",
    "            accs = []\n",
    "            for _ in range(10):\n",
    "                accs.append(self.sess.run(self.acc, {self.x: X_test,\n",
    "                                                     self.y: Y_test,\n",
    "                                                     self.task_idx: task_id,\n",
    "                                                     self.temp: self.lambda_1,\n",
    "                                                     self.training: False}))\n",
    "            print(\"%.2f, %s\" % (np.sum(sorted_STN < cutoff) / len(sorted_STN), np.mean(accs)))\n",
    "            return np.mean(accs)\n",
    "\n",
    "        # get weights\n",
    "        weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n",
    "        \n",
    "        # Get weights from network\n",
    "        # TODO get head weights and biases\n",
    "        mus_w = []\n",
    "        mus_b = []\n",
    "        sigmas_w = []\n",
    "        sigmas_b = []\n",
    "        mus_h = [] # weights and biases\n",
    "        sigmas_h = [] # weights and biases\n",
    "        for v in weights:\n",
    "            if re.match(\"^([w])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_w.append(v)\n",
    "            elif re.match(\"^([w])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_w.append(v)\n",
    "            elif re.match(\"^([b])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_b.append(v)\n",
    "            elif re.match(\"^([b])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_b.append(v)\n",
    "            elif re.match(\"^([wb])(_mu_h_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_h.append(v)\n",
    "            elif re.match(\"^([wb])(_sigma_h_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_h.append(v)\n",
    "            else:\n",
    "                print(\"Un-matched: {}\".format(v.name))\n",
    "                \n",
    "        # Get mask\n",
    "        # Need to apply mask to weights and biases for pruning\n",
    "        # Do not apply to head weights though!\n",
    "        # Zs \\in [x_test.size, dout], just need one sample of Z!\n",
    "        Zs = self.sess.run(self.Z, {self.x: X_test,\n",
    "                                    self.y: Y_test,\n",
    "                                    self.task_idx: task_id,\n",
    "                                    self.temp: self.lambda_1,\n",
    "                                    self.training: False})[0] # z mask for each layer in a list, each Z \\in dout\n",
    "\n",
    "        # cache network weights of resetting the network\n",
    "        _mus_w = [self.sess.run(w) for w in mus_w]\n",
    "        _sigmas_w = [self.sess.run(w) for w in sigmas_w]\n",
    "        _mus_b = [self.sess.run(w) for w in mus_b]\n",
    "        _sigmas_b = [self.sess.run(w) for w in sigmas_b]\n",
    "        _mus_h = [self.sess.run(w) for w in mus_h]\n",
    "        _sigmas_h = [self.sess.run(w) for w in sigmas_h]\n",
    "\n",
    "        # flatten values for cut-off finding\n",
    "        # Tile Zs [dout] --> [din, dout]\n",
    "        dins = [x.shape[0] for x in _mus_w]\n",
    "        Z_tiled = []\n",
    "        t_Z_tiled = []\n",
    "        for i in range(len(dins)):\n",
    "            Z_tiled.append(np.tile(Zs[i][0, :], (dins[i], 1))) \n",
    "            t_Z_tiled.append(tf.tile(tf.squeeze(self.Z[i], axis=0), [dins[i], 1]))\n",
    "        \n",
    "        # multiply Zs with \n",
    "        f_xx = []\n",
    "        f_yy = []\n",
    "        xx = []\n",
    "        yy = []\n",
    "        for i in range(len(mus_w)):\n",
    "            f_xx.append(np.multiply(self.sess.run(mus_w[i]), Z_tiled[i]).flatten())\n",
    "            f_yy.append(np.multiply(self.sess.run(mus_b[i]), Zs[i][0,:]).flatten())\n",
    "            xx.append(tf.multiply(mus_w[i], t_Z_tiled[i]))\n",
    "            yy.append(tf.multiply(mus_b[i], tf.squeeze(self.Z[i], axis=0)))\n",
    "        \n",
    "        print(yy)\n",
    "        \n",
    "        # flatten head network weights\n",
    "        f_mus_h = []\n",
    "        f_sigmas_h = []\n",
    "        for i in range(len(mus_h)):\n",
    "            f_mus_h.append(self.sess.run(mus_h[i]).flatten())\n",
    "            f_sigmas_h.append(self.sess.run(sigmas_h[i]).flatten())\n",
    "        \n",
    "        weightvalues_ibp = np.hstack(np.array(f_xx + f_yy + f_mus_h))\n",
    "        weightvalues = np.hstack(np.array([self.sess.run(w).flatten() for w in mus_w + mus_b + mus_h]))\n",
    "        sigmavalues_tr = np.hstack(np.array([self.sess.run(tf.exp(0.5*s)).flatten() for s in sigmas_w + sigmas_b + sigmas_h]))\n",
    "    \n",
    "        # pruning without IBP\n",
    "        xs = np.append(0.05 * np.array(range(20)), np.array([0.98, 0.99, 0.999]))\n",
    "        ya = []\n",
    "        for pct in xs:\n",
    "            ya.append(pruning(pct, weightvalues, sigmavalues_tr,\n",
    "                              mus_w + mus_b + mus_h, sigmas_w + sigmas_b + sigmas_h, \n",
    "                              uncert_pruning=False))\n",
    "\n",
    "        # reset etc.\n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        yb = []\n",
    "        for pct in xs:\n",
    "            yb.append(pruning(pct, weightvalues, sigmavalues_tr,\n",
    "                              mus_w + mus_b + mus_h, sigmas_w + sigmas_b + sigmas_h,\n",
    "                              uncert_pruning=True))\n",
    "            \n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        \n",
    "        # pruning with IBP\n",
    "        ya_ibp = []\n",
    "        for pct in xs:\n",
    "            ya.append(pruning(pct, weightvalues_ibp, sigmavalues_tr,\n",
    "                              xx + yy + mus_h, sigmas_w + sigmas_b + sigmas_h, \n",
    "                              uncert_pruning=False))\n",
    "\n",
    "        # reset etc.\n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        yb_ibp = []\n",
    "        for pct in xs:\n",
    "            yb.append(pruning(pct, weightvalues_ibp, sigmavalues_tr,\n",
    "                              xx + yy + mus_h, sigmas_w + sigmas_b + sigmas_h,\n",
    "                              uncert_pruning=True))\n",
    "            \n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        \n",
    "        return xs, ya, yb, ya_ibp, yb_ibp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.053854497\n",
      "Epoch: 0006 cost= 0.000837374\n",
      "Epoch: 0011 cost= 0.000233437\n",
      "Epoch: 0016 cost= 0.000093309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-de6c5997f12e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mml_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVanilla_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mmf_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmf_variances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, task_idx, no_epochs, batch_size, display_epoch)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 _, c = sess.run(\n\u001b[1;32m     95\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     feed_dict={self.x: batch_x, self.y: batch_y, self.task_idx: task_idx})\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;31m# Compute average loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 200\n",
    "alpha0 = 1.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = False\n",
    "data_gen = SplitMnistGenerator(val, num_tasks=1)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN_prune(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, lambda_1=tau0, lambda_2=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    \n",
    "    xs, ya, yb, ya_ibp, yb_ibp  = mf_model.prune_weights(x_test, y_test, head)\n",
    "    \n",
    "    mf_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ya, label='$|\\mu|$')\n",
    "plt.plot(xs, yb, label='$ \\\\frac{|\\mu|}{\\sigma}$')\n",
    "plt.xlabel('cut-off')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass CLF Weight Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistGenerator():\n",
    "    def __init__(self, max_iter=10):\n",
    "        with gzip.open('ddm/data/mnist.pkl.gz', 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "        self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
    "        self.Y_train = np.hstack((train_set[1], valid_set[1]))\n",
    "        self.X_test = test_set[0]\n",
    "        self.Y_test = test_set[1]\n",
    "        self.max_iter = max_iter\n",
    "        self.cur_iter = 0\n",
    "\n",
    "    def get_dims(self):\n",
    "        # Get data input and output dimensions\n",
    "        return self.X_train.shape[1], 10\n",
    "\n",
    "    def task(self):\n",
    "        # Retrieve train data\n",
    "        x_train = deepcopy(self.X_train)\n",
    "        y_train = np.eye(10)[self.Y_train]\n",
    "\n",
    "        # Retrieve test data\n",
    "        x_test = deepcopy(self.X_test)\n",
    "        y_test = np.eye(10)[self.Y_test]\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.426942452\n",
      "Epoch: 0006 cost= 0.075146306\n",
      "Epoch: 0011 cost= 0.037155623\n",
      "Epoch: 0016 cost= 0.018815619\n",
      "Epoch: 0021 cost= 0.010366963\n",
      "Epoch: 0026 cost= 0.005406210\n",
      "Epoch: 0031 cost= 0.003250390\n",
      "Epoch: 0036 cost= 0.002707957\n",
      "Epoch: 0041 cost= 0.000564609\n",
      "Epoch: 0046 cost= 0.000413428\n",
      "Epoch: 0051 cost= 0.008804641\n",
      "Epoch: 0056 cost= 0.000232297\n",
      "Epoch: 0061 cost= 0.005714477\n",
      "Epoch: 0066 cost= 0.000165049\n",
      "Epoch: 0071 cost= 0.000112707\n",
      "Epoch: 0076 cost= 0.010227541\n",
      "Epoch: 0081 cost= 0.000093582\n",
      "Epoch: 0086 cost= 0.000055837\n",
      "Epoch: 0091 cost= 0.008869045\n",
      "Epoch: 0096 cost= 0.000070020\n",
      "Optimization Finished!\n",
      "_Z: (1, ?, 100)\n",
      "Un-matched: w_0:0\n",
      "Un-matched: b_0:0\n",
      "Un-matched: w_h_0:0\n",
      "Un-matched: b_h_0:0\n",
      "Un-matched: beta_a_0:0\n",
      "Un-matched: beta_b_0:0\n",
      "[<tf.Tensor 'Mul_80:0' shape=(?, 100) dtype=float32>]\n",
      "0.00, 0.9226837\n",
      "0.05, 0.92364997\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 500\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "data_gen = MnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "task_id=0\n",
    "    \n",
    "tf.reset_default_graph()  \n",
    "x_train, y_train, x_test, y_test = data_gen.task()\n",
    "x_testsets.append(x_test)\n",
    "y_testsets.append(y_test)\n",
    "\n",
    "# Set the readout head to train\n",
    "head = 0 if single_head else task_id\n",
    "bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "# Train network with maximum likelihood to initialize first model\n",
    "if task_id == 0:\n",
    "    ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "    ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "    mf_weights = ml_model.get_weights()\n",
    "    mf_variances = None\n",
    "    mf_betas = None\n",
    "    ml_model.close_session()\n",
    "\n",
    "# Train on non-coreset data\n",
    "mf_model = MFVI_IBP_NN_prune(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                       prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=5.0, beta0=1.0,\n",
    "                       learning_rate=0.0001, lambda_1=1.0, lambda_2=1.0, no_pred_samples=100,\n",
    "                       name='ibp_wp_mnist')\n",
    "\n",
    "mf_model.restore(os.path.join(\"logs\", \"graph_{}_task{}\".format('ibp_wp_mnist', 0)))\n",
    "# mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "#                anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "\n",
    "xs, ya, yb, ya_ibp, yb_ibp  = mf_model.prune_weights(x_test, y_test, head)\n",
    "\n",
    "mf_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ya, label='$|\\mu|$')\n",
    "plt.plot(xs, yb, label='snr')\n",
    "plt.plot(xs, ya_ibp, label='ibp $|\\mu|$')\n",
    "plt.plot(xs, yb_ibp, label='ibp snr')\n",
    "plt.xlabel('cut-off')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
