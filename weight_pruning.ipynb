{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 19:42:21.077231 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:9: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0821 19:42:21.078042 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:13: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "import pdb\n",
    "import re\n",
    "from ddm.run_split import SplitMnistGenerator\n",
    "from ddm.alg.cla_models_multihead import MFVI_IBP_NN, MFVI_NN, Vanilla_NN\n",
    "from ddm.alg.utils import get_scores, concatenate_results\n",
    "from ddm.alg.vcl import run_vcl\n",
    "from copy import deepcopy\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFVI_IBP_NN_prune(MFVI_IBP_NN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, training_size, num_ibp_samples=10,\n",
    "                 no_train_samples=10, no_pred_samples=100, prev_means=None, prev_log_variances=None,\n",
    "                 prev_betas=None, learning_rate=0.001,\n",
    "                 prior_mean=0, prior_var=1, alpha0=5., beta0=1., lambda_1=1., lambda_2=1.,\n",
    "                 tensorboard_dir='logs', name='ibp_wp', min_temp=0.5, tb_logging=True,\n",
    "                output_tb_gradients=True):\n",
    "\n",
    "        super(MFVI_IBP_NN_prune, self).__init__(input_size, hidden_size, output_size, training_size,\n",
    "                 no_train_samples, no_pred_samples, num_ibp_samples, prev_means, prev_log_variances,\n",
    "                 prev_betas, learning_rate,\n",
    "                 prior_mean, prior_var, alpha0, beta0, lambda_1, lambda_2,\n",
    "                 tensorboard_dir, name, min_temp, tb_logging, output_tb_gradients=True)\n",
    "\n",
    "\n",
    "    def prune_weights(self, X_test, Y_test, task_id):\n",
    "        \"\"\" Performs weight pruning.\n",
    "        \n",
    "        Z is at a data level doesn't make sense to introduce this intot he mask over weights which get zeroed \n",
    "        out. Simlpy running the accuracy over the graph will entail that Z is incorporated into the \n",
    "        matrix math for the prediction calculations.\n",
    "        \n",
    "        Args:\n",
    "            X_test: numpy array\n",
    "            Y_test: numpy array\n",
    "            task_id: int\n",
    "        :return: cutoffs, accs via naive pruning, accs via snr pruning,\n",
    "        weight values, sigma values of network\n",
    "        \"\"\"\n",
    "\n",
    "        def reset_weights(pr_mus, pr_sigmas, _mus, _sigmas):\n",
    "            \"\"\" Reset weights of graph to original values\n",
    "            Args:\n",
    "                pr_mus: list of tf variables which have been pruned\n",
    "                pr_sigmas: list of tf variables which have been pruned\n",
    "                _mus: list of cached mus in numpy\n",
    "                _sigmas: list of cached sigmas in numpy\n",
    "            \"\"\"\n",
    "\n",
    "            for v, _v in zip(pr_mus, _mus):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "            for v, _v in zip(pr_sigmas, _sigmas):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "        def pruning(remove_pct, weightvalues, sigmavalues,\n",
    "                    weights, sigmas, uncert_pruning=True):\n",
    "            \"\"\" Performs weight pruning experiment\n",
    "            Args:\n",
    "                weightvalues: np array of weights\n",
    "                sigmavalues: np array of sigmas\n",
    "                weights: list of tf weight variable\n",
    "                new_weights: list of new tf weight variables which wil\n",
    "                sigmas: list of tf sigma variables\n",
    "                uncert_pruning: bool pruning by snr\n",
    "            \"\"\"\n",
    "            if uncert_pruning:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues) / sigmavalues)\n",
    "                \n",
    "            else:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues))\n",
    "            cutoff = sorted_STN[int(remove_pct * len(sorted_STN))]\n",
    "            \n",
    "            # Weights, biases and head weights\n",
    "            for v, s in zip(weights, sigmas):\n",
    "                if uncert_pruning:\n",
    "                    snr = tf.abs(v) / tf.exp(0.5*s)\n",
    "                    mask = tf.greater_equal(snr, cutoff)\n",
    "                else:\n",
    "                    mask = tf.greater_equal(tf.abs(v), cutoff)\n",
    "                self.sess.run(tf.assign(v, tf.multiply(v, tf.cast(mask, v.dtype))))\n",
    "                #self.sess.run(tf.assign(s, np.multiply(self.sess.run(s), mask)))  # also apply zero std to weight!!!\n",
    "                \n",
    "            accs = []\n",
    "            for _ in range(10):\n",
    "                accs.append(self.sess.run(self.acc, {self.x: X_test,\n",
    "                                                     self.y: Y_test,\n",
    "                                                     self.task_idx: task_id,\n",
    "                                                     self.temp: self.lambda_1,\n",
    "                                                     self.training: False}))\n",
    "            print(\"%.2f, %s\" % (np.sum(sorted_STN < cutoff) / len(sorted_STN), np.mean(accs)))\n",
    "            return np.mean(accs)\n",
    "\n",
    "        # get weights\n",
    "        weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n",
    "        \n",
    "        # Get weights from network\n",
    "        # TODO get head weights and biases\n",
    "        mus_w = []\n",
    "        mus_b = []\n",
    "        sigmas_w = []\n",
    "        sigmas_b = []\n",
    "        mus_h = [] # weights and biases\n",
    "        sigmas_h = [] # weights and biases\n",
    "        for v in weights:\n",
    "            if re.match(\"^([w])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_w.append(v)\n",
    "            elif re.match(\"^([w])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_w.append(v)\n",
    "            elif re.match(\"^([b])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_b.append(v)\n",
    "            elif re.match(\"^([b])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_b.append(v)\n",
    "            elif re.match(\"^([wb])(_mu_h_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_h.append(v)\n",
    "            elif re.match(\"^([wb])(_sigma_h_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_h.append(v)\n",
    "            else:\n",
    "                print(\"Un-matched: {}\".format(v.name))\n",
    "                \n",
    "        acc = self.sess.run(self.acc, {self.x: X_test,\n",
    "                                    self.y: Y_test,\n",
    "                                    self.task_idx: task_id,\n",
    "                                    self.temp: self.lambda_1,\n",
    "                                    self.training: False}) # z mask for each layer in a list, each Z \\in dout\n",
    "        print(\"test acc: {}\".format(acc))\n",
    "        # cache network weights of resetting the network\n",
    "        _mus_w = [self.sess.run(w) for w in mus_w]\n",
    "        _sigmas_w = [self.sess.run(w) for w in sigmas_w]\n",
    "        _mus_b = [self.sess.run(w) for w in mus_b]\n",
    "        _sigmas_b = [self.sess.run(w) for w in sigmas_b]\n",
    "        _mus_h = [self.sess.run(w) for w in mus_h]\n",
    "        _sigmas_h = [self.sess.run(w) for w in sigmas_h]\n",
    "        \n",
    "        weightvalues = np.hstack(np.array([self.sess.run(w).flatten() for w in mus_w + mus_b + mus_h]))\n",
    "        sigmavalues = np.hstack(np.array([self.sess.run(tf.exp(0.5*s)).flatten() for s in sigmas_w + sigmas_b + sigmas_h]))\n",
    "    \n",
    "        xs = np.append(0.05 * np.array(range(20)), np.array([0.98, 0.99, 0.999]))\n",
    "        ya_ibp = []\n",
    "        for pct in xs:\n",
    "            ya_ibp.append(pruning(pct, weightvalues, sigmavalues, mus_w + mus_b + mus_h,\n",
    "                              sigmas_w + sigmas_b + sigmas_h, uncert_pruning=False))\n",
    "\n",
    "        # reset etc.\n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        yb_ibp = []\n",
    "        for pct in xs:\n",
    "            yb_ibp.append(pruning(pct, weightvalues, sigmavalues, mus_w + mus_b + mus_h,\n",
    "                                  sigmas_w + sigmas_b + sigmas_h, uncert_pruning=True))\n",
    "            \n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        \n",
    "        #return xs, ya, yb, ya_ibp, yb_ibp\n",
    "        return xs, ya_ibp, yb_ibp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass CLF Weight Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistGenerator():\n",
    "    def __init__(self, max_iter=10):\n",
    "        with gzip.open('ddm/data/mnist.pkl.gz', 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "        self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
    "        self.Y_train = np.hstack((train_set[1], valid_set[1]))\n",
    "        self.X_test = test_set[0]\n",
    "        self.Y_test = test_set[1]\n",
    "        self.max_iter = max_iter\n",
    "        self.cur_iter = 0\n",
    "\n",
    "    def get_dims(self):\n",
    "        # Get data input and output dimensions\n",
    "        return self.X_train.shape[1], 10\n",
    "\n",
    "    def task(self):\n",
    "        # Retrieve train data\n",
    "        x_train = deepcopy(self.X_train)\n",
    "        y_train = np.eye(10)[self.Y_train]\n",
    "\n",
    "        # Retrieve test data\n",
    "        x_test = deepcopy(self.X_test)\n",
    "        y_test = np.eye(10)[self.Y_test]\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 19:42:22.788784 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0821 19:42:22.792086 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:177: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0821 19:42:22.862715 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:61: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0821 19:42:22.997229 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:65: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.417946803\n",
      "Epoch: 0006 cost= 0.075792010\n",
      "Epoch: 0011 cost= 0.037298302\n",
      "Epoch: 0016 cost= 0.019005517\n",
      "Epoch: 0021 cost= 0.009970345\n",
      "Epoch: 0026 cost= 0.005601576\n",
      "Epoch: 0031 cost= 0.001965217\n",
      "Epoch: 0036 cost= 0.000851523\n",
      "Epoch: 0041 cost= 0.000607356\n",
      "Epoch: 0046 cost= 0.004467624\n",
      "Epoch: 0051 cost= 0.000259006\n",
      "Epoch: 0056 cost= 0.004487546\n",
      "Epoch: 0061 cost= 0.000172686\n",
      "Epoch: 0066 cost= 0.000112348\n",
      "Epoch: 0071 cost= 0.000389601\n",
      "Epoch: 0076 cost= 0.000091579\n",
      "Epoch: 0081 cost= 0.000058314\n",
      "Epoch: 0086 cost= 0.001865642\n",
      "Epoch: 0091 cost= 0.000073635\n",
      "Epoch: 0096 cost= 0.000040658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 19:43:13.055495 140264918792000 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:575: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 19:43:13.313542 140264918792000 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:528: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 19:43:13.779579 140264918792000 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0821 19:43:14.644345 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:555: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0821 19:43:14.731306 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:535: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0821 19:43:14.762229 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:670: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0821 19:43:14.831711 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:687: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0821 19:43:14.836413 140264918792000 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:694: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0821 19:43:14.946965 140264918792000 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Z: (1, ?, 100)\n",
      "Un-matched: w_0:0\n",
      "Un-matched: b_0:0\n",
      "Un-matched: w_h_0:0\n",
      "Un-matched: b_h_0:0\n",
      "Un-matched: beta_a_0:0\n",
      "Un-matched: beta_b_0:0\n",
      "test acc: 0.9486740231513977\n",
      "0.00, 0.9483735\n",
      "0.05, 0.9485658\n",
      "0.10, 0.94854087\n",
      "0.15, 0.9485385\n",
      "0.20, 0.94814193\n",
      "0.25, 0.94832736\n",
      "0.30, 0.948668\n",
      "0.35, 0.9484043\n",
      "0.40, 0.9482589\n",
      "0.45, 0.9484416\n",
      "0.50, 0.94873846\n",
      "0.55, 0.948386\n",
      "0.60, 0.948174\n",
      "0.65, 0.9484037\n",
      "0.70, 0.9488484\n",
      "0.75, 0.9484024\n",
      "0.80, 0.9483946\n",
      "0.85, 0.947657\n",
      "0.90, 0.93643504\n",
      "0.95, 0.8419142\n",
      "0.98, 0.47161478\n",
      "0.99, 0.2572486\n",
      "1.00, 0.1097814\n",
      "0.00, 0.9486372\n",
      "0.05, 0.9484426\n",
      "0.10, 0.94795275\n",
      "0.15, 0.94825983\n",
      "0.20, 0.9483253\n",
      "0.25, 0.9483965\n",
      "0.30, 0.94781655\n",
      "0.35, 0.94860137\n",
      "0.40, 0.94833195\n",
      "0.45, 0.94822294\n",
      "0.50, 0.9480139\n",
      "0.55, 0.9486286\n",
      "0.60, 0.94865406\n",
      "0.65, 0.9486963\n",
      "0.70, 0.94841385\n",
      "0.75, 0.94820136\n",
      "0.80, 0.9483887\n",
      "0.85, 0.9480044\n",
      "0.90, 0.9463827\n",
      "0.95, 0.928347\n",
      "0.98, 0.75886744\n",
      "0.99, 0.4000272\n",
      "1.00, 0.0882204\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 500\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "data_gen = MnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "task_id=0\n",
    "    \n",
    "tf.reset_default_graph()  \n",
    "x_train, y_train, x_test, y_test = data_gen.task()\n",
    "x_testsets.append(x_test)\n",
    "y_testsets.append(y_test)\n",
    "\n",
    "# Set the readout head to train\n",
    "head = 0 if single_head else task_id\n",
    "bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "# Train network with maximum likelihood to initialize first model\n",
    "if task_id == 0:\n",
    "    ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "    ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "    mf_weights = ml_model.get_weights()\n",
    "    mf_variances = None\n",
    "    mf_betas = None\n",
    "    ml_model.close_session()\n",
    "\n",
    "# Train on non-coreset data\n",
    "mf_model = MFVI_IBP_NN_prune(in_dim, hidden_size, out_dim, x_train.shape[0], \n",
    "                             num_ibp_samples=10, prev_means=mf_weights, \n",
    "                       prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=5.0, beta0=1.0,\n",
    "                       learning_rate=0.0001, lambda_1=1.0, lambda_2=1.0, no_pred_samples=100,\n",
    "                       name='ibp_wp_mnist')\n",
    "\n",
    "mf_model.restore(os.path.join(\"logs\", \"graph_{}_task{}\".format('ibp_wp_mnist', 0)))\n",
    "# mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "#               anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "\n",
    "xs, ya_ibp, yb_ibp  = mf_model.prune_weights(x_test, y_test, head)\n",
    "\n",
    "mf_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['cmsy10'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n",
      "/home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['cmr10'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n",
      "/home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['cmtt10'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n",
      "/home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['cmmi10'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n",
      "/home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['cmb10'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n",
      "/home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['cmss10'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n",
      "/home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['cmex10'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n",
      "/home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['DejaVu Sans Display'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(xs, ya, label='$|\\mu|$')\n",
    "#plt.plot(xs, yb, label='snr')\n",
    "plt.plot(xs, ya_ibp, label='ibp $|\\mu|$')\n",
    "plt.plot(xs, yb_ibp, label='ibp snr')\n",
    "plt.xlabel('cut-off')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No IBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFVI_NN_prune(MFVI_NN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, training_size, \n",
    "        no_train_samples=10, no_pred_samples=100, prev_means=None, prev_log_variances=None, learning_rate=0.001, \n",
    "        prior_mean=0, prior_var=1):\n",
    "\n",
    "        super(MFVI_NN_prune, self).__init__(input_size, hidden_size, output_size, training_size, \n",
    "        no_train_samples, no_pred_samples, prev_means, prev_log_variances, learning_rate, \n",
    "        prior_mean, prior_var)\n",
    "\n",
    "\n",
    "    def prune_weights(self, X_test, Y_test, task_id):\n",
    "        \"\"\" Performs weight pruning.\n",
    "        \n",
    "        Z is at a data level doesn't make sense to introduce this intot he mask over weights which get zeroed \n",
    "        out. Simlpy running the accuracy over the graph will entail that Z is incorporated into the \n",
    "        matrix math for the prediction calculations.\n",
    "        \n",
    "        Args:\n",
    "            X_test: numpy array\n",
    "            Y_test: numpy array\n",
    "            task_id: int\n",
    "        :return: cutoffs, accs via naive pruning, accs via snr pruning,\n",
    "        weight values, sigma values of network\n",
    "        \"\"\"\n",
    "\n",
    "        def reset_weights(pr_mus, pr_sigmas, _mus, _sigmas):\n",
    "            \"\"\" Reset weights of graph to original values\n",
    "            Args:\n",
    "                pr_mus: list of tf variables which have been pruned\n",
    "                pr_sigmas: list of tf variables which have been pruned\n",
    "                _mus: list of cached mus in numpy\n",
    "                _sigmas: list of cached sigmas in numpy\n",
    "            \"\"\"\n",
    "\n",
    "            for v, _v in zip(pr_mus, _mus):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "            for v, _v in zip(pr_sigmas, _sigmas):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "        def pruning(remove_pct, weightvalues, sigmavalues,\n",
    "                    weights, sigmas, uncert_pruning=True):\n",
    "            \"\"\" Performs weight pruning experiment\n",
    "            Args:\n",
    "                weightvalues: np array of weights\n",
    "                sigmavalues: np array of sigmas\n",
    "                weights: list of tf weight variable\n",
    "                sigmas: list of tf sigma variables\n",
    "                uncert_pruning: bool pruning by snr\n",
    "            \"\"\"\n",
    "            if uncert_pruning:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues) / sigmavalues)\n",
    "                \n",
    "            else:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues))\n",
    "            cutoff = sorted_STN[int(remove_pct * len(sorted_STN))]\n",
    "            \n",
    "            # Weights, biases and head weights\n",
    "            for v, s in zip(weights, sigmas):\n",
    "                if uncert_pruning:\n",
    "                    snr = tf.abs(v) / tf.exp(0.5*s)\n",
    "                    mask = tf.greater_equal(snr, cutoff)\n",
    "                else:\n",
    "                    mask = tf.greater_equal(tf.abs(v), cutoff)\n",
    "                self.sess.run(tf.assign(v, tf.multiply(v, tf.cast(mask, v.dtype))))\n",
    "                \n",
    "            accs = []\n",
    "            for _ in range(10):\n",
    "                accs.append(self.sess.run(self.acc, {self.x: X_test,\n",
    "                                                     self.y: Y_test,\n",
    "                                                     self.task_idx: task_id}))\n",
    "            print(\"%.2f, %s\" % (np.sum(sorted_STN < cutoff) / len(sorted_STN), np.mean(accs)))\n",
    "            return np.mean(accs)\n",
    "\n",
    "        # get weights\n",
    "        weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n",
    "        \n",
    "        # Get weights from network\n",
    "        mus_w = []\n",
    "        mus_b = []\n",
    "        sigmas_w = []\n",
    "        sigmas_b = []\n",
    "        mus_h = [] # weights and biases\n",
    "        sigmas_h = [] # weights and biases\n",
    "        for v in weights:\n",
    "            if re.match(\"^([w])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_w.append(v)\n",
    "            elif re.match(\"^([w])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_w.append(v)\n",
    "            elif re.match(\"^([b])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_b.append(v)\n",
    "            elif re.match(\"^([b])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_b.append(v)\n",
    "            elif re.match(\"^([wb])(_mu_h_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_h.append(v)\n",
    "            elif re.match(\"^([wb])(_sigma_h_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_h.append(v)\n",
    "            else:\n",
    "                print(\"Un-matched: {}\".format(v.name))\n",
    "                \n",
    "        # cache network weights of resetting the network\n",
    "        _mus_w = [self.sess.run(w) for w in mus_w]\n",
    "        _sigmas_w = [self.sess.run(w) for w in sigmas_w]\n",
    "        _mus_b = [self.sess.run(w) for w in mus_b]\n",
    "        _sigmas_b = [self.sess.run(w) for w in sigmas_b]\n",
    "        _mus_h = [self.sess.run(w) for w in mus_h]\n",
    "        _sigmas_h = [self.sess.run(w) for w in sigmas_h]\n",
    "        \n",
    "        weightvalues = np.hstack(np.array([self.sess.run(w).flatten() for w in mus_w + mus_b + mus_h]))\n",
    "        sigmavalues = np.hstack(np.array([self.sess.run(tf.exp(0.5*s)).flatten() for s in sigmas_w + sigmas_b + sigmas_h]))\n",
    "    \n",
    "        xs = np.append(0.05 * np.array(range(20)), np.array([0.98, 0.99, 0.999]))\n",
    "        # pruning\n",
    "        ya = []\n",
    "        for pct in xs:\n",
    "            ya_ibp.append(pruning(pct, weightvalues, sigmavalues, mus_w + mus_b + mus_h,\n",
    "                              sigmas_w + sigmas_b + sigmas_h, uncert_pruning=False))\n",
    "\n",
    "        # reset etc.\n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        yb = []\n",
    "        for pct in xs:\n",
    "            yb_ibp.append(pruning(pct, weightvalues, sigmavalues, mus_w + mus_b + mus_h,\n",
    "                                  sigmas_w + sigmas_b + sigmas_h, uncert_pruning=True))\n",
    "            \n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        \n",
    "        return xs, ya, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.423908216\n",
      "Epoch: 0006 cost= 0.074809015\n",
      "Epoch: 0011 cost= 0.034873054\n",
      "Epoch: 0016 cost= 0.017029525\n",
      "Epoch: 0021 cost= 0.008828043\n",
      "Epoch: 0026 cost= 0.005183751\n",
      "Epoch: 0031 cost= 0.004760422\n",
      "Epoch: 0036 cost= 0.000991851\n",
      "Epoch: 0041 cost= 0.000453620\n",
      "Epoch: 0046 cost= 0.001089015\n",
      "Epoch: 0051 cost= 0.000240368\n",
      "Epoch: 0056 cost= 0.000249355\n",
      "Epoch: 0061 cost= 0.000135572\n",
      "Epoch: 0066 cost= 0.011170367\n",
      "Epoch: 0071 cost= 0.000105972\n",
      "Epoch: 0076 cost= 0.000064127\n",
      "Epoch: 0081 cost= 0.000742145\n",
      "Epoch: 0086 cost= 0.000067122\n",
      "Epoch: 0091 cost= 0.000039976\n",
      "Epoch: 0096 cost= 0.003797296\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 3.228416839\n",
      "Epoch: 0006 cost= 2.006878323\n",
      "Epoch: 0011 cost= 1.208152405\n",
      "Epoch: 0016 cost= 0.899522673\n",
      "Epoch: 0021 cost= 0.783278817\n",
      "Epoch: 0026 cost= 0.700111097\n",
      "Epoch: 0031 cost= 0.634223401\n",
      "Epoch: 0036 cost= 0.581039405\n",
      "Epoch: 0041 cost= 0.532690271\n",
      "Epoch: 0046 cost= 0.493331300\n",
      "Epoch: 0051 cost= 0.460849917\n",
      "Epoch: 0056 cost= 0.432339303\n",
      "Epoch: 0061 cost= 0.407290286\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 500\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "coreset_size = 0\n",
    "data_gen = MnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "task_id=0\n",
    "    \n",
    "tf.reset_default_graph()  \n",
    "x_train, y_train, x_test, y_test = data_gen.task()\n",
    "x_testsets.append(x_test)\n",
    "y_testsets.append(y_test)\n",
    "\n",
    "# Set the readout head to train\n",
    "head = 0 if single_head else task_id\n",
    "bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "# Train network with maximum likelihood to initialize first model\n",
    "if task_id == 0:\n",
    "    ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "    ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "    mf_weights = ml_model.get_weights()\n",
    "    mf_variances = None\n",
    "    ml_model.close_session()\n",
    "\n",
    "# Train on non-coreset data\n",
    "mf_model = MFVI_NN_prune(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                         prev_log_variances=mf_variances)\n",
    "\n",
    "mf_model.train(x_train, y_train, head, no_epochs, bsize)\n",
    "\n",
    "xs, ya, yb  = mf_model.prune_weights(x_test, y_test, head)\n",
    "\n",
    "mf_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ya, label='$|\\mu|$')\n",
    "plt.plot(xs, yb, label='snr')\n",
    "plt.plot(xs, ya_ibp, label='ibp $|\\mu|$')\n",
    "plt.plot(xs, yb_ibp, label='ibp snr')\n",
    "plt.xlabel('cut-off')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 200\n",
    "alpha0 = 1.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = False\n",
    "data_gen = SplitMnistGenerator(val, num_tasks=1)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN_prune(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, lambda_1=tau0, lambda_2=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    \n",
    "    xs, ya, yb, ya_ibp, yb_ibp  = mf_model.prune_weights(x_test, y_test, head)\n",
    "    \n",
    "    mf_model.close_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
