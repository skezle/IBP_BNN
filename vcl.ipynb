{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import pickle\n",
    "import sys\n",
    "from ddm.run_split import SplitMnistGenerator\n",
    "from ddm.alg.cla_models_multihead import MFVI_IBP_NN, Vanilla_NN\n",
    "#from ddm.alg.vcl import run_vcl, run_vcl_ibp\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.326666718\n",
      "Epoch: 0006 cost= 0.002547519\n",
      "Epoch: 0011 cost= 0.001163158\n",
      "Epoch: 0016 cost= 0.000456997\n",
      "Epoch: 0021 cost= 0.000236349\n",
      "Epoch: 0026 cost= 0.000126062\n",
      "Epoch: 0031 cost= 0.000080582\n",
      "Epoch: 0036 cost= 0.000053394\n",
      "Epoch: 0041 cost= 0.000039299\n",
      "Epoch: 0046 cost= 0.000029185\n",
      "Epoch: 0051 cost= 0.000023426\n",
      "Epoch: 0056 cost= 0.000018092\n",
      "Epoch: 0061 cost= 0.000014716\n",
      "Epoch: 0066 cost= 0.000012215\n",
      "Epoch: 0071 cost= 0.000010244\n",
      "Epoch: 0076 cost= 0.000008630\n",
      "Epoch: 0081 cost= 0.000007770\n",
      "Epoch: 0086 cost= 0.000006347\n",
      "Epoch: 0091 cost= 0.000005492\n",
      "Epoch: 0096 cost= 0.000004839\n",
      "Optimization Finished!\n",
      "logpis: (100, 784, 50)\n",
      "logpis: (100, 1, 50)\n",
      "act: (100, ?, 50)\n",
      "logpis: (100, 50, 50)\n",
      "logpis: (100, 1, 50)\n",
      "act: (100, ?, 50)\n",
      "logpis: <unknown>\n",
      "logpis: <unknown>\n",
      "logpis: (100, 784, 50)\n",
      "logpis: (100, 784, 50)\n",
      "log_post: (100, 784, 50)\n",
      "log_prior: (100, 784, 50)\n",
      "log_sample: (100, 784, 50)\n",
      "mean: (784, 50)\n",
      "logpis: (100, 784, 50)\n",
      "logpis: (100, 784, 50)\n",
      "log_post: (100, 784, 50)\n",
      "log_prior: (100, 784, 50)\n",
      "log_sample: (100, 784, 50)\n",
      "mean: (784, 50)\n",
      "logpis: (100, 50, 50)\n",
      "logpis: (100, 50, 50)\n",
      "log_post: (100, 50, 50)\n",
      "log_prior: (100, 50, 50)\n",
      "log_sample: (100, 50, 50)\n",
      "mean: (50, 50)\n",
      "logpis: (100, 50, 50)\n",
      "logpis: (100, 50, 50)\n",
      "log_post: (100, 50, 50)\n",
      "log_prior: (100, 50, 50)\n",
      "log_sample: (100, 50, 50)\n",
      "mean: (50, 50)\n",
      "logpis: (100, 50, 2)\n",
      "logpis: (100, 50, 2)\n",
      "log_post: (100, 50, 2)\n",
      "log_prior: (100, 50, 2)\n",
      "log_sample: (100, 50, 2)\n",
      "mean: (50, 2)\n",
      "logpis: (100, 50, 2)\n",
      "logpis: (100, 50, 2)\n",
      "log_post: (100, 50, 2)\n",
      "log_prior: (100, 50, 2)\n",
      "log_sample: (100, 50, 2)\n",
      "mean: (50, 2)\n",
      "logpis: (10, 784, 50)\n",
      "logpis: (10, 1, 50)\n",
      "act: (10, ?, 50)\n",
      "logpis: (10, 50, 50)\n",
      "logpis: (10, 1, 50)\n",
      "act: (10, ?, 50)\n",
      "logpis: <unknown>\n",
      "logpis: <unknown>\n",
      "logpis: (100, 784, 50)\n",
      "logpis: (100, 1, 50)\n",
      "act: (100, ?, 50)\n",
      "logpis: (100, 50, 50)\n",
      "logpis: (100, 1, 50)\n",
      "act: (100, ?, 50)\n",
      "logpis: <unknown>\n",
      "logpis: <unknown>\n",
      "Epoch: 0001 cost= 57867.075625000\n",
      "Epoch: 0006 cost= 56627.496406250\n",
      "Epoch: 0011 cost= 55641.178750000\n",
      "Epoch: 0016 cost= 54820.119687500\n",
      "Epoch: 0021 cost= 54094.438750000\n",
      "Epoch: 0026 cost= 53393.853437500\n",
      "Epoch: 0031 cost= 52708.019843750\n",
      "Epoch: 0036 cost= 51993.765937500\n",
      "Epoch: 0041 cost= 51251.249531250\n",
      "Epoch: 0046 cost= 50457.007812500\n",
      "Epoch: 0051 cost= 49578.447656250\n",
      "Epoch: 0056 cost= 48997.905468750\n",
      "Epoch: 0061 cost= 48453.629062500\n",
      "Epoch: 0066 cost= 47793.476406250\n",
      "Epoch: 0071 cost= 47023.584843750\n",
      "Epoch: 0076 cost= 46095.775156250\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [50,50]\n",
    "batch_size = 512\n",
    "no_epochs = 100\n",
    "alpha0 = 50.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.5\n",
    "\n",
    "# Run vanilla VCL\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "coreset_size = 0\n",
    "data_gen = SplitMnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, temp_prior=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize, tau0=tau0, \n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "    # Incorporate coreset data and make prediction\n",
    "#     x_coresets = []\n",
    "#     y_coresets = []\n",
    "#     acc = get_scores(mf_model, x_testsets, y_testsets, x_coresets, y_coresets, hidden_size, no_epochs, single_head, batch_size)\n",
    "#     all_acc = concatenate_results(acc, all_acc)\n",
    "\n",
    "    mf_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
