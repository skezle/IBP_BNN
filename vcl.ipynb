{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 20:05:56.469430 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:9: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0821 20:05:56.470671 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:13: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "import pdb\n",
    "import re\n",
    "from ddm.run_split import SplitMnistGenerator\n",
    "from ddm.run_not import NotMnistGenerator\n",
    "from ddm.alg.cla_models_multihead import MFVI_IBP_NN, Vanilla_NN\n",
    "from ddm.alg.utils import get_scores, concatenate_results\n",
    "from ddm.alg.vcl import run_vcl\n",
    "from copy import deepcopy\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:14.920098 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0820 10:28:14.928613 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:177: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0820 10:28:15.041888 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:61: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0820 10:28:15.235116 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:65: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.054045935\n",
      "Epoch: 0006 cost= 0.000989384\n",
      "Epoch: 0011 cost= 0.000217877\n",
      "Epoch: 0016 cost= 0.000095243\n",
      "Epoch: 0021 cost= 0.000053343\n",
      "Epoch: 0026 cost= 0.000032177\n",
      "Epoch: 0031 cost= 0.000020990\n",
      "Epoch: 0036 cost= 0.000013812\n",
      "Epoch: 0041 cost= 0.000010000\n",
      "Epoch: 0046 cost= 0.000007089\n",
      "Epoch: 0051 cost= 0.000005146\n",
      "Epoch: 0056 cost= 0.000003880\n",
      "Epoch: 0061 cost= 0.000002915\n",
      "Epoch: 0066 cost= 0.000002253\n",
      "Epoch: 0071 cost= 0.000001736\n",
      "Epoch: 0076 cost= 0.000001336\n",
      "Epoch: 0081 cost= 0.000001051\n",
      "Epoch: 0086 cost= 0.000000806\n",
      "Epoch: 0091 cost= 0.000000640\n",
      "Epoch: 0096 cost= 0.000000494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:39.117458 140357229651776 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:567: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:39.556495 140357229651776 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:520: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:40.307626 140357229651776 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0820 10:28:42.101294 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:527: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0820 10:28:42.166632 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:662: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0820 10:28:42.207280 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:671: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0820 10:28:42.330168 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:678: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0820 10:28:42.337678 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:685: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Z: (1, ?, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:42.556347 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:955: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train cost= 22.979616349\n",
      "Epoch: 0006 train cost= 7.847686078\n",
      "Epoch: 0011 train cost= 4.484062356\n",
      "Epoch: 0016 train cost= 4.284478007\n",
      "Epoch: 0021 train cost= 4.183985552\n",
      "Epoch: 0026 train cost= 4.085030877\n",
      "Epoch: 0031 train cost= 4.028940247\n",
      "Epoch: 0036 train cost= 3.973704306\n",
      "Epoch: 0041 train cost= 3.931877205\n",
      "Epoch: 0046 train cost= 3.905022799\n",
      "Epoch: 0051 train cost= 3.867956587\n",
      "Epoch: 0056 train cost= 3.860971652\n",
      "Epoch: 0061 train cost= 3.825531069\n",
      "Epoch: 0066 train cost= 3.800201376\n",
      "Epoch: 0071 train cost= 3.808827998\n",
      "Epoch: 0076 train cost= 3.780617613\n",
      "Epoch: 0081 train cost= 3.751397926\n",
      "Epoch: 0086 train cost= 3.753835310\n",
      "Epoch: 0091 train cost= 3.723489175\n",
      "Epoch: 0096 train cost= 3.734567102\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 2.041086149\n",
      "Epoch: 0006 train cost= 1.672260149\n",
      "Epoch: 0011 train cost= 1.626302651\n",
      "Epoch: 0016 train cost= 1.594602039\n",
      "Epoch: 0021 train cost= 1.577647405\n",
      "Epoch: 0026 train cost= 1.556065313\n",
      "Epoch: 0031 train cost= 1.545514980\n",
      "Epoch: 0036 train cost= 1.533725304\n",
      "Epoch: 0041 train cost= 1.520413913\n",
      "Epoch: 0046 train cost= 1.513239266\n",
      "Epoch: 0051 train cost= 1.502713502\n",
      "Epoch: 0056 train cost= 1.483722706\n",
      "Epoch: 0061 train cost= 1.480324591\n",
      "Epoch: 0066 train cost= 1.467855182\n",
      "Epoch: 0071 train cost= 1.464583448\n",
      "Epoch: 0076 train cost= 1.458046335\n",
      "Epoch: 0081 train cost= 1.455978492\n",
      "Epoch: 0086 train cost= 1.453517416\n",
      "Epoch: 0091 train cost= 1.445287403\n",
      "Epoch: 0096 train cost= 1.433009645\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 1.579997695\n",
      "Epoch: 0006 train cost= 1.341037467\n",
      "Epoch: 0011 train cost= 1.326953120\n",
      "Epoch: 0016 train cost= 1.314823799\n",
      "Epoch: 0021 train cost= 1.306956675\n",
      "Epoch: 0026 train cost= 1.302121327\n",
      "Epoch: 0031 train cost= 1.291707468\n",
      "Epoch: 0036 train cost= 1.294732140\n",
      "Epoch: 0041 train cost= 1.290278093\n",
      "Epoch: 0046 train cost= 1.279487659\n",
      "Epoch: 0051 train cost= 1.274208863\n",
      "Epoch: 0056 train cost= 1.267571174\n",
      "Epoch: 0061 train cost= 1.269242670\n",
      "Epoch: 0066 train cost= 1.263228418\n",
      "Epoch: 0071 train cost= 1.260797440\n",
      "Epoch: 0076 train cost= 1.259605261\n",
      "Epoch: 0081 train cost= 1.252451645\n",
      "Epoch: 0086 train cost= 1.254834579\n",
      "Epoch: 0091 train cost= 1.250740369\n",
      "Epoch: 0096 train cost= 1.243335017\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 1.484987153\n",
      "Epoch: 0006 train cost= 1.204871920\n",
      "Epoch: 0011 train cost= 1.185894356\n",
      "Epoch: 0016 train cost= 1.185475276\n",
      "Epoch: 0021 train cost= 1.181127488\n",
      "Epoch: 0026 train cost= 1.180518514\n",
      "Epoch: 0031 train cost= 1.177639461\n",
      "Epoch: 0036 train cost= 1.178248239\n",
      "Epoch: 0041 train cost= 1.170061696\n",
      "Epoch: 0046 train cost= 1.172773105\n",
      "Epoch: 0051 train cost= 1.168427238\n",
      "Epoch: 0056 train cost= 1.169774820\n",
      "Epoch: 0061 train cost= 1.161941421\n",
      "Epoch: 0066 train cost= 1.158429320\n",
      "Epoch: 0071 train cost= 1.159045209\n",
      "Epoch: 0076 train cost= 1.160654365\n",
      "Epoch: 0081 train cost= 1.151641416\n",
      "Epoch: 0086 train cost= 1.154243989\n",
      "Epoch: 0091 train cost= 1.152490561\n",
      "Epoch: 0096 train cost= 1.148243466\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 1.551890070\n",
      "Epoch: 0006 train cost= 1.395971133\n",
      "Epoch: 0011 train cost= 1.379279669\n",
      "Epoch: 0016 train cost= 1.380170680\n",
      "Epoch: 0021 train cost= 1.377598857\n",
      "Epoch: 0026 train cost= 1.375637954\n",
      "Epoch: 0031 train cost= 1.369959839\n",
      "Epoch: 0036 train cost= 1.364577871\n",
      "Epoch: 0041 train cost= 1.370788833\n",
      "Epoch: 0046 train cost= 1.368471565\n",
      "Epoch: 0051 train cost= 1.366206689\n",
      "Epoch: 0056 train cost= 1.360178633\n",
      "Epoch: 0061 train cost= 1.364964792\n",
      "Epoch: 0066 train cost= 1.359770708\n",
      "Epoch: 0071 train cost= 1.359749746\n",
      "Epoch: 0076 train cost= 1.359280961\n",
      "Epoch: 0081 train cost= 1.355742337\n",
      "Epoch: 0086 train cost= 1.353503842\n",
      "Epoch: 0091 train cost= 1.354933672\n",
      "Epoch: 0096 train cost= 1.357612789\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99756691,        nan,        nan,        nan,        nan],\n",
       "       [0.52895377, 0.97425743,        nan,        nan,        nan],\n",
       "       [0.62871046, 0.87722772, 0.971549  ,        nan,        nan],\n",
       "       [0.54257908, 0.79158416, 0.97312961, 0.98833252,        nan],\n",
       "       [0.52652068, 0.61485149, 0.95574289, 0.98492951, 0.97664975]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 100\n",
    "alpha0 = 1.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "ibp_samples = 10\n",
    "\n",
    "# Run vanilla VCL\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = True\n",
    "data_gen = SplitMnistGenerator(val)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN(in_dim, hidden_size, out_dim, x_train.shape[0], num_ibp_samples=ibp_samples,\n",
    "                           prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, lambda_1=tau0, lambda_2=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "    acc = get_scores(mf_model, x_valsets, y_valsets, single_head)\n",
    "    ibp_acc = concatenate_results(acc, ibp_acc)\n",
    "    \n",
    "    mf_model.close_session()\n",
    "    \n",
    "ibp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.260041745\n",
      "Epoch: 0006 cost= 0.005151312\n",
      "Epoch: 0011 cost= 0.002609195\n",
      "Epoch: 0016 cost= 0.001677913\n",
      "Epoch: 0021 cost= 0.001073658\n",
      "Epoch: 0026 cost= 0.000651595\n",
      "Epoch: 0031 cost= 0.000435936\n",
      "Epoch: 0036 cost= 0.000280965\n",
      "Epoch: 0041 cost= 0.000201473\n",
      "Epoch: 0046 cost= 0.000137499\n",
      "Epoch: 0051 cost= 0.000096778\n",
      "Epoch: 0056 cost= 0.000071720\n",
      "Epoch: 0061 cost= 0.000053018\n",
      "Epoch: 0066 cost= 0.000040704\n",
      "Epoch: 0071 cost= 0.000031381\n",
      "Epoch: 0076 cost= 0.000023424\n",
      "Epoch: 0081 cost= 0.000018736\n",
      "Epoch: 0086 cost= 0.000013911\n",
      "Epoch: 0091 cost= 0.000010898\n",
      "Epoch: 0096 cost= 0.000008418\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 1.846670676\n",
      "Epoch: 0006 cost= 1.689760324\n",
      "Epoch: 0011 cost= 1.536770825\n",
      "Epoch: 0016 cost= 1.385002039\n",
      "Epoch: 0021 cost= 1.234384163\n",
      "Epoch: 0026 cost= 1.086196825\n",
      "Epoch: 0031 cost= 0.940980348\n",
      "Epoch: 0036 cost= 0.800019918\n",
      "Epoch: 0041 cost= 0.664992467\n",
      "Epoch: 0046 cost= 0.537858195\n",
      "Epoch: 0051 cost= 0.420864664\n",
      "Epoch: 0056 cost= 0.315800496\n",
      "Epoch: 0061 cost= 0.227134357\n",
      "Epoch: 0066 cost= 0.156525758\n",
      "Epoch: 0071 cost= 0.107231790\n",
      "Epoch: 0076 cost= 0.074533225\n",
      "Epoch: 0081 cost= 0.055905868\n",
      "Epoch: 0086 cost= 0.046409810\n",
      "Epoch: 0091 cost= 0.041310130\n",
      "Epoch: 0096 cost= 0.036952582\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.783298159\n",
      "Epoch: 0006 cost= 0.254706398\n",
      "Epoch: 0011 cost= 0.186603178\n",
      "Epoch: 0016 cost= 0.169851414\n",
      "Epoch: 0021 cost= 0.156130277\n",
      "Epoch: 0026 cost= 0.145390899\n",
      "Epoch: 0031 cost= 0.143236202\n",
      "Epoch: 0036 cost= 0.135447690\n",
      "Epoch: 0041 cost= 0.131772337\n",
      "Epoch: 0046 cost= 0.129090687\n",
      "Epoch: 0051 cost= 0.127712758\n",
      "Epoch: 0056 cost= 0.125231992\n",
      "Epoch: 0061 cost= 0.123649791\n",
      "Epoch: 0066 cost= 0.124717583\n",
      "Epoch: 0071 cost= 0.120561787\n",
      "Epoch: 0076 cost= 0.119413396\n",
      "Epoch: 0081 cost= 0.117522474\n",
      "Epoch: 0086 cost= 0.118759446\n",
      "Epoch: 0091 cost= 0.115632451\n",
      "Epoch: 0096 cost= 0.115514013\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.721903037\n",
      "Epoch: 0006 cost= 0.178758990\n",
      "Epoch: 0011 cost= 0.114722312\n",
      "Epoch: 0016 cost= 0.090272589\n",
      "Epoch: 0021 cost= 0.080876312\n",
      "Epoch: 0026 cost= 0.077547105\n",
      "Epoch: 0031 cost= 0.074734857\n",
      "Epoch: 0036 cost= 0.071093876\n",
      "Epoch: 0041 cost= 0.068447910\n",
      "Epoch: 0046 cost= 0.066602131\n",
      "Epoch: 0051 cost= 0.067853554\n",
      "Epoch: 0056 cost= 0.066697769\n",
      "Epoch: 0061 cost= 0.067348756\n",
      "Epoch: 0066 cost= 0.066392783\n",
      "Epoch: 0071 cost= 0.064257479\n",
      "Epoch: 0076 cost= 0.063865856\n",
      "Epoch: 0081 cost= 0.065018147\n",
      "Epoch: 0086 cost= 0.066137813\n",
      "Epoch: 0091 cost= 0.064019810\n",
      "Epoch: 0096 cost= 0.064602594\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.917744419\n",
      "Epoch: 0006 cost= 0.064089139\n",
      "Epoch: 0011 cost= 0.045433316\n",
      "Epoch: 0016 cost= 0.039231084\n",
      "Epoch: 0021 cost= 0.035372090\n",
      "Epoch: 0026 cost= 0.034368573\n",
      "Epoch: 0031 cost= 0.033419235\n",
      "Epoch: 0036 cost= 0.033916489\n",
      "Epoch: 0041 cost= 0.033761202\n",
      "Epoch: 0046 cost= 0.032982266\n",
      "Epoch: 0051 cost= 0.031784233\n",
      "Epoch: 0056 cost= 0.030813103\n",
      "Epoch: 0061 cost= 0.030423413\n",
      "Epoch: 0066 cost= 0.030501187\n",
      "Epoch: 0071 cost= 0.031372322\n",
      "Epoch: 0076 cost= 0.030543002\n",
      "Epoch: 0081 cost= 0.030356304\n",
      "Epoch: 0086 cost= 0.030751476\n",
      "Epoch: 0091 cost= 0.029227438\n",
      "Epoch: 0096 cost= 0.030815437\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.878556023\n",
      "Epoch: 0006 cost= 0.201092169\n",
      "Epoch: 0011 cost= 0.131515339\n",
      "Epoch: 0016 cost= 0.111028400\n",
      "Epoch: 0021 cost= 0.102196311\n",
      "Epoch: 0026 cost= 0.095191910\n",
      "Epoch: 0031 cost= 0.091067829\n",
      "Epoch: 0036 cost= 0.087748593\n",
      "Epoch: 0041 cost= 0.087145448\n",
      "Epoch: 0046 cost= 0.085484879\n",
      "Epoch: 0051 cost= 0.082004695\n",
      "Epoch: 0056 cost= 0.082447818\n",
      "Epoch: 0061 cost= 0.083955940\n",
      "Epoch: 0066 cost= 0.080973811\n",
      "Epoch: 0071 cost= 0.081272674\n",
      "Epoch: 0076 cost= 0.081227183\n",
      "Epoch: 0081 cost= 0.078943589\n",
      "Epoch: 0086 cost= 0.079895213\n",
      "Epoch: 0091 cost= 0.078648855\n",
      "Epoch: 0096 cost= 0.078743465\n",
      "Optimization Finished!\n",
      "[[0.99952719        nan        nan        nan        nan]\n",
      " [0.99054374 0.9877571         nan        nan        nan]\n",
      " [0.87044917 0.95200784 0.99893276        nan        nan]\n",
      " [0.89267139 0.98334966 0.99626467 0.9979859         nan]\n",
      " [0.87990544 0.85406464 0.99306297 0.98640483 0.98587998]]\n"
     ]
    }
   ],
   "source": [
    "# Run Vanilla VCL\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "hidden_size = [10]\n",
    "coreset_size = 0\n",
    "\n",
    "data_gen = SplitMnistGenerator(val=True)\n",
    "vcl_result = run_vcl(hidden_size, no_epochs, data_gen, \n",
    "                              lambda a: a, coreset_size, batch_size, single_head, val=True)\n",
    "print(vcl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99609375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data_gen.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(data_gen.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run IBP VCL\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(12)\n",
    "# np.random.seed(1)\n",
    "# coreset_size = 0\n",
    "\n",
    "# hidden_size = [50]\n",
    "# batch_size = 128\n",
    "# no_epochs = 100\n",
    "# alpha0 = 5.0\n",
    "# tau0=0.1 # initial temperature\n",
    "# temp_prior=1.0\n",
    "# ANNEAL_RATE=0.000\n",
    "# MIN_TEMP=0.1\n",
    "# single_head=False\n",
    "\n",
    "# # data_gen = SplitMnistGenerator()\n",
    "# # vcl_ibp_result = vcl.run_vcl_ibp(hidden_size=hidden_size, no_epochs=no_epochs, data_gen=data_gen,\n",
    "# #                                   batch_size=batch_size, single_head=single_head, alpha0=alpha0,\n",
    "# #                                   learning_rate=0.01, temp_prior=temp_prior, no_pred_samples=100,\n",
    "# #                                   tau0=tau0, tau_anneal_rate=ANNEAL_RATE, tau_min=MIN_TEMP)\n",
    "# # print(vcl_ibp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ibp_acc = np.nanmean(ibp_acc, 1)\n",
    "_vcl_result = np.nanmean(vcl_result, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vcl_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = plt.gca()\n",
    "plt.plot(np.arange(len(_ibp_acc))+1, _ibp_acc, label='VCL + IBP', marker='o')\n",
    "plt.plot(np.arange(len(_vcl_result))+1, _vcl_result, label='VCL', marker='o')\n",
    "ax.set_xticks(range(1, len(_ibp_acc)+1))\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('\\# tasks')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.146469993\n",
      "Epoch: 0006 cost= 0.051735276\n",
      "Epoch: 0011 cost= 0.028104113\n",
      "Epoch: 0016 cost= 0.015942755\n",
      "Epoch: 0021 cost= 0.015865596\n",
      "Epoch: 0026 cost= 0.011969231\n",
      "Epoch: 0031 cost= 0.009667647\n",
      "Epoch: 0036 cost= 0.009111372\n",
      "Epoch: 0041 cost= 0.008909420\n",
      "Epoch: 0046 cost= 0.008711578\n",
      "Epoch: 0051 cost= 0.010608444\n",
      "Epoch: 0056 cost= 0.008632005\n",
      "Epoch: 0061 cost= 0.011303626\n",
      "Epoch: 0066 cost= 0.006225984\n",
      "Epoch: 0071 cost= 0.005884801\n",
      "Epoch: 0076 cost= 0.005166127\n",
      "Epoch: 0081 cost= 0.006437723\n",
      "Epoch: 0086 cost= 0.006132824\n",
      "Epoch: 0091 cost= 0.006260679\n",
      "Epoch: 0096 cost= 0.006746725\n",
      "Epoch: 0101 cost= 0.005560117\n",
      "Epoch: 0106 cost= 0.005934628\n",
      "Epoch: 0111 cost= 0.005584506\n",
      "Epoch: 0116 cost= 0.003656060\n",
      "Epoch: 0121 cost= 0.004826352\n",
      "Epoch: 0126 cost= 0.004615627\n",
      "Epoch: 0131 cost= 0.004941612\n",
      "Epoch: 0136 cost= 0.003932573\n",
      "Epoch: 0141 cost= 0.005345335\n",
      "Epoch: 0146 cost= 0.006734761\n",
      "Epoch: 0151 cost= 0.005482487\n",
      "Epoch: 0156 cost= 0.004824669\n",
      "Epoch: 0161 cost= 0.005427741\n",
      "Epoch: 0166 cost= 0.005529169\n",
      "Epoch: 0171 cost= 0.003909187\n",
      "Epoch: 0176 cost= 0.003996661\n",
      "Epoch: 0181 cost= 0.004196767\n",
      "Epoch: 0186 cost= 0.005134154\n",
      "Epoch: 0191 cost= 0.006026610\n",
      "Epoch: 0196 cost= 0.003417335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 20:13:43.979037 140352637986624 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:575: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 20:13:44.308247 140352637986624 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:528: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 20:13:45.146842 140352637986624 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0821 20:13:46.593549 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:535: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0821 20:13:46.644495 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:670: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0821 20:13:46.668156 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:680: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0821 20:13:46.751312 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:687: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0821 20:13:46.756525 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:694: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Z: (1, ?, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 20:13:46.904784 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:965: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train cost= 5.202547515\n",
      "Epoch: 0006 train cost= 3.581586042\n",
      "Epoch: 0011 train cost= 2.206910697\n",
      "Epoch: 0016 train cost= 1.116205960\n",
      "Epoch: 0021 train cost= 0.593110222\n",
      "Epoch: 0026 train cost= 0.485295451\n",
      "Epoch: 0031 train cost= 0.425318644\n",
      "Epoch: 0036 train cost= 0.385313988\n",
      "Epoch: 0041 train cost= 0.355264273\n",
      "Epoch: 0046 train cost= 0.336294629\n",
      "Epoch: 0051 train cost= 0.322879804\n",
      "Epoch: 0056 train cost= 0.313448984\n",
      "Epoch: 0061 train cost= 0.306135205\n",
      "Epoch: 0066 train cost= 0.301395781\n",
      "Epoch: 0071 train cost= 0.297382401\n",
      "Epoch: 0076 train cost= 0.295581180\n",
      "Epoch: 0081 train cost= 0.292744761\n",
      "Epoch: 0086 train cost= 0.290746831\n",
      "Epoch: 0091 train cost= 0.290177066\n",
      "Epoch: 0096 train cost= 0.288360518\n",
      "Epoch: 0101 train cost= 0.287183318\n",
      "Epoch: 0106 train cost= 0.286402736\n",
      "Epoch: 0111 train cost= 0.284753941\n",
      "Epoch: 0116 train cost= 0.283848950\n",
      "Epoch: 0121 train cost= 0.284287527\n",
      "Epoch: 0126 train cost= 0.284116088\n",
      "Epoch: 0131 train cost= 0.281811037\n",
      "Epoch: 0136 train cost= 0.282912589\n",
      "Epoch: 0141 train cost= 0.281899319\n",
      "Epoch: 0146 train cost= 0.281253792\n",
      "Epoch: 0151 train cost= 0.281499689\n",
      "Epoch: 0156 train cost= 0.280103175\n",
      "Epoch: 0161 train cost= 0.281018923\n",
      "Epoch: 0166 train cost= 0.280627601\n",
      "Epoch: 0171 train cost= 0.279709041\n",
      "Epoch: 0176 train cost= 0.279801923\n",
      "Epoch: 0181 train cost= 0.279090449\n",
      "Epoch: 0186 train cost= 0.279295081\n",
      "Epoch: 0191 train cost= 0.278917749\n",
      "Epoch: 0196 train cost= 0.278974115\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 0.435980088\n",
      "Epoch: 0006 train cost= 0.220334152\n",
      "Epoch: 0011 train cost= 0.207404240\n",
      "Epoch: 0016 train cost= 0.202172433\n",
      "Epoch: 0021 train cost= 0.196900151\n",
      "Epoch: 0026 train cost= 0.194998902\n",
      "Epoch: 0031 train cost= 0.193677134\n",
      "Epoch: 0036 train cost= 0.192890717\n",
      "Epoch: 0041 train cost= 0.191949455\n",
      "Epoch: 0046 train cost= 0.190884772\n",
      "Epoch: 0051 train cost= 0.190218578\n",
      "Epoch: 0056 train cost= 0.189447271\n",
      "Epoch: 0061 train cost= 0.189774837\n",
      "Epoch: 0066 train cost= 0.189257013\n",
      "Epoch: 0071 train cost= 0.187914174\n",
      "Epoch: 0076 train cost= 0.188005403\n",
      "Epoch: 0081 train cost= 0.188802444\n",
      "Epoch: 0086 train cost= 0.188039956\n",
      "Epoch: 0091 train cost= 0.187202129\n",
      "Epoch: 0096 train cost= 0.186795458\n",
      "Epoch: 0101 train cost= 0.187128086\n",
      "Epoch: 0106 train cost= 0.186675746\n",
      "Epoch: 0111 train cost= 0.187015504\n",
      "Epoch: 0116 train cost= 0.186631516\n",
      "Epoch: 0121 train cost= 0.187088147\n",
      "Epoch: 0126 train cost= 0.186070911\n",
      "Epoch: 0131 train cost= 0.185473330\n",
      "Epoch: 0136 train cost= 0.186760943\n",
      "Epoch: 0141 train cost= 0.185289397\n",
      "Epoch: 0146 train cost= 0.185619029\n",
      "Epoch: 0151 train cost= 0.185129987\n",
      "Epoch: 0156 train cost= 0.185768031\n",
      "Epoch: 0161 train cost= 0.185273770\n",
      "Epoch: 0166 train cost= 0.185774434\n",
      "Epoch: 0171 train cost= 0.184921301\n",
      "Epoch: 0176 train cost= 0.185943079\n",
      "Epoch: 0181 train cost= 0.185663946\n",
      "Epoch: 0186 train cost= 0.184785669\n",
      "Epoch: 0191 train cost= 0.185080358\n",
      "Epoch: 0196 train cost= 0.184010238\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 0.572373798\n",
      "Epoch: 0006 train cost= 0.335456498\n",
      "Epoch: 0011 train cost= 0.320246753\n",
      "Epoch: 0016 train cost= 0.312265820\n",
      "Epoch: 0021 train cost= 0.306546732\n",
      "Epoch: 0026 train cost= 0.304123478\n",
      "Epoch: 0031 train cost= 0.302035928\n",
      "Epoch: 0036 train cost= 0.301019350\n",
      "Epoch: 0041 train cost= 0.299330553\n",
      "Epoch: 0046 train cost= 0.299067483\n",
      "Epoch: 0051 train cost= 0.299227608\n",
      "Epoch: 0056 train cost= 0.297637792\n",
      "Epoch: 0061 train cost= 0.296105505\n",
      "Epoch: 0066 train cost= 0.297345015\n",
      "Epoch: 0071 train cost= 0.297480001\n",
      "Epoch: 0076 train cost= 0.296622923\n",
      "Epoch: 0081 train cost= 0.295630098\n",
      "Epoch: 0086 train cost= 0.296293068\n",
      "Epoch: 0091 train cost= 0.295174457\n",
      "Epoch: 0096 train cost= 0.295162259\n",
      "Epoch: 0101 train cost= 0.295209091\n",
      "Epoch: 0106 train cost= 0.295260009\n",
      "Epoch: 0111 train cost= 0.294315272\n",
      "Epoch: 0116 train cost= 0.294431575\n",
      "Epoch: 0121 train cost= 0.294131719\n",
      "Epoch: 0126 train cost= 0.293902715\n",
      "Epoch: 0131 train cost= 0.295033636\n",
      "Epoch: 0136 train cost= 0.293736771\n",
      "Epoch: 0141 train cost= 0.294994373\n",
      "Epoch: 0146 train cost= 0.294397257\n",
      "Epoch: 0151 train cost= 0.293823028\n",
      "Epoch: 0156 train cost= 0.293063638\n",
      "Epoch: 0161 train cost= 0.293345627\n",
      "Epoch: 0166 train cost= 0.292225102\n",
      "Epoch: 0171 train cost= 0.293063354\n",
      "Epoch: 0176 train cost= 0.294645047\n",
      "Epoch: 0181 train cost= 0.293515940\n",
      "Epoch: 0186 train cost= 0.293543485\n",
      "Epoch: 0191 train cost= 0.293914182\n",
      "Epoch: 0196 train cost= 0.294248112\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 0.676864910\n",
      "Epoch: 0006 train cost= 0.403922528\n",
      "Epoch: 0011 train cost= 0.387763458\n",
      "Epoch: 0016 train cost= 0.379301826\n",
      "Epoch: 0021 train cost= 0.375192738\n",
      "Epoch: 0026 train cost= 0.373469051\n",
      "Epoch: 0031 train cost= 0.370888186\n",
      "Epoch: 0036 train cost= 0.371152478\n",
      "Epoch: 0041 train cost= 0.370181469\n",
      "Epoch: 0046 train cost= 0.370109864\n",
      "Epoch: 0051 train cost= 0.368938529\n",
      "Epoch: 0056 train cost= 0.368696612\n",
      "Epoch: 0061 train cost= 0.367741325\n",
      "Epoch: 0066 train cost= 0.368633268\n",
      "Epoch: 0071 train cost= 0.368451102\n",
      "Epoch: 0076 train cost= 0.368357864\n",
      "Epoch: 0081 train cost= 0.368146533\n",
      "Epoch: 0086 train cost= 0.366754920\n",
      "Epoch: 0091 train cost= 0.368711269\n",
      "Epoch: 0096 train cost= 0.367555705\n",
      "Epoch: 0101 train cost= 0.368278783\n",
      "Epoch: 0106 train cost= 0.369257481\n",
      "Epoch: 0111 train cost= 0.368264797\n",
      "Epoch: 0116 train cost= 0.367761260\n",
      "Epoch: 0121 train cost= 0.367589561\n",
      "Epoch: 0126 train cost= 0.367628157\n",
      "Epoch: 0131 train cost= 0.366418344\n",
      "Epoch: 0136 train cost= 0.367095525\n",
      "Epoch: 0141 train cost= 0.367785142\n",
      "Epoch: 0146 train cost= 0.367483726\n",
      "Epoch: 0151 train cost= 0.368045099\n",
      "Epoch: 0156 train cost= 0.367703531\n",
      "Epoch: 0161 train cost= 0.367609317\n",
      "Epoch: 0166 train cost= 0.367235688\n",
      "Epoch: 0171 train cost= 0.367173026\n",
      "Epoch: 0176 train cost= 0.366995232\n",
      "Epoch: 0181 train cost= 0.367988395\n",
      "Epoch: 0186 train cost= 0.366181398\n",
      "Epoch: 0191 train cost= 0.367764359\n",
      "Epoch: 0196 train cost= 0.367390441\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 0.799725606\n",
      "Epoch: 0006 train cost= 0.589495919\n",
      "Epoch: 0011 train cost= 0.568783136\n",
      "Epoch: 0016 train cost= 0.556845696\n",
      "Epoch: 0021 train cost= 0.553498842\n",
      "Epoch: 0026 train cost= 0.550240441\n",
      "Epoch: 0031 train cost= 0.547269051\n",
      "Epoch: 0036 train cost= 0.546220982\n",
      "Epoch: 0041 train cost= 0.546190347\n",
      "Epoch: 0046 train cost= 0.545487501\n",
      "Epoch: 0051 train cost= 0.545160841\n",
      "Epoch: 0056 train cost= 0.545052161\n",
      "Epoch: 0061 train cost= 0.544862472\n",
      "Epoch: 0066 train cost= 0.544935500\n",
      "Epoch: 0071 train cost= 0.545080989\n",
      "Epoch: 0076 train cost= 0.544852425\n",
      "Epoch: 0081 train cost= 0.543706867\n",
      "Epoch: 0086 train cost= 0.543528112\n",
      "Epoch: 0091 train cost= 0.543385383\n",
      "Epoch: 0096 train cost= 0.543777098\n",
      "Epoch: 0101 train cost= 0.542691297\n",
      "Epoch: 0106 train cost= 0.543231087\n",
      "Epoch: 0111 train cost= 0.543757548\n",
      "Epoch: 0116 train cost= 0.542838851\n",
      "Epoch: 0121 train cost= 0.542594346\n",
      "Epoch: 0126 train cost= 0.541868451\n",
      "Epoch: 0131 train cost= 0.543095147\n",
      "Epoch: 0136 train cost= 0.542431031\n",
      "Epoch: 0141 train cost= 0.541145048\n",
      "Epoch: 0146 train cost= 0.541731275\n",
      "Epoch: 0151 train cost= 0.542290283\n",
      "Epoch: 0156 train cost= 0.541097395\n",
      "Epoch: 0161 train cost= 0.542466503\n",
      "Epoch: 0166 train cost= 0.541341209\n",
      "Epoch: 0171 train cost= 0.542131807\n",
      "Epoch: 0176 train cost= 0.542564735\n",
      "Epoch: 0181 train cost= 0.542792784\n",
      "Epoch: 0186 train cost= 0.541784970\n",
      "Epoch: 0191 train cost= 0.542561507\n",
      "Epoch: 0196 train cost= 0.541688875\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.984 ,    nan,    nan,    nan,    nan],\n",
       "       [0.9645, 0.9875,    nan,    nan,    nan],\n",
       "       [0.958 , 0.971 , 0.978 ,    nan,    nan],\n",
       "       [0.9585, 0.9575, 0.9605, 0.978 ,    nan],\n",
       "       [0.883 , 0.935 , 0.8065, 0.969 , 0.9585]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 200\n",
    "ibp_samples = 10\n",
    "\n",
    "# Run vanilla VCL\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "data_gen = NotMnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "#x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    x_train, y_train, x_test, y_test, _, _ = data_gen.next_task()\n",
    "    #x_valsets.append(x_val)\n",
    "    #y_valsets.append(y_val)\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN(in_dim, hidden_size, out_dim, x_train.shape[0], num_ibp_samples=ibp_samples,\n",
    "                           prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas, \n",
    "                           alpha0=5., beta0=1.,\n",
    "                           learning_rate=0.001, lambda_1=1.0, lambda_2=1.0, no_pred_samples=100)\n",
    "\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=0.0, min_temp=1.0)\n",
    "    mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "    acc = get_scores(mf_model, x_testsets, y_testsets, single_head)\n",
    "    ibp_acc = concatenate_results(acc, ibp_acc)\n",
    "    \n",
    "    mf_model.close_session()\n",
    "    \n",
    "ibp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.194894983\n",
      "Epoch: 0006 cost= 0.096757376\n",
      "Epoch: 0011 cost= 0.079385228\n",
      "Epoch: 0016 cost= 0.067739842\n",
      "Epoch: 0021 cost= 0.060180040\n",
      "Epoch: 0026 cost= 0.052712276\n",
      "Epoch: 0031 cost= 0.048527642\n",
      "Epoch: 0036 cost= 0.044342909\n",
      "Epoch: 0041 cost= 0.040176251\n",
      "Epoch: 0046 cost= 0.035888922\n",
      "Epoch: 0051 cost= 0.033815378\n",
      "Epoch: 0056 cost= 0.030088093\n",
      "Epoch: 0061 cost= 0.027864948\n",
      "Epoch: 0066 cost= 0.025638432\n",
      "Epoch: 0071 cost= 0.022936127\n",
      "Epoch: 0076 cost= 0.021338913\n",
      "Epoch: 0081 cost= 0.019360169\n",
      "Epoch: 0086 cost= 0.020213691\n",
      "Epoch: 0091 cost= 0.016724435\n",
      "Epoch: 0096 cost= 0.014948512\n",
      "Epoch: 0101 cost= 0.013673355\n",
      "Epoch: 0106 cost= 0.013652850\n",
      "Epoch: 0111 cost= 0.012843400\n",
      "Epoch: 0116 cost= 0.011959156\n",
      "Epoch: 0121 cost= 0.012440523\n",
      "Epoch: 0126 cost= 0.012730863\n",
      "Epoch: 0131 cost= 0.010156876\n",
      "Epoch: 0136 cost= 0.011786172\n",
      "Epoch: 0141 cost= 0.010874130\n",
      "Epoch: 0146 cost= 0.011499197\n",
      "Epoch: 0151 cost= 0.009101483\n",
      "Epoch: 0156 cost= 0.008977741\n",
      "Epoch: 0161 cost= 0.009146964\n",
      "Epoch: 0166 cost= 0.009084526\n",
      "Epoch: 0171 cost= 0.007970764\n",
      "Epoch: 0176 cost= 0.009661399\n",
      "Epoch: 0181 cost= 0.008730857\n",
      "Epoch: 0186 cost= 0.007597622\n",
      "Epoch: 0191 cost= 0.008253179\n",
      "Epoch: 0196 cost= 0.008539987\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.538870443\n",
      "Epoch: 0006 cost= 0.523553199\n",
      "Epoch: 0011 cost= 0.510257049\n",
      "Epoch: 0016 cost= 0.498335874\n",
      "Epoch: 0021 cost= 0.487070906\n",
      "Epoch: 0026 cost= 0.475317745\n",
      "Epoch: 0031 cost= 0.462882859\n",
      "Epoch: 0036 cost= 0.451750205\n",
      "Epoch: 0041 cost= 0.439749971\n",
      "Epoch: 0046 cost= 0.425826888\n",
      "Epoch: 0051 cost= 0.411537096\n",
      "Epoch: 0056 cost= 0.396596850\n",
      "Epoch: 0061 cost= 0.380465598\n",
      "Epoch: 0066 cost= 0.363705722\n",
      "Epoch: 0071 cost= 0.345258064\n",
      "Epoch: 0076 cost= 0.326181648\n",
      "Epoch: 0081 cost= 0.306171092\n",
      "Epoch: 0086 cost= 0.287908144\n",
      "Epoch: 0091 cost= 0.269341726\n",
      "Epoch: 0096 cost= 0.251629055\n",
      "Epoch: 0101 cost= 0.234696847\n",
      "Epoch: 0106 cost= 0.219616532\n",
      "Epoch: 0111 cost= 0.206680943\n",
      "Epoch: 0116 cost= 0.194829200\n",
      "Epoch: 0121 cost= 0.185429424\n",
      "Epoch: 0126 cost= 0.178421825\n",
      "Epoch: 0131 cost= 0.171371634\n",
      "Epoch: 0136 cost= 0.167095037\n",
      "Epoch: 0141 cost= 0.163289214\n",
      "Epoch: 0146 cost= 0.160947644\n",
      "Epoch: 0151 cost= 0.158222337\n",
      "Epoch: 0156 cost= 0.156161536\n",
      "Epoch: 0161 cost= 0.156123069\n",
      "Epoch: 0166 cost= 0.155655271\n",
      "Epoch: 0171 cost= 0.154477763\n",
      "Epoch: 0176 cost= 0.153719605\n",
      "Epoch: 0181 cost= 0.153941812\n",
      "Epoch: 0186 cost= 0.152436581\n",
      "Epoch: 0191 cost= 0.152095775\n",
      "Epoch: 0196 cost= 0.152511220\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.485823144\n",
      "Epoch: 0006 cost= 0.181172499\n",
      "Epoch: 0011 cost= 0.152104483\n",
      "Epoch: 0016 cost= 0.137737909\n",
      "Epoch: 0021 cost= 0.128895052\n",
      "Epoch: 0026 cost= 0.123923198\n",
      "Epoch: 0031 cost= 0.121121261\n",
      "Epoch: 0036 cost= 0.118970182\n",
      "Epoch: 0041 cost= 0.117982448\n",
      "Epoch: 0046 cost= 0.117512280\n",
      "Epoch: 0051 cost= 0.117279226\n",
      "Epoch: 0056 cost= 0.116897838\n",
      "Epoch: 0061 cost= 0.116179079\n",
      "Epoch: 0066 cost= 0.115583281\n",
      "Epoch: 0071 cost= 0.115203633\n",
      "Epoch: 0076 cost= 0.114450724\n",
      "Epoch: 0081 cost= 0.114709470\n",
      "Epoch: 0086 cost= 0.114160149\n",
      "Epoch: 0091 cost= 0.113769223\n",
      "Epoch: 0096 cost= 0.113929036\n",
      "Epoch: 0101 cost= 0.113744302\n",
      "Epoch: 0106 cost= 0.114124661\n",
      "Epoch: 0111 cost= 0.113714529\n",
      "Epoch: 0116 cost= 0.113250941\n",
      "Epoch: 0121 cost= 0.113438987\n",
      "Epoch: 0126 cost= 0.113860756\n",
      "Epoch: 0131 cost= 0.113537462\n",
      "Epoch: 0136 cost= 0.113869306\n",
      "Epoch: 0141 cost= 0.113626410\n",
      "Epoch: 0146 cost= 0.113920246\n",
      "Epoch: 0151 cost= 0.113544088\n",
      "Epoch: 0156 cost= 0.113312799\n",
      "Epoch: 0161 cost= 0.113573379\n",
      "Epoch: 0166 cost= 0.113223769\n",
      "Epoch: 0171 cost= 0.113085457\n",
      "Epoch: 0176 cost= 0.113660312\n",
      "Epoch: 0181 cost= 0.113395408\n",
      "Epoch: 0186 cost= 0.113937064\n",
      "Epoch: 0191 cost= 0.113331086\n",
      "Epoch: 0196 cost= 0.113359206\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.783941766\n",
      "Epoch: 0006 cost= 0.233882461\n",
      "Epoch: 0011 cost= 0.198836276\n",
      "Epoch: 0016 cost= 0.181183278\n",
      "Epoch: 0021 cost= 0.171104626\n",
      "Epoch: 0026 cost= 0.163763849\n",
      "Epoch: 0031 cost= 0.161009232\n",
      "Epoch: 0036 cost= 0.157333524\n",
      "Epoch: 0041 cost= 0.156260887\n",
      "Epoch: 0046 cost= 0.154773757\n",
      "Epoch: 0051 cost= 0.153899566\n",
      "Epoch: 0056 cost= 0.154160244\n",
      "Epoch: 0061 cost= 0.153565550\n",
      "Epoch: 0066 cost= 0.153841710\n",
      "Epoch: 0071 cost= 0.153106006\n",
      "Epoch: 0076 cost= 0.153325962\n",
      "Epoch: 0081 cost= 0.152393612\n",
      "Epoch: 0086 cost= 0.153320924\n",
      "Epoch: 0091 cost= 0.152542535\n",
      "Epoch: 0096 cost= 0.153056970\n",
      "Epoch: 0101 cost= 0.152898628\n",
      "Epoch: 0106 cost= 0.152817930\n",
      "Epoch: 0111 cost= 0.152112773\n",
      "Epoch: 0116 cost= 0.153047390\n",
      "Epoch: 0121 cost= 0.152205522\n",
      "Epoch: 0126 cost= 0.152503489\n",
      "Epoch: 0131 cost= 0.152661332\n",
      "Epoch: 0136 cost= 0.152063095\n",
      "Epoch: 0141 cost= 0.152256490\n",
      "Epoch: 0146 cost= 0.152368495\n",
      "Epoch: 0151 cost= 0.151640523\n",
      "Epoch: 0156 cost= 0.152827656\n",
      "Epoch: 0161 cost= 0.151894289\n",
      "Epoch: 0166 cost= 0.151578006\n",
      "Epoch: 0171 cost= 0.152814098\n",
      "Epoch: 0176 cost= 0.151790922\n",
      "Epoch: 0181 cost= 0.152514642\n",
      "Epoch: 0186 cost= 0.152884376\n",
      "Epoch: 0191 cost= 0.151641447\n",
      "Epoch: 0196 cost= 0.151740848\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.464540843\n",
      "Epoch: 0006 cost= 0.198404886\n",
      "Epoch: 0011 cost= 0.172454445\n",
      "Epoch: 0016 cost= 0.158252851\n",
      "Epoch: 0021 cost= 0.148134429\n",
      "Epoch: 0026 cost= 0.143700531\n",
      "Epoch: 0031 cost= 0.140032622\n",
      "Epoch: 0036 cost= 0.138074378\n",
      "Epoch: 0041 cost= 0.136873415\n",
      "Epoch: 0046 cost= 0.136172248\n",
      "Epoch: 0051 cost= 0.135675056\n",
      "Epoch: 0056 cost= 0.135874807\n",
      "Epoch: 0061 cost= 0.135876603\n",
      "Epoch: 0066 cost= 0.135912963\n",
      "Epoch: 0071 cost= 0.135407914\n",
      "Epoch: 0076 cost= 0.135250808\n",
      "Epoch: 0081 cost= 0.135756058\n",
      "Epoch: 0086 cost= 0.135025520\n",
      "Epoch: 0091 cost= 0.135559928\n",
      "Epoch: 0096 cost= 0.135467987\n",
      "Epoch: 0101 cost= 0.135604285\n",
      "Epoch: 0106 cost= 0.135320940\n",
      "Epoch: 0111 cost= 0.134998522\n",
      "Epoch: 0116 cost= 0.135302223\n",
      "Epoch: 0121 cost= 0.134852933\n",
      "Epoch: 0126 cost= 0.134910199\n",
      "Epoch: 0131 cost= 0.134798151\n",
      "Epoch: 0136 cost= 0.135084802\n",
      "Epoch: 0141 cost= 0.135357321\n",
      "Epoch: 0146 cost= 0.135253043\n",
      "Epoch: 0151 cost= 0.135613660\n",
      "Epoch: 0156 cost= 0.135411971\n",
      "Epoch: 0161 cost= 0.134572244\n",
      "Epoch: 0166 cost= 0.135419976\n",
      "Epoch: 0171 cost= 0.135299410\n",
      "Epoch: 0176 cost= 0.135404005\n",
      "Epoch: 0181 cost= 0.134722975\n",
      "Epoch: 0186 cost= 0.135023226\n",
      "Epoch: 0191 cost= 0.135224043\n",
      "Epoch: 0196 cost= 0.135344816\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 1.008779616\n",
      "Epoch: 0006 cost= 0.312329986\n",
      "Epoch: 0011 cost= 0.266577791\n",
      "Epoch: 0016 cost= 0.248665851\n",
      "Epoch: 0021 cost= 0.239121027\n",
      "Epoch: 0026 cost= 0.231888344\n",
      "Epoch: 0031 cost= 0.227821135\n",
      "Epoch: 0036 cost= 0.224977694\n",
      "Epoch: 0041 cost= 0.223184509\n",
      "Epoch: 0046 cost= 0.222193658\n",
      "Epoch: 0051 cost= 0.221382925\n",
      "Epoch: 0056 cost= 0.220396146\n",
      "Epoch: 0061 cost= 0.220551168\n",
      "Epoch: 0066 cost= 0.220289134\n",
      "Epoch: 0071 cost= 0.219496435\n",
      "Epoch: 0076 cost= 0.219521774\n",
      "Epoch: 0081 cost= 0.219158161\n",
      "Epoch: 0086 cost= 0.218429723\n",
      "Epoch: 0091 cost= 0.218829423\n",
      "Epoch: 0096 cost= 0.219641647\n",
      "Epoch: 0101 cost= 0.218879388\n",
      "Epoch: 0106 cost= 0.218654466\n",
      "Epoch: 0111 cost= 0.218664610\n",
      "Epoch: 0116 cost= 0.218733360\n",
      "Epoch: 0121 cost= 0.218933860\n",
      "Epoch: 0126 cost= 0.219211988\n",
      "Epoch: 0131 cost= 0.218866455\n",
      "Epoch: 0136 cost= 0.219988711\n",
      "Epoch: 0141 cost= 0.218379400\n",
      "Epoch: 0146 cost= 0.218608024\n",
      "Epoch: 0151 cost= 0.218254897\n",
      "Epoch: 0156 cost= 0.218935845\n",
      "Epoch: 0161 cost= 0.218572636\n",
      "Epoch: 0166 cost= 0.217762839\n",
      "Epoch: 0171 cost= 0.219415328\n",
      "Epoch: 0176 cost= 0.218569030\n",
      "Epoch: 0181 cost= 0.218790179\n",
      "Epoch: 0186 cost= 0.218965097\n",
      "Epoch: 0191 cost= 0.218925110\n",
      "Epoch: 0196 cost= 0.218698303\n",
      "Optimization Finished!\n",
      "[[0.9855    nan    nan    nan    nan]\n",
      " [0.979  0.9875    nan    nan    nan]\n",
      " [0.953  0.967  0.9785    nan    nan]\n",
      " [0.9585 0.9115 0.97   0.9765    nan]\n",
      " [0.917  0.846  0.774  0.9535 0.9575]]\n"
     ]
    }
   ],
   "source": [
    "# Run Vanilla VCL\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "hidden_size = [10]\n",
    "coreset_size = 0\n",
    "\n",
    "data_gen = NotMnistGenerator()\n",
    "vcl_result = run_vcl(hidden_size, no_epochs, data_gen, \n",
    "                     lambda a: a, coreset_size, batch_size, single_head, val=True)\n",
    "print(vcl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.211736251\n",
      "Epoch: 0006 cost= 0.107794914\n",
      "Epoch: 0011 cost= 0.096261189\n",
      "Epoch: 0016 cost= 0.089602290\n",
      "Epoch: 0021 cost= 0.084507436\n",
      "Epoch: 0026 cost= 0.080454039\n",
      "Epoch: 0031 cost= 0.076957272\n",
      "Epoch: 0036 cost= 0.074719685\n",
      "Epoch: 0041 cost= 0.071458950\n",
      "Epoch: 0046 cost= 0.069443085\n",
      "Epoch: 0051 cost= 0.067098836\n",
      "Epoch: 0056 cost= 0.065269344\n",
      "Epoch: 0061 cost= 0.062815753\n",
      "Epoch: 0066 cost= 0.061346556\n",
      "Epoch: 0071 cost= 0.060075689\n",
      "Epoch: 0076 cost= 0.058046302\n",
      "Epoch: 0081 cost= 0.056467461\n",
      "Epoch: 0086 cost= 0.055744268\n",
      "Epoch: 0091 cost= 0.054359862\n",
      "Epoch: 0096 cost= 0.053127319\n",
      "Epoch: 0101 cost= 0.051854244\n",
      "Epoch: 0106 cost= 0.050908735\n",
      "Epoch: 0111 cost= 0.049672897\n",
      "Epoch: 0116 cost= 0.049877447\n",
      "Epoch: 0121 cost= 0.048096809\n",
      "Epoch: 0126 cost= 0.048598100\n",
      "Epoch: 0131 cost= 0.046465958\n",
      "Epoch: 0136 cost= 0.044872621\n",
      "Epoch: 0141 cost= 0.044639408\n",
      "Epoch: 0146 cost= 0.044706659\n",
      "Epoch: 0151 cost= 0.042965389\n",
      "Epoch: 0156 cost= 0.042411680\n",
      "Epoch: 0161 cost= 0.042046499\n",
      "Epoch: 0166 cost= 0.041280410\n",
      "Epoch: 0171 cost= 0.041586867\n",
      "Epoch: 0176 cost= 0.039944235\n",
      "Epoch: 0181 cost= 0.039392479\n",
      "Epoch: 0186 cost= 0.039247124\n",
      "Epoch: 0191 cost= 0.038638533\n",
      "Epoch: 0196 cost= 0.038315605\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.334972474\n",
      "Epoch: 0006 cost= 0.325950648\n",
      "Epoch: 0011 cost= 0.318711584\n",
      "Epoch: 0016 cost= 0.313192172\n",
      "Epoch: 0021 cost= 0.305726079\n",
      "Epoch: 0026 cost= 0.300283342\n",
      "Epoch: 0031 cost= 0.292009220\n",
      "Epoch: 0036 cost= 0.286187811\n",
      "Epoch: 0041 cost= 0.277812307\n",
      "Epoch: 0046 cost= 0.270403640\n",
      "Epoch: 0051 cost= 0.262962491\n",
      "Epoch: 0056 cost= 0.256020988\n",
      "Epoch: 0061 cost= 0.247604473\n",
      "Epoch: 0066 cost= 0.240484776\n",
      "Epoch: 0071 cost= 0.233150677\n",
      "Epoch: 0076 cost= 0.225531829\n",
      "Epoch: 0081 cost= 0.217766543\n",
      "Epoch: 0086 cost= 0.210950122\n",
      "Epoch: 0091 cost= 0.203676315\n",
      "Epoch: 0096 cost= 0.198410942\n",
      "Epoch: 0101 cost= 0.191348788\n",
      "Epoch: 0106 cost= 0.185723443\n",
      "Epoch: 0111 cost= 0.180498771\n",
      "Epoch: 0116 cost= 0.175582487\n",
      "Epoch: 0121 cost= 0.171745472\n",
      "Epoch: 0126 cost= 0.167249254\n",
      "Epoch: 0131 cost= 0.164725585\n",
      "Epoch: 0136 cost= 0.162451983\n",
      "Epoch: 0141 cost= 0.159494925\n",
      "Epoch: 0146 cost= 0.158294753\n",
      "Epoch: 0151 cost= 0.156160998\n",
      "Epoch: 0156 cost= 0.154613890\n",
      "Epoch: 0161 cost= 0.152691672\n",
      "Epoch: 0166 cost= 0.152608267\n",
      "Epoch: 0171 cost= 0.152284241\n",
      "Epoch: 0176 cost= 0.151345862\n",
      "Epoch: 0181 cost= 0.150709689\n",
      "Epoch: 0186 cost= 0.150094374\n",
      "Epoch: 0191 cost= 0.149940070\n",
      "Epoch: 0196 cost= 0.149574827\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.549139053\n",
      "Epoch: 0006 cost= 0.182195462\n",
      "Epoch: 0011 cost= 0.151968590\n",
      "Epoch: 0016 cost= 0.140694776\n",
      "Epoch: 0021 cost= 0.134018843\n",
      "Epoch: 0026 cost= 0.130128668\n",
      "Epoch: 0031 cost= 0.127844725\n",
      "Epoch: 0036 cost= 0.125943361\n",
      "Epoch: 0041 cost= 0.125477076\n",
      "Epoch: 0046 cost= 0.124940031\n",
      "Epoch: 0051 cost= 0.124450348\n",
      "Epoch: 0056 cost= 0.124689963\n",
      "Epoch: 0061 cost= 0.125148326\n",
      "Epoch: 0066 cost= 0.124121028\n",
      "Epoch: 0071 cost= 0.124454721\n",
      "Epoch: 0076 cost= 0.124410178\n",
      "Epoch: 0081 cost= 0.123630374\n",
      "Epoch: 0086 cost= 0.123895487\n",
      "Epoch: 0091 cost= 0.123701218\n",
      "Epoch: 0096 cost= 0.124440430\n",
      "Epoch: 0101 cost= 0.124109060\n",
      "Epoch: 0106 cost= 0.124405065\n",
      "Epoch: 0111 cost= 0.124157547\n",
      "Epoch: 0116 cost= 0.124142302\n",
      "Epoch: 0121 cost= 0.123994913\n",
      "Epoch: 0126 cost= 0.124226910\n",
      "Epoch: 0131 cost= 0.124120192\n",
      "Epoch: 0136 cost= 0.124124255\n",
      "Epoch: 0141 cost= 0.123808450\n",
      "Epoch: 0146 cost= 0.123898859\n",
      "Epoch: 0151 cost= 0.124106843\n",
      "Epoch: 0156 cost= 0.124062028\n",
      "Epoch: 0161 cost= 0.123728486\n",
      "Epoch: 0166 cost= 0.123756384\n",
      "Epoch: 0171 cost= 0.123855213\n",
      "Epoch: 0176 cost= 0.123565938\n",
      "Epoch: 0181 cost= 0.123686180\n",
      "Epoch: 0186 cost= 0.123953861\n",
      "Epoch: 0191 cost= 0.123896467\n",
      "Epoch: 0196 cost= 0.123719963\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.768144125\n",
      "Epoch: 0006 cost= 0.227882951\n",
      "Epoch: 0011 cost= 0.194666759\n",
      "Epoch: 0016 cost= 0.181153732\n",
      "Epoch: 0021 cost= 0.171250216\n",
      "Epoch: 0026 cost= 0.167893554\n",
      "Epoch: 0031 cost= 0.165273286\n",
      "Epoch: 0036 cost= 0.163421996\n",
      "Epoch: 0041 cost= 0.162409346\n",
      "Epoch: 0046 cost= 0.162170366\n",
      "Epoch: 0051 cost= 0.161252778\n",
      "Epoch: 0056 cost= 0.161020687\n",
      "Epoch: 0061 cost= 0.160763617\n",
      "Epoch: 0066 cost= 0.161194256\n",
      "Epoch: 0071 cost= 0.160653978\n",
      "Epoch: 0076 cost= 0.160836196\n",
      "Epoch: 0081 cost= 0.160391228\n",
      "Epoch: 0086 cost= 0.160958641\n",
      "Epoch: 0091 cost= 0.160747313\n",
      "Epoch: 0096 cost= 0.160626785\n",
      "Epoch: 0101 cost= 0.160877973\n",
      "Epoch: 0106 cost= 0.160955222\n",
      "Epoch: 0111 cost= 0.160615734\n",
      "Epoch: 0116 cost= 0.160477627\n",
      "Epoch: 0121 cost= 0.161153320\n",
      "Epoch: 0126 cost= 0.160951872\n",
      "Epoch: 0131 cost= 0.160499591\n",
      "Epoch: 0136 cost= 0.160914113\n",
      "Epoch: 0141 cost= 0.161017247\n",
      "Epoch: 0146 cost= 0.160724063\n",
      "Epoch: 0151 cost= 0.160242413\n",
      "Epoch: 0156 cost= 0.159916569\n",
      "Epoch: 0161 cost= 0.160712862\n",
      "Epoch: 0166 cost= 0.160658964\n",
      "Epoch: 0171 cost= 0.160674055\n",
      "Epoch: 0176 cost= 0.160794129\n",
      "Epoch: 0181 cost= 0.160698009\n",
      "Epoch: 0186 cost= 0.160393589\n",
      "Epoch: 0191 cost= 0.160809578\n",
      "Epoch: 0196 cost= 0.160350758\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.518382504\n",
      "Epoch: 0006 cost= 0.192418201\n",
      "Epoch: 0011 cost= 0.171589398\n",
      "Epoch: 0016 cost= 0.161024713\n",
      "Epoch: 0021 cost= 0.153688529\n",
      "Epoch: 0026 cost= 0.149795554\n",
      "Epoch: 0031 cost= 0.147230422\n",
      "Epoch: 0036 cost= 0.146033548\n",
      "Epoch: 0041 cost= 0.144372183\n",
      "Epoch: 0046 cost= 0.144314818\n",
      "Epoch: 0051 cost= 0.143973889\n",
      "Epoch: 0056 cost= 0.143803889\n",
      "Epoch: 0061 cost= 0.143543339\n",
      "Epoch: 0066 cost= 0.143533939\n",
      "Epoch: 0071 cost= 0.143238995\n",
      "Epoch: 0076 cost= 0.143036269\n",
      "Epoch: 0081 cost= 0.143151464\n",
      "Epoch: 0086 cost= 0.143346318\n",
      "Epoch: 0091 cost= 0.143493858\n",
      "Epoch: 0096 cost= 0.143030965\n",
      "Epoch: 0101 cost= 0.143369562\n",
      "Epoch: 0106 cost= 0.142844502\n",
      "Epoch: 0111 cost= 0.143491943\n",
      "Epoch: 0116 cost= 0.143606845\n",
      "Epoch: 0121 cost= 0.143332983\n",
      "Epoch: 0126 cost= 0.142931005\n",
      "Epoch: 0131 cost= 0.143366663\n",
      "Epoch: 0136 cost= 0.143239561\n",
      "Epoch: 0141 cost= 0.143707754\n",
      "Epoch: 0146 cost= 0.143176943\n",
      "Epoch: 0151 cost= 0.143169613\n",
      "Epoch: 0156 cost= 0.143380965\n",
      "Epoch: 0161 cost= 0.143129341\n",
      "Epoch: 0166 cost= 0.142985534\n",
      "Epoch: 0171 cost= 0.142920428\n",
      "Epoch: 0176 cost= 0.142998443\n",
      "Epoch: 0181 cost= 0.143566170\n",
      "Epoch: 0186 cost= 0.143209523\n",
      "Epoch: 0191 cost= 0.143617406\n",
      "Epoch: 0196 cost= 0.143222420\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.817847928\n",
      "Epoch: 0006 cost= 0.286739393\n",
      "Epoch: 0011 cost= 0.250512775\n",
      "Epoch: 0016 cost= 0.237838640\n",
      "Epoch: 0021 cost= 0.232146363\n",
      "Epoch: 0026 cost= 0.229445031\n",
      "Epoch: 0031 cost= 0.228144495\n",
      "Epoch: 0036 cost= 0.225952066\n",
      "Epoch: 0041 cost= 0.226024721\n",
      "Epoch: 0046 cost= 0.225578575\n",
      "Epoch: 0051 cost= 0.225522597\n",
      "Epoch: 0056 cost= 0.225293137\n",
      "Epoch: 0061 cost= 0.225311390\n",
      "Epoch: 0066 cost= 0.225486551\n",
      "Epoch: 0071 cost= 0.225047065\n",
      "Epoch: 0076 cost= 0.225070295\n",
      "Epoch: 0081 cost= 0.224810720\n",
      "Epoch: 0086 cost= 0.225191424\n",
      "Epoch: 0091 cost= 0.224505342\n",
      "Epoch: 0096 cost= 0.225400948\n",
      "Epoch: 0101 cost= 0.225068375\n",
      "Epoch: 0106 cost= 0.224493950\n",
      "Epoch: 0111 cost= 0.224924033\n",
      "Epoch: 0116 cost= 0.224771319\n",
      "Epoch: 0121 cost= 0.225237221\n",
      "Epoch: 0126 cost= 0.224561629\n",
      "Epoch: 0131 cost= 0.224946979\n",
      "Epoch: 0136 cost= 0.225622264\n",
      "Epoch: 0141 cost= 0.224987106\n",
      "Epoch: 0146 cost= 0.225797943\n",
      "Epoch: 0151 cost= 0.224864238\n",
      "Epoch: 0156 cost= 0.225307693\n",
      "Epoch: 0161 cost= 0.225218839\n",
      "Epoch: 0166 cost= 0.224878131\n",
      "Epoch: 0171 cost= 0.224982657\n",
      "Epoch: 0176 cost= 0.224528958\n",
      "Epoch: 0181 cost= 0.224734261\n",
      "Epoch: 0186 cost= 0.225506862\n",
      "Epoch: 0191 cost= 0.225109734\n",
      "Epoch: 0196 cost= 0.225111200\n",
      "Optimization Finished!\n",
      "[[0.9845    nan    nan    nan    nan]\n",
      " [0.952  0.985     nan    nan    nan]\n",
      " [0.9055 0.937  0.9745    nan    nan]\n",
      " [0.9125 0.913  0.945  0.975     nan]\n",
      " [0.864  0.926  0.8555 0.9205 0.954 ]]\n"
     ]
    }
   ],
   "source": [
    "# Run Vanilla VCL\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "hidden_size = [5]\n",
    "coreset_size = 0\n",
    "\n",
    "data_gen = NotMnistGenerator()\n",
    "vcl_result_h5 = run_vcl(hidden_size, no_epochs, data_gen, \n",
    "                     lambda a: a, coreset_size, batch_size, single_head, val=True)\n",
    "print(vcl_result_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.153718561\n",
      "Epoch: 0006 cost= 0.061457908\n",
      "Epoch: 0011 cost= 0.037431205\n",
      "Epoch: 0016 cost= 0.022582043\n",
      "Epoch: 0021 cost= 0.017541323\n",
      "Epoch: 0026 cost= 0.012438207\n",
      "Epoch: 0031 cost= 0.010182058\n",
      "Epoch: 0036 cost= 0.009672760\n",
      "Epoch: 0041 cost= 0.007883243\n",
      "Epoch: 0046 cost= 0.007413055\n",
      "Epoch: 0051 cost= 0.009715796\n",
      "Epoch: 0056 cost= 0.007014894\n",
      "Epoch: 0061 cost= 0.009667960\n",
      "Epoch: 0066 cost= 0.007403898\n",
      "Epoch: 0071 cost= 0.007036378\n",
      "Epoch: 0076 cost= 0.006717535\n",
      "Epoch: 0081 cost= 0.006098075\n",
      "Epoch: 0086 cost= 0.007884045\n",
      "Epoch: 0091 cost= 0.005445113\n",
      "Epoch: 0096 cost= 0.007306467\n",
      "Epoch: 0101 cost= 0.007086758\n",
      "Epoch: 0106 cost= 0.004177887\n",
      "Epoch: 0111 cost= 0.007087358\n",
      "Epoch: 0116 cost= 0.003494912\n",
      "Epoch: 0121 cost= 0.005541459\n",
      "Epoch: 0126 cost= 0.005622239\n",
      "Epoch: 0131 cost= 0.005170572\n",
      "Epoch: 0136 cost= 0.004932709\n",
      "Epoch: 0141 cost= 0.005369881\n",
      "Epoch: 0146 cost= 0.005383066\n",
      "Epoch: 0151 cost= 0.005779357\n",
      "Epoch: 0156 cost= 0.004426445\n",
      "Epoch: 0161 cost= 0.004463694\n",
      "Epoch: 0166 cost= 0.005516784\n",
      "Epoch: 0171 cost= 0.004373270\n",
      "Epoch: 0176 cost= 0.004360532\n",
      "Epoch: 0181 cost= 0.004940671\n",
      "Epoch: 0186 cost= 0.004359924\n",
      "Epoch: 0191 cost= 0.004573512\n",
      "Epoch: 0196 cost= 0.003327583\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 2.430952279\n",
      "Epoch: 0006 cost= 1.820938481\n",
      "Epoch: 0011 cost= 1.471879939\n",
      "Epoch: 0016 cost= 1.253862028\n",
      "Epoch: 0021 cost= 1.073704530\n",
      "Epoch: 0026 cost= 0.900286006\n",
      "Epoch: 0031 cost= 0.733901633\n",
      "Epoch: 0036 cost= 0.583204236\n",
      "Epoch: 0041 cost= 0.460054348\n",
      "Epoch: 0046 cost= 0.365313535\n",
      "Epoch: 0051 cost= 0.296358233\n",
      "Epoch: 0056 cost= 0.251883552\n",
      "Epoch: 0061 cost= 0.222917912\n",
      "Epoch: 0066 cost= 0.203902308\n",
      "Epoch: 0071 cost= 0.191728673\n",
      "Epoch: 0076 cost= 0.183654894\n",
      "Epoch: 0081 cost= 0.179158730\n",
      "Epoch: 0086 cost= 0.174998139\n",
      "Epoch: 0091 cost= 0.173311496\n",
      "Epoch: 0096 cost= 0.172608913\n",
      "Epoch: 0101 cost= 0.170390488\n",
      "Epoch: 0106 cost= 0.169941667\n",
      "Epoch: 0111 cost= 0.168743992\n",
      "Epoch: 0116 cost= 0.168768789\n",
      "Epoch: 0121 cost= 0.168086385\n",
      "Epoch: 0126 cost= 0.167446822\n",
      "Epoch: 0131 cost= 0.167849584\n",
      "Epoch: 0136 cost= 0.167550016\n",
      "Epoch: 0141 cost= 0.167923915\n",
      "Epoch: 0146 cost= 0.167019466\n",
      "Epoch: 0151 cost= 0.166598634\n",
      "Epoch: 0156 cost= 0.166804681\n",
      "Epoch: 0161 cost= 0.167272607\n",
      "Epoch: 0166 cost= 0.165966142\n",
      "Epoch: 0171 cost= 0.165845817\n",
      "Epoch: 0176 cost= 0.166083273\n",
      "Epoch: 0181 cost= 0.165730807\n",
      "Epoch: 0186 cost= 0.165224022\n",
      "Epoch: 0191 cost= 0.165674912\n",
      "Epoch: 0196 cost= 0.165853340\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.701123941\n",
      "Epoch: 0006 cost= 0.225769205\n",
      "Epoch: 0011 cost= 0.184359114\n",
      "Epoch: 0016 cost= 0.164439623\n",
      "Epoch: 0021 cost= 0.150444740\n",
      "Epoch: 0026 cost= 0.143351124\n",
      "Epoch: 0031 cost= 0.137805217\n",
      "Epoch: 0036 cost= 0.133069448\n",
      "Epoch: 0041 cost= 0.131045980\n",
      "Epoch: 0046 cost= 0.128540470\n",
      "Epoch: 0051 cost= 0.127112188\n",
      "Epoch: 0056 cost= 0.127180481\n",
      "Epoch: 0061 cost= 0.126472855\n",
      "Epoch: 0066 cost= 0.126665782\n",
      "Epoch: 0071 cost= 0.126216661\n",
      "Epoch: 0076 cost= 0.125839631\n",
      "Epoch: 0081 cost= 0.126017333\n",
      "Epoch: 0086 cost= 0.124733288\n",
      "Epoch: 0091 cost= 0.125234934\n",
      "Epoch: 0096 cost= 0.125770310\n",
      "Epoch: 0101 cost= 0.125066177\n",
      "Epoch: 0106 cost= 0.126018644\n",
      "Epoch: 0111 cost= 0.125086671\n",
      "Epoch: 0116 cost= 0.125841327\n",
      "Epoch: 0121 cost= 0.125570565\n",
      "Epoch: 0126 cost= 0.125802708\n",
      "Epoch: 0131 cost= 0.124983457\n",
      "Epoch: 0136 cost= 0.125333989\n",
      "Epoch: 0141 cost= 0.125235846\n",
      "Epoch: 0146 cost= 0.124988638\n",
      "Epoch: 0151 cost= 0.125800790\n",
      "Epoch: 0156 cost= 0.125131256\n",
      "Epoch: 0161 cost= 0.125222501\n",
      "Epoch: 0166 cost= 0.125035212\n",
      "Epoch: 0171 cost= 0.125648896\n",
      "Epoch: 0176 cost= 0.125609944\n",
      "Epoch: 0181 cost= 0.125496103\n",
      "Epoch: 0186 cost= 0.125029759\n",
      "Epoch: 0191 cost= 0.125752094\n",
      "Epoch: 0196 cost= 0.124629804\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.743781712\n",
      "Epoch: 0006 cost= 0.280186551\n",
      "Epoch: 0011 cost= 0.228596028\n",
      "Epoch: 0016 cost= 0.206928533\n",
      "Epoch: 0021 cost= 0.194608862\n",
      "Epoch: 0026 cost= 0.184203096\n",
      "Epoch: 0031 cost= 0.178471387\n",
      "Epoch: 0036 cost= 0.174080906\n",
      "Epoch: 0041 cost= 0.171625208\n",
      "Epoch: 0046 cost= 0.169027768\n",
      "Epoch: 0051 cost= 0.167686150\n",
      "Epoch: 0056 cost= 0.166559349\n",
      "Epoch: 0061 cost= 0.165995287\n",
      "Epoch: 0066 cost= 0.166511933\n",
      "Epoch: 0071 cost= 0.165951025\n",
      "Epoch: 0076 cost= 0.166507659\n",
      "Epoch: 0081 cost= 0.164467995\n",
      "Epoch: 0086 cost= 0.165394064\n",
      "Epoch: 0091 cost= 0.164939674\n",
      "Epoch: 0096 cost= 0.165193216\n",
      "Epoch: 0101 cost= 0.164692205\n",
      "Epoch: 0106 cost= 0.164440583\n",
      "Epoch: 0111 cost= 0.164558340\n",
      "Epoch: 0116 cost= 0.165438422\n",
      "Epoch: 0121 cost= 0.164280385\n",
      "Epoch: 0126 cost= 0.164568392\n",
      "Epoch: 0131 cost= 0.164598754\n",
      "Epoch: 0136 cost= 0.164466413\n",
      "Epoch: 0141 cost= 0.164103889\n",
      "Epoch: 0146 cost= 0.164571696\n",
      "Epoch: 0151 cost= 0.164106461\n",
      "Epoch: 0156 cost= 0.163796036\n",
      "Epoch: 0161 cost= 0.164189434\n",
      "Epoch: 0166 cost= 0.164397328\n",
      "Epoch: 0171 cost= 0.164579317\n",
      "Epoch: 0176 cost= 0.164039983\n",
      "Epoch: 0181 cost= 0.165428732\n",
      "Epoch: 0186 cost= 0.164491498\n",
      "Epoch: 0191 cost= 0.163674699\n",
      "Epoch: 0196 cost= 0.163815274\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 1.048783926\n",
      "Epoch: 0006 cost= 0.271859669\n",
      "Epoch: 0011 cost= 0.222094969\n",
      "Epoch: 0016 cost= 0.194965658\n",
      "Epoch: 0021 cost= 0.180337692\n",
      "Epoch: 0026 cost= 0.171205775\n",
      "Epoch: 0031 cost= 0.162785304\n",
      "Epoch: 0036 cost= 0.157912535\n",
      "Epoch: 0041 cost= 0.153737614\n",
      "Epoch: 0046 cost= 0.150858896\n",
      "Epoch: 0051 cost= 0.149071467\n",
      "Epoch: 0056 cost= 0.148238694\n",
      "Epoch: 0061 cost= 0.146962681\n",
      "Epoch: 0066 cost= 0.146206788\n",
      "Epoch: 0071 cost= 0.145969785\n",
      "Epoch: 0076 cost= 0.145824942\n",
      "Epoch: 0081 cost= 0.146736996\n",
      "Epoch: 0086 cost= 0.145736856\n",
      "Epoch: 0091 cost= 0.145089909\n",
      "Epoch: 0096 cost= 0.145954632\n",
      "Epoch: 0101 cost= 0.145685964\n",
      "Epoch: 0106 cost= 0.144354567\n",
      "Epoch: 0111 cost= 0.144529063\n",
      "Epoch: 0116 cost= 0.145850586\n",
      "Epoch: 0121 cost= 0.145081612\n",
      "Epoch: 0126 cost= 0.144609468\n",
      "Epoch: 0131 cost= 0.144704545\n",
      "Epoch: 0136 cost= 0.145210538\n",
      "Epoch: 0141 cost= 0.144987438\n",
      "Epoch: 0146 cost= 0.144768096\n",
      "Epoch: 0151 cost= 0.144926905\n",
      "Epoch: 0156 cost= 0.145197387\n",
      "Epoch: 0161 cost= 0.144293795\n",
      "Epoch: 0166 cost= 0.144308622\n",
      "Epoch: 0171 cost= 0.144296086\n",
      "Epoch: 0176 cost= 0.144495326\n",
      "Epoch: 0181 cost= 0.144282895\n",
      "Epoch: 0186 cost= 0.143984848\n",
      "Epoch: 0191 cost= 0.144811016\n",
      "Epoch: 0196 cost= 0.144575974\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 1.264545427\n",
      "Epoch: 0006 cost= 0.394137138\n",
      "Epoch: 0011 cost= 0.322871314\n",
      "Epoch: 0016 cost= 0.285931300\n",
      "Epoch: 0021 cost= 0.263980540\n",
      "Epoch: 0026 cost= 0.252877993\n",
      "Epoch: 0031 cost= 0.244092295\n",
      "Epoch: 0036 cost= 0.236194810\n",
      "Epoch: 0041 cost= 0.232335962\n",
      "Epoch: 0046 cost= 0.228538734\n",
      "Epoch: 0051 cost= 0.226936305\n",
      "Epoch: 0056 cost= 0.223682604\n",
      "Epoch: 0061 cost= 0.224088696\n",
      "Epoch: 0066 cost= 0.221250482\n",
      "Epoch: 0071 cost= 0.221170430\n",
      "Epoch: 0076 cost= 0.220155046\n",
      "Epoch: 0081 cost= 0.220622075\n",
      "Epoch: 0086 cost= 0.219909639\n",
      "Epoch: 0091 cost= 0.219974744\n",
      "Epoch: 0096 cost= 0.219962024\n",
      "Epoch: 0101 cost= 0.219997680\n",
      "Epoch: 0106 cost= 0.219715810\n",
      "Epoch: 0111 cost= 0.219088401\n",
      "Epoch: 0116 cost= 0.218930295\n",
      "Epoch: 0121 cost= 0.219585139\n",
      "Epoch: 0126 cost= 0.219589936\n",
      "Epoch: 0131 cost= 0.218399847\n",
      "Epoch: 0136 cost= 0.219161437\n",
      "Epoch: 0141 cost= 0.218946670\n",
      "Epoch: 0146 cost= 0.218758457\n",
      "Epoch: 0151 cost= 0.218782380\n",
      "Epoch: 0156 cost= 0.219976436\n",
      "Epoch: 0161 cost= 0.218173284\n",
      "Epoch: 0166 cost= 0.218770622\n",
      "Epoch: 0171 cost= 0.218927515\n",
      "Epoch: 0176 cost= 0.219465039\n",
      "Epoch: 0181 cost= 0.219865549\n",
      "Epoch: 0186 cost= 0.218625975\n",
      "Epoch: 0191 cost= 0.219787385\n",
      "Epoch: 0196 cost= 0.219174424\n",
      "Optimization Finished!\n",
      "[[0.985     nan    nan    nan    nan]\n",
      " [0.9715 0.986     nan    nan    nan]\n",
      " [0.937  0.9785 0.9795    nan    nan]\n",
      " [0.92   0.966  0.977  0.9805    nan]\n",
      " [0.8805 0.9635 0.9225 0.9745 0.96  ]]\n"
     ]
    }
   ],
   "source": [
    "# Run Vanilla VCL\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "hidden_size = [50]\n",
    "coreset_size = 0\n",
    "\n",
    "data_gen = NotMnistGenerator()\n",
    "vcl_result_h50 = run_vcl(hidden_size, no_epochs, data_gen, \n",
    "                     lambda a: a, coreset_size, batch_size, single_head, val=True)\n",
    "print(vcl_result_h50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/figure.py:445: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ibp_acc = np.nanmean(ibp_acc, 1)\n",
    "_vcl_result_h10 = np.nanmean(vcl_result, 1)\n",
    "_vcl_result_h5 = np.nanmean(vcl_result_h5, 1)\n",
    "_vcl_result_h50 = np.nanmean(vcl_result_h50, 1)\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = plt.gca()\n",
    "plt.plot(np.arange(len(_ibp_acc))+1, _ibp_acc, label='VCL + IBP', marker='o')\n",
    "plt.plot(np.arange(len(_vcl_result))+1, _vcl_result_h10, label='VCL h10', marker='o')\n",
    "plt.plot(np.arange(len(_vcl_result))+1, _vcl_result_h5, label='VCL h5', marker='o')\n",
    "plt.plot(np.arange(len(_vcl_result))+1, _vcl_result_h50, label='VCL h50', marker='o')\n",
    "ax.set_xticks(range(1, len(_ibp_acc)+1))\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('\\# tasks')\n",
    "ax.set_title('Split notMnist')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
