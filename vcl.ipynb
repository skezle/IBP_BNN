{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 20:05:56.469430 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:9: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0821 20:05:56.470671 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:13: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "import pdb\n",
    "import re\n",
    "from ddm.run_split import SplitMnistGenerator\n",
    "from ddm.run_not import NotMnistGenerator\n",
    "from ddm.alg.cla_models_multihead import MFVI_IBP_NN, Vanilla_NN\n",
    "from ddm.alg.utils import get_scores, concatenate_results\n",
    "from ddm.alg.vcl import run_vcl\n",
    "from copy import deepcopy\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:14.920098 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0820 10:28:14.928613 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:177: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0820 10:28:15.041888 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:61: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0820 10:28:15.235116 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:65: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.054045935\n",
      "Epoch: 0006 cost= 0.000989384\n",
      "Epoch: 0011 cost= 0.000217877\n",
      "Epoch: 0016 cost= 0.000095243\n",
      "Epoch: 0021 cost= 0.000053343\n",
      "Epoch: 0026 cost= 0.000032177\n",
      "Epoch: 0031 cost= 0.000020990\n",
      "Epoch: 0036 cost= 0.000013812\n",
      "Epoch: 0041 cost= 0.000010000\n",
      "Epoch: 0046 cost= 0.000007089\n",
      "Epoch: 0051 cost= 0.000005146\n",
      "Epoch: 0056 cost= 0.000003880\n",
      "Epoch: 0061 cost= 0.000002915\n",
      "Epoch: 0066 cost= 0.000002253\n",
      "Epoch: 0071 cost= 0.000001736\n",
      "Epoch: 0076 cost= 0.000001336\n",
      "Epoch: 0081 cost= 0.000001051\n",
      "Epoch: 0086 cost= 0.000000806\n",
      "Epoch: 0091 cost= 0.000000640\n",
      "Epoch: 0096 cost= 0.000000494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:39.117458 140357229651776 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:567: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:39.556495 140357229651776 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:520: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:40.307626 140357229651776 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0820 10:28:42.101294 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:527: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0820 10:28:42.166632 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:662: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0820 10:28:42.207280 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:671: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0820 10:28:42.330168 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:678: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0820 10:28:42.337678 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:685: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Z: (1, ?, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:28:42.556347 140357229651776 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:955: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train cost= 22.979616349\n",
      "Epoch: 0006 train cost= 7.847686078\n",
      "Epoch: 0011 train cost= 4.484062356\n",
      "Epoch: 0016 train cost= 4.284478007\n",
      "Epoch: 0021 train cost= 4.183985552\n",
      "Epoch: 0026 train cost= 4.085030877\n",
      "Epoch: 0031 train cost= 4.028940247\n",
      "Epoch: 0036 train cost= 3.973704306\n",
      "Epoch: 0041 train cost= 3.931877205\n",
      "Epoch: 0046 train cost= 3.905022799\n",
      "Epoch: 0051 train cost= 3.867956587\n",
      "Epoch: 0056 train cost= 3.860971652\n",
      "Epoch: 0061 train cost= 3.825531069\n",
      "Epoch: 0066 train cost= 3.800201376\n",
      "Epoch: 0071 train cost= 3.808827998\n",
      "Epoch: 0076 train cost= 3.780617613\n",
      "Epoch: 0081 train cost= 3.751397926\n",
      "Epoch: 0086 train cost= 3.753835310\n",
      "Epoch: 0091 train cost= 3.723489175\n",
      "Epoch: 0096 train cost= 3.734567102\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 2.041086149\n",
      "Epoch: 0006 train cost= 1.672260149\n",
      "Epoch: 0011 train cost= 1.626302651\n",
      "Epoch: 0016 train cost= 1.594602039\n",
      "Epoch: 0021 train cost= 1.577647405\n",
      "Epoch: 0026 train cost= 1.556065313\n",
      "Epoch: 0031 train cost= 1.545514980\n",
      "Epoch: 0036 train cost= 1.533725304\n",
      "Epoch: 0041 train cost= 1.520413913\n",
      "Epoch: 0046 train cost= 1.513239266\n",
      "Epoch: 0051 train cost= 1.502713502\n",
      "Epoch: 0056 train cost= 1.483722706\n",
      "Epoch: 0061 train cost= 1.480324591\n",
      "Epoch: 0066 train cost= 1.467855182\n",
      "Epoch: 0071 train cost= 1.464583448\n",
      "Epoch: 0076 train cost= 1.458046335\n",
      "Epoch: 0081 train cost= 1.455978492\n",
      "Epoch: 0086 train cost= 1.453517416\n",
      "Epoch: 0091 train cost= 1.445287403\n",
      "Epoch: 0096 train cost= 1.433009645\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 1.579997695\n",
      "Epoch: 0006 train cost= 1.341037467\n",
      "Epoch: 0011 train cost= 1.326953120\n",
      "Epoch: 0016 train cost= 1.314823799\n",
      "Epoch: 0021 train cost= 1.306956675\n",
      "Epoch: 0026 train cost= 1.302121327\n",
      "Epoch: 0031 train cost= 1.291707468\n",
      "Epoch: 0036 train cost= 1.294732140\n",
      "Epoch: 0041 train cost= 1.290278093\n",
      "Epoch: 0046 train cost= 1.279487659\n",
      "Epoch: 0051 train cost= 1.274208863\n",
      "Epoch: 0056 train cost= 1.267571174\n",
      "Epoch: 0061 train cost= 1.269242670\n",
      "Epoch: 0066 train cost= 1.263228418\n",
      "Epoch: 0071 train cost= 1.260797440\n",
      "Epoch: 0076 train cost= 1.259605261\n",
      "Epoch: 0081 train cost= 1.252451645\n",
      "Epoch: 0086 train cost= 1.254834579\n",
      "Epoch: 0091 train cost= 1.250740369\n",
      "Epoch: 0096 train cost= 1.243335017\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 1.484987153\n",
      "Epoch: 0006 train cost= 1.204871920\n",
      "Epoch: 0011 train cost= 1.185894356\n",
      "Epoch: 0016 train cost= 1.185475276\n",
      "Epoch: 0021 train cost= 1.181127488\n",
      "Epoch: 0026 train cost= 1.180518514\n",
      "Epoch: 0031 train cost= 1.177639461\n",
      "Epoch: 0036 train cost= 1.178248239\n",
      "Epoch: 0041 train cost= 1.170061696\n",
      "Epoch: 0046 train cost= 1.172773105\n",
      "Epoch: 0051 train cost= 1.168427238\n",
      "Epoch: 0056 train cost= 1.169774820\n",
      "Epoch: 0061 train cost= 1.161941421\n",
      "Epoch: 0066 train cost= 1.158429320\n",
      "Epoch: 0071 train cost= 1.159045209\n",
      "Epoch: 0076 train cost= 1.160654365\n",
      "Epoch: 0081 train cost= 1.151641416\n",
      "Epoch: 0086 train cost= 1.154243989\n",
      "Epoch: 0091 train cost= 1.152490561\n",
      "Epoch: 0096 train cost= 1.148243466\n",
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 train cost= 1.551890070\n",
      "Epoch: 0006 train cost= 1.395971133\n",
      "Epoch: 0011 train cost= 1.379279669\n",
      "Epoch: 0016 train cost= 1.380170680\n",
      "Epoch: 0021 train cost= 1.377598857\n",
      "Epoch: 0026 train cost= 1.375637954\n",
      "Epoch: 0031 train cost= 1.369959839\n",
      "Epoch: 0036 train cost= 1.364577871\n",
      "Epoch: 0041 train cost= 1.370788833\n",
      "Epoch: 0046 train cost= 1.368471565\n",
      "Epoch: 0051 train cost= 1.366206689\n",
      "Epoch: 0056 train cost= 1.360178633\n",
      "Epoch: 0061 train cost= 1.364964792\n",
      "Epoch: 0066 train cost= 1.359770708\n",
      "Epoch: 0071 train cost= 1.359749746\n",
      "Epoch: 0076 train cost= 1.359280961\n",
      "Epoch: 0081 train cost= 1.355742337\n",
      "Epoch: 0086 train cost= 1.353503842\n",
      "Epoch: 0091 train cost= 1.354933672\n",
      "Epoch: 0096 train cost= 1.357612789\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99756691,        nan,        nan,        nan,        nan],\n",
       "       [0.52895377, 0.97425743,        nan,        nan,        nan],\n",
       "       [0.62871046, 0.87722772, 0.971549  ,        nan,        nan],\n",
       "       [0.54257908, 0.79158416, 0.97312961, 0.98833252,        nan],\n",
       "       [0.52652068, 0.61485149, 0.95574289, 0.98492951, 0.97664975]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 100\n",
    "alpha0 = 1.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "ibp_samples = 10\n",
    "\n",
    "# Run vanilla VCL\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = True\n",
    "data_gen = SplitMnistGenerator(val)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN(in_dim, hidden_size, out_dim, x_train.shape[0], num_ibp_samples=ibp_samples,\n",
    "                           prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, lambda_1=tau0, lambda_2=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "    acc = get_scores(mf_model, x_valsets, y_valsets, single_head)\n",
    "    ibp_acc = concatenate_results(acc, ibp_acc)\n",
    "    \n",
    "    mf_model.close_session()\n",
    "    \n",
    "ibp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.260041745\n",
      "Epoch: 0006 cost= 0.005151312\n",
      "Epoch: 0011 cost= 0.002609195\n",
      "Epoch: 0016 cost= 0.001677913\n",
      "Epoch: 0021 cost= 0.001073658\n",
      "Epoch: 0026 cost= 0.000651595\n",
      "Epoch: 0031 cost= 0.000435936\n",
      "Epoch: 0036 cost= 0.000280965\n",
      "Epoch: 0041 cost= 0.000201473\n",
      "Epoch: 0046 cost= 0.000137499\n",
      "Epoch: 0051 cost= 0.000096778\n",
      "Epoch: 0056 cost= 0.000071720\n",
      "Epoch: 0061 cost= 0.000053018\n",
      "Epoch: 0066 cost= 0.000040704\n",
      "Epoch: 0071 cost= 0.000031381\n",
      "Epoch: 0076 cost= 0.000023424\n",
      "Epoch: 0081 cost= 0.000018736\n",
      "Epoch: 0086 cost= 0.000013911\n",
      "Epoch: 0091 cost= 0.000010898\n",
      "Epoch: 0096 cost= 0.000008418\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 1.846670676\n",
      "Epoch: 0006 cost= 1.689760324\n",
      "Epoch: 0011 cost= 1.536770825\n",
      "Epoch: 0016 cost= 1.385002039\n",
      "Epoch: 0021 cost= 1.234384163\n",
      "Epoch: 0026 cost= 1.086196825\n",
      "Epoch: 0031 cost= 0.940980348\n",
      "Epoch: 0036 cost= 0.800019918\n",
      "Epoch: 0041 cost= 0.664992467\n",
      "Epoch: 0046 cost= 0.537858195\n",
      "Epoch: 0051 cost= 0.420864664\n",
      "Epoch: 0056 cost= 0.315800496\n",
      "Epoch: 0061 cost= 0.227134357\n",
      "Epoch: 0066 cost= 0.156525758\n",
      "Epoch: 0071 cost= 0.107231790\n",
      "Epoch: 0076 cost= 0.074533225\n",
      "Epoch: 0081 cost= 0.055905868\n",
      "Epoch: 0086 cost= 0.046409810\n",
      "Epoch: 0091 cost= 0.041310130\n",
      "Epoch: 0096 cost= 0.036952582\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.783298159\n",
      "Epoch: 0006 cost= 0.254706398\n",
      "Epoch: 0011 cost= 0.186603178\n",
      "Epoch: 0016 cost= 0.169851414\n",
      "Epoch: 0021 cost= 0.156130277\n",
      "Epoch: 0026 cost= 0.145390899\n",
      "Epoch: 0031 cost= 0.143236202\n",
      "Epoch: 0036 cost= 0.135447690\n",
      "Epoch: 0041 cost= 0.131772337\n",
      "Epoch: 0046 cost= 0.129090687\n",
      "Epoch: 0051 cost= 0.127712758\n",
      "Epoch: 0056 cost= 0.125231992\n",
      "Epoch: 0061 cost= 0.123649791\n",
      "Epoch: 0066 cost= 0.124717583\n",
      "Epoch: 0071 cost= 0.120561787\n",
      "Epoch: 0076 cost= 0.119413396\n",
      "Epoch: 0081 cost= 0.117522474\n",
      "Epoch: 0086 cost= 0.118759446\n",
      "Epoch: 0091 cost= 0.115632451\n",
      "Epoch: 0096 cost= 0.115514013\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.721903037\n",
      "Epoch: 0006 cost= 0.178758990\n",
      "Epoch: 0011 cost= 0.114722312\n",
      "Epoch: 0016 cost= 0.090272589\n",
      "Epoch: 0021 cost= 0.080876312\n",
      "Epoch: 0026 cost= 0.077547105\n",
      "Epoch: 0031 cost= 0.074734857\n",
      "Epoch: 0036 cost= 0.071093876\n",
      "Epoch: 0041 cost= 0.068447910\n",
      "Epoch: 0046 cost= 0.066602131\n",
      "Epoch: 0051 cost= 0.067853554\n",
      "Epoch: 0056 cost= 0.066697769\n",
      "Epoch: 0061 cost= 0.067348756\n",
      "Epoch: 0066 cost= 0.066392783\n",
      "Epoch: 0071 cost= 0.064257479\n",
      "Epoch: 0076 cost= 0.063865856\n",
      "Epoch: 0081 cost= 0.065018147\n",
      "Epoch: 0086 cost= 0.066137813\n",
      "Epoch: 0091 cost= 0.064019810\n",
      "Epoch: 0096 cost= 0.064602594\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.917744419\n",
      "Epoch: 0006 cost= 0.064089139\n",
      "Epoch: 0011 cost= 0.045433316\n",
      "Epoch: 0016 cost= 0.039231084\n",
      "Epoch: 0021 cost= 0.035372090\n",
      "Epoch: 0026 cost= 0.034368573\n",
      "Epoch: 0031 cost= 0.033419235\n",
      "Epoch: 0036 cost= 0.033916489\n",
      "Epoch: 0041 cost= 0.033761202\n",
      "Epoch: 0046 cost= 0.032982266\n",
      "Epoch: 0051 cost= 0.031784233\n",
      "Epoch: 0056 cost= 0.030813103\n",
      "Epoch: 0061 cost= 0.030423413\n",
      "Epoch: 0066 cost= 0.030501187\n",
      "Epoch: 0071 cost= 0.031372322\n",
      "Epoch: 0076 cost= 0.030543002\n",
      "Epoch: 0081 cost= 0.030356304\n",
      "Epoch: 0086 cost= 0.030751476\n",
      "Epoch: 0091 cost= 0.029227438\n",
      "Epoch: 0096 cost= 0.030815437\n",
      "Optimization Finished!\n",
      "Epoch: 0001 cost= 0.878556023\n",
      "Epoch: 0006 cost= 0.201092169\n",
      "Epoch: 0011 cost= 0.131515339\n",
      "Epoch: 0016 cost= 0.111028400\n",
      "Epoch: 0021 cost= 0.102196311\n",
      "Epoch: 0026 cost= 0.095191910\n",
      "Epoch: 0031 cost= 0.091067829\n",
      "Epoch: 0036 cost= 0.087748593\n",
      "Epoch: 0041 cost= 0.087145448\n",
      "Epoch: 0046 cost= 0.085484879\n",
      "Epoch: 0051 cost= 0.082004695\n",
      "Epoch: 0056 cost= 0.082447818\n",
      "Epoch: 0061 cost= 0.083955940\n",
      "Epoch: 0066 cost= 0.080973811\n",
      "Epoch: 0071 cost= 0.081272674\n",
      "Epoch: 0076 cost= 0.081227183\n",
      "Epoch: 0081 cost= 0.078943589\n",
      "Epoch: 0086 cost= 0.079895213\n",
      "Epoch: 0091 cost= 0.078648855\n",
      "Epoch: 0096 cost= 0.078743465\n",
      "Optimization Finished!\n",
      "[[0.99952719        nan        nan        nan        nan]\n",
      " [0.99054374 0.9877571         nan        nan        nan]\n",
      " [0.87044917 0.95200784 0.99893276        nan        nan]\n",
      " [0.89267139 0.98334966 0.99626467 0.9979859         nan]\n",
      " [0.87990544 0.85406464 0.99306297 0.98640483 0.98587998]]\n"
     ]
    }
   ],
   "source": [
    "# Run Vanilla VCL\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "hidden_size = [10]\n",
    "coreset_size = 0\n",
    "\n",
    "data_gen = SplitMnistGenerator(val=True)\n",
    "vcl_result = run_vcl(hidden_size, no_epochs, data_gen, \n",
    "                              lambda a: a, coreset_size, batch_size, single_head, val=True)\n",
    "print(vcl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99609375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data_gen.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(data_gen.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run IBP VCL\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(12)\n",
    "# np.random.seed(1)\n",
    "# coreset_size = 0\n",
    "\n",
    "# hidden_size = [50]\n",
    "# batch_size = 128\n",
    "# no_epochs = 100\n",
    "# alpha0 = 5.0\n",
    "# tau0=0.1 # initial temperature\n",
    "# temp_prior=1.0\n",
    "# ANNEAL_RATE=0.000\n",
    "# MIN_TEMP=0.1\n",
    "# single_head=False\n",
    "\n",
    "# # data_gen = SplitMnistGenerator()\n",
    "# # vcl_ibp_result = vcl.run_vcl_ibp(hidden_size=hidden_size, no_epochs=no_epochs, data_gen=data_gen,\n",
    "# #                                   batch_size=batch_size, single_head=single_head, alpha0=alpha0,\n",
    "# #                                   learning_rate=0.01, temp_prior=temp_prior, no_pred_samples=100,\n",
    "# #                                   tau0=tau0, tau_anneal_rate=ANNEAL_RATE, tau_min=MIN_TEMP)\n",
    "# # print(vcl_ibp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ibp_acc = np.nanmean(ibp_acc, 1)\n",
    "_vcl_result = np.nanmean(vcl_result, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vcl_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = plt.gca()\n",
    "plt.plot(np.arange(len(_ibp_acc))+1, _ibp_acc, label='VCL + IBP', marker='o')\n",
    "plt.plot(np.arange(len(_vcl_result))+1, _vcl_result, label='VCL', marker='o')\n",
    "ax.set_xticks(range(1, len(_ibp_acc)+1))\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('\\# tasks')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.146469993\n",
      "Epoch: 0006 cost= 0.051735276\n",
      "Epoch: 0011 cost= 0.028104113\n",
      "Epoch: 0016 cost= 0.015942755\n",
      "Epoch: 0021 cost= 0.015865596\n",
      "Epoch: 0026 cost= 0.011969231\n",
      "Epoch: 0031 cost= 0.009667647\n",
      "Epoch: 0036 cost= 0.009111372\n",
      "Epoch: 0041 cost= 0.008909420\n",
      "Epoch: 0046 cost= 0.008711578\n",
      "Epoch: 0051 cost= 0.010608444\n",
      "Epoch: 0056 cost= 0.008632005\n",
      "Epoch: 0061 cost= 0.011303626\n",
      "Epoch: 0066 cost= 0.006225984\n",
      "Epoch: 0071 cost= 0.005884801\n",
      "Epoch: 0076 cost= 0.005166127\n",
      "Epoch: 0081 cost= 0.006437723\n",
      "Epoch: 0086 cost= 0.006132824\n",
      "Epoch: 0091 cost= 0.006260679\n",
      "Epoch: 0096 cost= 0.006746725\n",
      "Epoch: 0101 cost= 0.005560117\n",
      "Epoch: 0106 cost= 0.005934628\n",
      "Epoch: 0111 cost= 0.005584506\n",
      "Epoch: 0116 cost= 0.003656060\n",
      "Epoch: 0121 cost= 0.004826352\n",
      "Epoch: 0126 cost= 0.004615627\n",
      "Epoch: 0131 cost= 0.004941612\n",
      "Epoch: 0136 cost= 0.003932573\n",
      "Epoch: 0141 cost= 0.005345335\n",
      "Epoch: 0146 cost= 0.006734761\n",
      "Epoch: 0151 cost= 0.005482487\n",
      "Epoch: 0156 cost= 0.004824669\n",
      "Epoch: 0161 cost= 0.005427741\n",
      "Epoch: 0166 cost= 0.005529169\n",
      "Epoch: 0171 cost= 0.003909187\n",
      "Epoch: 0176 cost= 0.003996661\n",
      "Epoch: 0181 cost= 0.004196767\n",
      "Epoch: 0186 cost= 0.005134154\n",
      "Epoch: 0191 cost= 0.006026610\n",
      "Epoch: 0196 cost= 0.003417335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 20:13:43.979037 140352637986624 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:575: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 20:13:44.308247 140352637986624 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:528: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 20:13:45.146842 140352637986624 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0821 20:13:46.593549 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:535: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0821 20:13:46.644495 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:670: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0821 20:13:46.668156 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:680: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0821 20:13:46.751312 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:687: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0821 20:13:46.756525 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:694: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Z: (1, ?, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 20:13:46.904784 140352637986624 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:965: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train cost= 5.202547515\n",
      "Epoch: 0006 train cost= 3.581586042\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 200\n",
    "ibp_samples = 10\n",
    "\n",
    "# Run vanilla VCL\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "data_gen = NotMnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "#x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    x_train, y_train, x_test, y_test, _, _ = data_gen.next_task()\n",
    "    #x_valsets.append(x_val)\n",
    "    #y_valsets.append(y_val)\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN(in_dim, hidden_size, out_dim, x_train.shape[0], num_ibp_samples=ibp_samples,\n",
    "                           prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas, \n",
    "                           alpha0=5., beta0=1.,\n",
    "                           learning_rate=0.001, lambda_1=1.0, lambda_2=1.0, no_pred_samples=100)\n",
    "\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=0.0, min_temp=1.0)\n",
    "    mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "    acc = get_scores(mf_model, x_testsets, y_testsets, single_head)\n",
    "    ibp_acc = concatenate_results(acc, ibp_acc)\n",
    "    \n",
    "    mf_model.close_session()\n",
    "    \n",
    "ibp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
