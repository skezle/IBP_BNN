{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 18:28:57.083678 140369993201472 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:10: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "from ddm.run_split import SplitMnistGenerator\n",
    "from ddm.alg.cla_models_multihead import MFVI_IBP_NN, Vanilla_NN\n",
    "from ddm.alg.utils import get_scores, concatenate_results\n",
    "from ddm.alg.vcl import run_vcl\n",
    "from copy import deepcopy\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:30.424512 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0813 18:37:30.427169 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:164: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0813 18:37:30.482261 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:58: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0813 18:37:30.588547 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:62: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0813 18:37:30.590138 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:65: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.059754695\n",
      "Epoch: 0006 cost= 0.000913144\n",
      "Epoch: 0011 cost= 0.000234285\n",
      "Epoch: 0016 cost= 0.000082365\n",
      "Epoch: 0021 cost= 0.000042895\n",
      "Epoch: 0026 cost= 0.000023777\n",
      "Epoch: 0031 cost= 0.000014887\n",
      "Epoch: 0036 cost= 0.000009923\n",
      "Epoch: 0041 cost= 0.000006549\n",
      "Epoch: 0046 cost= 0.000004535\n",
      "Epoch: 0051 cost= 0.000003333\n",
      "Epoch: 0056 cost= 0.000002337\n",
      "Epoch: 0061 cost= 0.000001674\n",
      "Epoch: 0066 cost= 0.000001251\n",
      "Epoch: 0071 cost= 0.000000906\n",
      "Epoch: 0076 cost= 0.000000680\n",
      "Epoch: 0081 cost= 0.000000512\n",
      "Epoch: 0086 cost= 0.000000387\n",
      "Epoch: 0091 cost= 0.000000281\n",
      "Epoch: 0096 cost= 0.000000211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:40.962682 140408376342336 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:525: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:41.177778 140408376342336 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:496: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:41.663980 140408376342336 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0813 18:37:42.680103 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:503: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0813 18:37:42.716534 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:621: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0813 18:37:42.732589 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:630: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0813 18:37:42.799620 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:637: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0813 18:37:42.804113 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:644: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Z: (1, ?, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:42.987454 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:913: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 19.072743214\n",
      "Epoch: 0006 cost= 5.168781454\n",
      "Epoch: 0011 cost= 3.800158814\n",
      "Epoch: 0016 cost= 3.674991938\n",
      "Epoch: 0021 cost= 3.574922829\n",
      "Epoch: 0026 cost= 3.511328907\n",
      "Epoch: 0031 cost= 3.403818682\n",
      "Epoch: 0036 cost= 3.414693139\n",
      "Epoch: 0041 cost= 3.346116540\n",
      "Epoch: 0046 cost= 3.362689755\n",
      "Epoch: 0051 cost= 3.332217416\n",
      "Epoch: 0056 cost= 3.280205180\n",
      "Epoch: 0061 cost= 3.252052155\n",
      "Epoch: 0066 cost= 3.273583114\n",
      "Epoch: 0071 cost= 3.220419619\n",
      "Epoch: 0076 cost= 3.239492937\n",
      "Epoch: 0081 cost= 3.198088482\n",
      "Epoch: 0086 cost= 3.186336327\n",
      "Epoch: 0091 cost= 3.173645304\n",
      "Epoch: 0096 cost= 3.183016286\n",
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 cost= 1.909118086\n",
      "Epoch: 0006 cost= 1.566231799\n",
      "Epoch: 0011 cost= 1.535252309\n",
      "Epoch: 0016 cost= 1.504906576\n",
      "Epoch: 0021 cost= 1.506916551\n",
      "Epoch: 0026 cost= 1.488153032\n",
      "Epoch: 0031 cost= 1.463602068\n",
      "Epoch: 0036 cost= 1.463065257\n",
      "Epoch: 0041 cost= 1.434082117\n",
      "Epoch: 0046 cost= 1.438577353\n",
      "Epoch: 0051 cost= 1.408260581\n",
      "Epoch: 0056 cost= 1.422142242\n",
      "Epoch: 0061 cost= 1.414103870\n",
      "Epoch: 0066 cost= 1.393510203\n",
      "Epoch: 0071 cost= 1.408731267\n",
      "Epoch: 0076 cost= 1.393795632\n",
      "Epoch: 0081 cost= 1.386688328\n",
      "Epoch: 0086 cost= 1.383588419\n",
      "Epoch: 0091 cost= 1.389535990\n",
      "Epoch: 0096 cost= 1.383818848\n",
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 cost= 1.585528219\n",
      "Epoch: 0006 cost= 1.274924992\n",
      "Epoch: 0011 cost= 1.245047809\n",
      "Epoch: 0016 cost= 1.259380926\n",
      "Epoch: 0021 cost= 1.237668551\n",
      "Epoch: 0026 cost= 1.224873802\n",
      "Epoch: 0031 cost= 1.216191843\n",
      "Epoch: 0036 cost= 1.230660688\n",
      "Epoch: 0041 cost= 1.221169866\n",
      "Epoch: 0046 cost= 1.220540526\n",
      "Epoch: 0051 cost= 1.200252410\n",
      "Epoch: 0056 cost= 1.204207777\n",
      "Epoch: 0061 cost= 1.201252750\n",
      "Epoch: 0066 cost= 1.192589523\n",
      "Epoch: 0071 cost= 1.213362136\n",
      "Epoch: 0076 cost= 1.185991255\n",
      "Epoch: 0081 cost= 1.185599490\n",
      "Epoch: 0086 cost= 1.171081013\n",
      "Epoch: 0091 cost= 1.189016043\n",
      "Epoch: 0096 cost= 1.203665702\n",
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 cost= 1.264822223\n",
      "Epoch: 0006 cost= 1.041652664\n",
      "Epoch: 0011 cost= 1.029968591\n",
      "Epoch: 0016 cost= 1.033784383\n",
      "Epoch: 0021 cost= 1.018066683\n",
      "Epoch: 0026 cost= 1.013130570\n",
      "Epoch: 0031 cost= 1.004937966\n",
      "Epoch: 0036 cost= 1.007604193\n",
      "Epoch: 0041 cost= 1.005988004\n",
      "Epoch: 0046 cost= 1.020865483\n",
      "Epoch: 0051 cost= 1.014231551\n",
      "Epoch: 0056 cost= 0.999595554\n",
      "Epoch: 0061 cost= 0.998128097\n",
      "Epoch: 0066 cost= 0.984954735\n",
      "Epoch: 0071 cost= 1.000397318\n",
      "Epoch: 0076 cost= 0.983623189\n",
      "Epoch: 0081 cost= 0.994124802\n",
      "Epoch: 0086 cost= 0.991285635\n",
      "Epoch: 0091 cost= 0.980908627\n",
      "Epoch: 0096 cost= 0.997434050\n",
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 cost= 1.314980200\n",
      "Epoch: 0006 cost= 1.128882988\n",
      "Epoch: 0011 cost= 1.121336703\n",
      "Epoch: 0016 cost= 1.119447818\n",
      "Epoch: 0021 cost= 1.124731733\n",
      "Epoch: 0026 cost= 1.127327268\n",
      "Epoch: 0031 cost= 1.124581941\n",
      "Epoch: 0036 cost= 1.110276766\n",
      "Epoch: 0041 cost= 1.117073931\n",
      "Epoch: 0046 cost= 1.121064160\n",
      "Epoch: 0051 cost= 1.101729352\n",
      "Epoch: 0056 cost= 1.125848736\n",
      "Epoch: 0061 cost= 1.109237320\n",
      "Epoch: 0066 cost= 1.095020455\n",
      "Epoch: 0071 cost= 1.087014419\n",
      "Epoch: 0076 cost= 1.099980509\n",
      "Epoch: 0081 cost= 1.097690584\n",
      "Epoch: 0086 cost= 1.100533514\n",
      "Epoch: 0091 cost= 1.089171456\n",
      "Epoch: 0096 cost= 1.090654365\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9607565 ,        nan,        nan,        nan,        nan],\n",
       "       [0.8141844 , 0.92654261,        nan,        nan,        nan],\n",
       "       [0.6212766 , 0.87267385, 0.92902882,        nan,        nan],\n",
       "       [0.62222222, 0.85651322, 0.90394877, 0.94561934,        nan],\n",
       "       [0.46146572, 0.67825661, 0.91141942, 0.82678751, 0.9293999 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 100\n",
    "alpha0 = 1.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "# Run vanilla VCL\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = True\n",
    "data_gen = SplitMnistGenerator(val)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, temp=tau0, temp_prior=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "    acc = get_scores(mf_model, x_valsets, y_valsets, single_head)\n",
    "    ibp_acc = concatenate_results(acc, ibp_acc)\n",
    "    \n",
    "    mf_model.close_session()\n",
    "    \n",
    "ibp_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |   beta    | lambda_1  | lambda_2  |\n",
      "-------------------------------------------------------------------------\n",
      "Epoch: 0001 cost= 0.060023063\n",
      "Epoch: 0006 cost= 0.001070564\n",
      "Epoch: 0011 cost= 0.000234583\n",
      "Epoch: 0016 cost= 0.000098851\n",
      "Epoch: 0021 cost= 0.000055228\n",
      "Epoch: 0026 cost= 0.000034481\n",
      "Epoch: 0031 cost= 0.000022698\n",
      "Epoch: 0036 cost= 0.000014909\n",
      "Epoch: 0041 cost= 0.000010818\n",
      "Epoch: 0046 cost= 0.000007686\n",
      "Epoch: 0051 cost= 0.000005569\n",
      "Epoch: 0056 cost= 0.000004187\n",
      "Epoch: 0061 cost= 0.000003140\n",
      "Epoch: 0066 cost= 0.000002421\n",
      "Epoch: 0071 cost= 0.000001876\n",
      "Epoch: 0076 cost= 0.000001440\n",
      "Epoch: 0081 cost= 0.000001133\n",
      "Epoch: 0086 cost= 0.000000870\n",
      "Epoch: 0091 cost= 0.000000686\n",
      "Epoch: 0096 cost= 0.000000534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 18:29:17.721094 140369993201472 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:531: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 18:29:17.947698 140369993201472 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:502: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 18:29:18.463605 140369993201472 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0814 18:29:19.341619 140369993201472 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:509: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0814 18:29:19.383440 140369993201472 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:627: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0814 18:29:19.401348 140369993201472 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:636: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0814 18:29:19.466131 140369993201472 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:643: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0814 18:29:19.470989 140369993201472 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:650: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0814 18:29:19.595089 140369993201472 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:918: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Z: (1, ?, 100)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Can not convert a float64 into a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5651\u001b[0m                  self).get_controller(default) as g, context.graph_mode():\n\u001b[0;32m-> 5652\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5653\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (7.5943491424744725, 8.410864994725817, 0.8043467303625325, 4.8969227134387605)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1112\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1113\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1114\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3795\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3796\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3884\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" %\n\u001b[0;32m-> 3885\u001b[0;31m                       (type(obj).__name__, types_str))\n\u001b[0m\u001b[1;32m   3886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a float64 into a Tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-af5c64c6ea30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_bounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-af5c64c6ea30>\u001b[0m in \u001b[0;36mcv_exp\u001b[0;34m(alpha, beta, lambda_1, lambda_2)\u001b[0m\n\u001b[1;32m     84\u001b[0m                        \u001b[0mmodel_params_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                        \u001b[0manneal_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anneal_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                        min_temp=model_params_cv['min_temp'])\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mmf_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmf_variances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmf_betas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, task_idx, no_epochs, batch_size, display_epoch, anneal_rate, min_temp)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 _, c, summary = sess.run(\n\u001b[1;32m    937\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                     feed_dict={self.x: batch_x, self.y: batch_y, self.task_idx: task_idx, self.training: True, self.temp: temp})\n\u001b[0m\u001b[1;32m    939\u001b[0m                 \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1114\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1116\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Can not convert a float64 into a Tensor."
     ]
    }
   ],
   "source": [
    "bo_params = {'acq': 'ei',\n",
    "            'init_points': 5,\n",
    "            'n_iter': 5}\n",
    "\n",
    "param_bounds = {'alpha': (1, 10),\n",
    "               'beta': (1, 10),\n",
    "               'lambda_1': (0.1, 5.0),\n",
    "               'lambda_2': (0.1, 5.0)}\n",
    "\n",
    "model_params = {'hidden_size' : [100],\n",
    "                'batch_size' : 128,\n",
    "                'no_epochs' : 200,\n",
    "                'learning_rate' : 0.001,\n",
    "                'anneal_rate' : 0.0,\n",
    "                'pred_samples': 100}\n",
    "\n",
    "def cv_exp(alpha, beta, lambda_1, lambda_2):\n",
    "    \"\"\" Runs BayesOpt on Split MNIST for lifelong learning with the BNN+IBP prior\n",
    "    \n",
    "    :params: optim params\n",
    "    returns av accuracy over val set\n",
    "    \"\"\"\n",
    "    # Run vanilla VCL\n",
    "    tf.set_random_seed(12)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    ibp_acc = np.array([])\n",
    "    \n",
    "    model_params_cv = copy.deepcopy(model_params)\n",
    "\n",
    "    val = True\n",
    "    data_gen = SplitMnistGenerator(val)\n",
    "    single_head=False\n",
    "    in_dim, out_dim = data_gen.get_dims()\n",
    "    x_testsets, y_testsets = [], []\n",
    "    x_valsets, y_valsets = [], []\n",
    "    for task_id in range(data_gen.max_iter):\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        if val:\n",
    "            x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "            x_valsets.append(x_val)\n",
    "            y_valsets.append(y_val)\n",
    "        else:    \n",
    "            x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "        x_testsets.append(x_test)\n",
    "        y_testsets.append(y_test)\n",
    "\n",
    "        # Set the readout head to train\n",
    "        head = 0 if single_head else task_id\n",
    "        bsize = x_train.shape[0] if (model_params['batch_size'] is None) else model_params['batch_size']\n",
    "\n",
    "        # Train network with maximum likelihood to initialize first model\n",
    "        if task_id == 0:\n",
    "            ml_model = Vanilla_NN(in_dim, model_params_cv['hidden_size'], out_dim, x_train.shape[0])\n",
    "            ml_model.train(x_train, y_train, task_id, 100, \n",
    "                           model_params_cv['batch_size'])\n",
    "            mf_weights = ml_model.get_weights()\n",
    "            mf_variances = None\n",
    "            mf_betas = None\n",
    "            ml_model.close_session()\n",
    "\n",
    "        # Train on non-coreset data\n",
    "        mf_model = MFVI_IBP_NN(in_dim,\n",
    "                               model_params_cv['hidden_size'],\n",
    "                               out_dim,\n",
    "                               x_train.shape[0],\n",
    "                               prev_means=mf_weights, \n",
    "                               prev_log_variances=mf_variances,\n",
    "                               prev_betas=mf_betas,\n",
    "                               alpha0=alpha,\n",
    "                               beta0=beta,\n",
    "                               learning_rate=model_params_cv['learning_rate'],\n",
    "                               lambda_1 = lambda_1, # initial temperature of the variational posterior for task 1\n",
    "                               lambda_2 = lambda_2, # temperature of the Concrete prior\n",
    "                               no_pred_samples=model_params_cv['pred_samples'], \n",
    "                               name='ibp_bo_alpha_{:.02}_beta_{:.02}_lambda_1_{:.02}_lambda_2_{:02}'.format(alpha,\n",
    "                                                                                                           beta, \n",
    "                                                                                                           lambda_1,\n",
    "                                                                                                           lambda_2))\n",
    "        \n",
    "        mf_model.train(x_train, y_train, head, model_params_cv['no_epochs'], \n",
    "                       model_params_cv['batch_size'],\n",
    "                       anneal_rate=model_params_cv['anneal_rate'], \n",
    "                       min_temp=lambda_1)\n",
    "        mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "        acc = get_scores(mf_model, x_valsets, y_valsets, single_head)\n",
    "        ibp_acc = concatenate_results(acc, ibp_acc)\n",
    "\n",
    "        mf_model.close_session()\n",
    "\n",
    "    return np.nanmean(ibp_acc)\n",
    "\n",
    "bo = BayesianOptimization(cv_exp, param_bounds)\n",
    "bo.maximize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_name(experiment_name, param_bounds, bo_params, data_params, model_params, train_params, optim_params, results_folder=\"./results\"):\n",
    "    pp = ''.join('{}:{}|'.format(key, val) for key, val in sorted(param_bounds.items()))[:-1]\n",
    "    bp = ''.join('{}:{}|'.format(key, val) for key, val in sorted(bo_params.items()))[:-1]\n",
    "    mp = ''.join('{}:{}|'.format(key, val) for key, val in sorted(model_params.items()))[:-1]\n",
    "    return os.path.join(results_folder, experiment_name, pp, bp, mp)\n",
    "\n",
    "######################\n",
    "## Store BO results ##\n",
    "######################\n",
    "\n",
    "# Folder for storing results\n",
    "results_folder = \"./results/\"\n",
    "\n",
    "experiment_name = 'ibp_split_mnist_bo'\n",
    "folder = folder_name(results_folder=results_folder,\n",
    "                     experiment_name=experiment_name,\n",
    "                     param_bounds=param_bounds,\n",
    "                     bo_params=bo_params,\n",
    "                     model_params=model_params)\n",
    "\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(folder, 'res_all.pkl'), 'wb') as input_file:\n",
    "    pickle.dump(bo.res, output)\n",
    "    \n",
    "with open(os.path.join(folder, 'res_max.pkl'), 'wb') as input_file:\n",
    "    pickle.dump(bo.max, output)\n",
    "    \n",
    "alpha_opt = bo.max['params']['alpha']\n",
    "beta_opt = bo.max['params']['beta']\n",
    "lambda_1_opt = bo.max['params']['lambda_1']\n",
    "lambda_2_opt = bo.max['params']['lambda_2']\n",
    "print(\"alpha_opt: {}\".format(alpha_opt))\n",
    "print(\"beta_opt: {}\".format(beta_opt))\n",
    "print(\"lambda_1_opt: {}\".format(lambda_1_opt))\n",
    "print(\"lambda_2_opt: {}\".format(lambda_2_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## Experiment with Optimal Parameters ##\n",
    "########################################\n",
    "\n",
    "# Run vanilla VCL\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = True\n",
    "data_gen = SplitMnistGenerator(val)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train\n",
    "    mf_model = MFVI_IBP_NN(in_dim,\n",
    "                           model_params['hidden_size'],\n",
    "                           out_dim,\n",
    "                           x_train.shape[0],\n",
    "                           prev_means=mf_weights,\n",
    "                           prev_log_variances=mf_variances,\n",
    "                           prev_betas=mf_betas,\n",
    "                           alpha0=alpha_opt,\n",
    "                           beta0=beta_opt,\n",
    "                           learning_rate=model_params['learning_rate'],\n",
    "                           temp = lambda_1_opt,\n",
    "                           temp_prior=lambda_2_opt,\n",
    "                           no_pred_samples=model_params['pred_samples'],\n",
    "                           name='ibp_bo_opt')\n",
    "        \n",
    "    mf_model.train(x_train, y_train, head, model_params['no_epochs'], \n",
    "                   model_params['batch_size'],\n",
    "                   anneal_rate=model_params['anneal_rate'], \n",
    "                   min_temp=model_params['min_temp'])\n",
    "    \n",
    "    mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "    acc = get_scores(mf_model, x_testsets, y_testsets, single_head)\n",
    "    ibp_acc = concatenate_results(acc, ibp_acc)\n",
    "    \n",
    "    mf_model.close_session()\n",
    "    \n",
    "ibp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Vanilla VCL\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "hidden_size = [10]\n",
    "coreset_size = 0\n",
    "\n",
    "data_gen = SplitMnistGenerator()\n",
    "vcl_result = run_vcl(hidden_size, no_epochs, data_gen, \n",
    "                              lambda a: a, coreset_size, batch_size, single_head)\n",
    "print(vcl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run IBP VCL\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(12)\n",
    "# np.random.seed(1)\n",
    "# coreset_size = 0\n",
    "\n",
    "# hidden_size = [50]\n",
    "# batch_size = 128\n",
    "# no_epochs = 100\n",
    "# alpha0 = 5.0\n",
    "# tau0=0.1 # initial temperature\n",
    "# temp_prior=1.0\n",
    "# ANNEAL_RATE=0.000\n",
    "# MIN_TEMP=0.1\n",
    "# single_head=False\n",
    "\n",
    "# # data_gen = SplitMnistGenerator()\n",
    "# # vcl_ibp_result = vcl.run_vcl_ibp(hidden_size=hidden_size, no_epochs=no_epochs, data_gen=data_gen,\n",
    "# #                                   batch_size=batch_size, single_head=single_head, alpha0=alpha0,\n",
    "# #                                   learning_rate=0.01, temp_prior=temp_prior, no_pred_samples=100,\n",
    "# #                                   tau0=tau0, tau_anneal_rate=ANNEAL_RATE, tau_min=MIN_TEMP)\n",
    "# # print(vcl_ibp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ibp_acc = np.nanmean(ibp_acc, 1)\n",
    "_vcl_result = np.nanmean(vcl_result, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vcl_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = plt.gca()\n",
    "plt.plot(np.arange(len(_ibp_acc))+1, _ibp_acc, label='VCL + IBP', marker='o')\n",
    "plt.plot(np.arange(len(_vcl_result))+1, _vcl_result, label='VCL', marker='o')\n",
    "ax.set_xticks(range(1, len(_ibp_acc)+1))\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('\\# tasks')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
