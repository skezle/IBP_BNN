{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0816 18:17:56.460163 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:9: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0816 18:17:56.461135 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:13: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "import pdb\n",
    "import re\n",
    "from ddm.run_split import SplitMnistGenerator\n",
    "from ddm.alg.cla_models_multihead import MFVI_IBP_NN, Vanilla_NN\n",
    "from ddm.alg.utils import get_scores, concatenate_results\n",
    "from ddm.alg.vcl import run_vcl\n",
    "from copy import deepcopy\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:30.424512 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0813 18:37:30.427169 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:164: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0813 18:37:30.482261 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:58: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0813 18:37:30.588547 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:62: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0813 18:37:30.590138 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:65: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.059754695\n",
      "Epoch: 0006 cost= 0.000913144\n",
      "Epoch: 0011 cost= 0.000234285\n",
      "Epoch: 0016 cost= 0.000082365\n",
      "Epoch: 0021 cost= 0.000042895\n",
      "Epoch: 0026 cost= 0.000023777\n",
      "Epoch: 0031 cost= 0.000014887\n",
      "Epoch: 0036 cost= 0.000009923\n",
      "Epoch: 0041 cost= 0.000006549\n",
      "Epoch: 0046 cost= 0.000004535\n",
      "Epoch: 0051 cost= 0.000003333\n",
      "Epoch: 0056 cost= 0.000002337\n",
      "Epoch: 0061 cost= 0.000001674\n",
      "Epoch: 0066 cost= 0.000001251\n",
      "Epoch: 0071 cost= 0.000000906\n",
      "Epoch: 0076 cost= 0.000000680\n",
      "Epoch: 0081 cost= 0.000000512\n",
      "Epoch: 0086 cost= 0.000000387\n",
      "Epoch: 0091 cost= 0.000000281\n",
      "Epoch: 0096 cost= 0.000000211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:40.962682 140408376342336 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:525: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:41.177778 140408376342336 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:496: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:41.663980 140408376342336 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0813 18:37:42.680103 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:503: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0813 18:37:42.716534 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:621: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0813 18:37:42.732589 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:630: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0813 18:37:42.799620 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:637: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0813 18:37:42.804113 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:644: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Z: (1, ?, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 18:37:42.987454 140408376342336 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:913: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 19.072743214\n",
      "Epoch: 0006 cost= 5.168781454\n",
      "Epoch: 0011 cost= 3.800158814\n",
      "Epoch: 0016 cost= 3.674991938\n",
      "Epoch: 0021 cost= 3.574922829\n",
      "Epoch: 0026 cost= 3.511328907\n",
      "Epoch: 0031 cost= 3.403818682\n",
      "Epoch: 0036 cost= 3.414693139\n",
      "Epoch: 0041 cost= 3.346116540\n",
      "Epoch: 0046 cost= 3.362689755\n",
      "Epoch: 0051 cost= 3.332217416\n",
      "Epoch: 0056 cost= 3.280205180\n",
      "Epoch: 0061 cost= 3.252052155\n",
      "Epoch: 0066 cost= 3.273583114\n",
      "Epoch: 0071 cost= 3.220419619\n",
      "Epoch: 0076 cost= 3.239492937\n",
      "Epoch: 0081 cost= 3.198088482\n",
      "Epoch: 0086 cost= 3.186336327\n",
      "Epoch: 0091 cost= 3.173645304\n",
      "Epoch: 0096 cost= 3.183016286\n",
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 cost= 1.909118086\n",
      "Epoch: 0006 cost= 1.566231799\n",
      "Epoch: 0011 cost= 1.535252309\n",
      "Epoch: 0016 cost= 1.504906576\n",
      "Epoch: 0021 cost= 1.506916551\n",
      "Epoch: 0026 cost= 1.488153032\n",
      "Epoch: 0031 cost= 1.463602068\n",
      "Epoch: 0036 cost= 1.463065257\n",
      "Epoch: 0041 cost= 1.434082117\n",
      "Epoch: 0046 cost= 1.438577353\n",
      "Epoch: 0051 cost= 1.408260581\n",
      "Epoch: 0056 cost= 1.422142242\n",
      "Epoch: 0061 cost= 1.414103870\n",
      "Epoch: 0066 cost= 1.393510203\n",
      "Epoch: 0071 cost= 1.408731267\n",
      "Epoch: 0076 cost= 1.393795632\n",
      "Epoch: 0081 cost= 1.386688328\n",
      "Epoch: 0086 cost= 1.383588419\n",
      "Epoch: 0091 cost= 1.389535990\n",
      "Epoch: 0096 cost= 1.383818848\n",
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 cost= 1.585528219\n",
      "Epoch: 0006 cost= 1.274924992\n",
      "Epoch: 0011 cost= 1.245047809\n",
      "Epoch: 0016 cost= 1.259380926\n",
      "Epoch: 0021 cost= 1.237668551\n",
      "Epoch: 0026 cost= 1.224873802\n",
      "Epoch: 0031 cost= 1.216191843\n",
      "Epoch: 0036 cost= 1.230660688\n",
      "Epoch: 0041 cost= 1.221169866\n",
      "Epoch: 0046 cost= 1.220540526\n",
      "Epoch: 0051 cost= 1.200252410\n",
      "Epoch: 0056 cost= 1.204207777\n",
      "Epoch: 0061 cost= 1.201252750\n",
      "Epoch: 0066 cost= 1.192589523\n",
      "Epoch: 0071 cost= 1.213362136\n",
      "Epoch: 0076 cost= 1.185991255\n",
      "Epoch: 0081 cost= 1.185599490\n",
      "Epoch: 0086 cost= 1.171081013\n",
      "Epoch: 0091 cost= 1.189016043\n",
      "Epoch: 0096 cost= 1.203665702\n",
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 cost= 1.264822223\n",
      "Epoch: 0006 cost= 1.041652664\n",
      "Epoch: 0011 cost= 1.029968591\n",
      "Epoch: 0016 cost= 1.033784383\n",
      "Epoch: 0021 cost= 1.018066683\n",
      "Epoch: 0026 cost= 1.013130570\n",
      "Epoch: 0031 cost= 1.004937966\n",
      "Epoch: 0036 cost= 1.007604193\n",
      "Epoch: 0041 cost= 1.005988004\n",
      "Epoch: 0046 cost= 1.020865483\n",
      "Epoch: 0051 cost= 1.014231551\n",
      "Epoch: 0056 cost= 0.999595554\n",
      "Epoch: 0061 cost= 0.998128097\n",
      "Epoch: 0066 cost= 0.984954735\n",
      "Epoch: 0071 cost= 1.000397318\n",
      "Epoch: 0076 cost= 0.983623189\n",
      "Epoch: 0081 cost= 0.994124802\n",
      "Epoch: 0086 cost= 0.991285635\n",
      "Epoch: 0091 cost= 0.980908627\n",
      "Epoch: 0096 cost= 0.997434050\n",
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "_Z: (1, ?, 100)\n",
      "Epoch: 0001 cost= 1.314980200\n",
      "Epoch: 0006 cost= 1.128882988\n",
      "Epoch: 0011 cost= 1.121336703\n",
      "Epoch: 0016 cost= 1.119447818\n",
      "Epoch: 0021 cost= 1.124731733\n",
      "Epoch: 0026 cost= 1.127327268\n",
      "Epoch: 0031 cost= 1.124581941\n",
      "Epoch: 0036 cost= 1.110276766\n",
      "Epoch: 0041 cost= 1.117073931\n",
      "Epoch: 0046 cost= 1.121064160\n",
      "Epoch: 0051 cost= 1.101729352\n",
      "Epoch: 0056 cost= 1.125848736\n",
      "Epoch: 0061 cost= 1.109237320\n",
      "Epoch: 0066 cost= 1.095020455\n",
      "Epoch: 0071 cost= 1.087014419\n",
      "Epoch: 0076 cost= 1.099980509\n",
      "Epoch: 0081 cost= 1.097690584\n",
      "Epoch: 0086 cost= 1.100533514\n",
      "Epoch: 0091 cost= 1.089171456\n",
      "Epoch: 0096 cost= 1.090654365\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9607565 ,        nan,        nan,        nan,        nan],\n",
       "       [0.8141844 , 0.92654261,        nan,        nan,        nan],\n",
       "       [0.6212766 , 0.87267385, 0.92902882,        nan,        nan],\n",
       "       [0.62222222, 0.85651322, 0.90394877, 0.94561934,        nan],\n",
       "       [0.46146572, 0.67825661, 0.91141942, 0.82678751, 0.9293999 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 100\n",
    "alpha0 = 1.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "# Run vanilla VCL\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = True\n",
    "data_gen = SplitMnistGenerator(val)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, no_epochs, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, temp=tau0, temp_prior=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    mf_weights, mf_variances, mf_betas = mf_model.get_weights()\n",
    "\n",
    "    acc = get_scores(mf_model, x_valsets, y_valsets, single_head)\n",
    "    ibp_acc = concatenate_results(acc, ibp_acc)\n",
    "    \n",
    "    mf_model.close_session()\n",
    "    \n",
    "ibp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Vanilla VCL\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "hidden_size = [10]\n",
    "coreset_size = 0\n",
    "\n",
    "data_gen = SplitMnistGenerator()\n",
    "vcl_result = run_vcl(hidden_size, no_epochs, data_gen, \n",
    "                              lambda a: a, coreset_size, batch_size, single_head)\n",
    "print(vcl_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run IBP VCL\n",
    "# tf.reset_default_graph()\n",
    "# tf.set_random_seed(12)\n",
    "# np.random.seed(1)\n",
    "# coreset_size = 0\n",
    "\n",
    "# hidden_size = [50]\n",
    "# batch_size = 128\n",
    "# no_epochs = 100\n",
    "# alpha0 = 5.0\n",
    "# tau0=0.1 # initial temperature\n",
    "# temp_prior=1.0\n",
    "# ANNEAL_RATE=0.000\n",
    "# MIN_TEMP=0.1\n",
    "# single_head=False\n",
    "\n",
    "# # data_gen = SplitMnistGenerator()\n",
    "# # vcl_ibp_result = vcl.run_vcl_ibp(hidden_size=hidden_size, no_epochs=no_epochs, data_gen=data_gen,\n",
    "# #                                   batch_size=batch_size, single_head=single_head, alpha0=alpha0,\n",
    "# #                                   learning_rate=0.01, temp_prior=temp_prior, no_pred_samples=100,\n",
    "# #                                   tau0=tau0, tau_anneal_rate=ANNEAL_RATE, tau_min=MIN_TEMP)\n",
    "# # print(vcl_ibp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ibp_acc = np.nanmean(ibp_acc, 1)\n",
    "_vcl_result = np.nanmean(vcl_result, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vcl_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = plt.gca()\n",
    "plt.plot(np.arange(len(_ibp_acc))+1, _ibp_acc, label='VCL + IBP', marker='o')\n",
    "plt.plot(np.arange(len(_vcl_result))+1, _vcl_result, label='VCL', marker='o')\n",
    "ax.set_xticks(range(1, len(_ibp_acc)+1))\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('\\# tasks')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFVI_IBP_NN_prune(MFVI_IBP_NN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, training_size,\n",
    "                 no_train_samples=10, no_pred_samples=100, prev_means=None, prev_log_variances=None,\n",
    "                 prev_betas=None, learning_rate=0.001,\n",
    "                 prior_mean=0, prior_var=1, alpha0=5., beta0=1., lambda_1=1., lambda_2=1.,\n",
    "                 tensorboard_dir='logs', name='ibp_wp', min_temp=0.5, tb_logging=True):\n",
    "\n",
    "        super(MFVI_IBP_NN_prune, self).__init__(input_size, hidden_size, output_size, training_size,\n",
    "                 no_train_samples, no_pred_samples, prev_means, prev_log_variances,\n",
    "                 prev_betas, learning_rate,\n",
    "                 prior_mean, prior_var, alpha0, beta0, lambda_1, lambda_2,\n",
    "                 tensorboard_dir, name, min_temp, tb_logging)\n",
    "\n",
    "\n",
    "    def prune_weights(self, X_test, Y_test, task_id):\n",
    "        \"\"\" Performs weight pruning\n",
    "        Args:\n",
    "            X_test: numpy array\n",
    "            Y_test: numpy array\n",
    "            task_id: int\n",
    "        :return: cutoffs, accs via naive pruning, accs via snr pruning,\n",
    "        weight values, sigma values of network\n",
    "        \"\"\"\n",
    "\n",
    "        def reset_weights(pr_mus, pr_sigmas, _mus, _sigmas):\n",
    "            \"\"\" Reset weights of graph to original values\n",
    "            Args:\n",
    "                pr_mus: list of tf variables which have been pruned\n",
    "                pr_sigmas: list of tf variables which have been pruned\n",
    "                _mus: list of cached mus in numpy\n",
    "                _sigmas: list of cached sigmas in numpy\n",
    "            \"\"\"\n",
    "\n",
    "            for v, _v in zip(pr_mus, _mus):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "            for v, _v in zip(pr_sigmas, _sigmas):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "        def pruning(remove_pct, weightvalues_ibp, weightvalues, sigmavalues, Zs, Z_tiled,\n",
    "                    mu_weights, mu_sigmas, b_weights, b_sigmas, uncert_pruning=True):\n",
    "            \"\"\" Performs weight pruning experiment\n",
    "            Args:\n",
    "                weightvalues_ibp: np array of weights with ibp mask applied\n",
    "                weightvalues: np array of weights\n",
    "                sigmavalues: np array of sigmas\n",
    "                Zs: list of tf IBP masks\n",
    "                Z_tiled: list of tf IBP masks, tiled for application to weights, not biases\n",
    "                mu_weights: list of tf weight variable\n",
    "                mu_sigmas: list of tf sigma variables\n",
    "                b_weights: list of tf mean variables for biases\n",
    "                b_sigmas: list of tf sigma variables for biases\n",
    "                uncert_pruning: bool pruning by snr\n",
    "            \"\"\"\n",
    "            if uncert_pruning:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues_ibp) / sigmavalues)\n",
    "            else:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues))\n",
    "            cutoff = sorted_STN[int(remove_pct * len(sorted_STN))]\n",
    "            \n",
    "            # Weights\n",
    "            for v, s in zip(mu_weights, mu_sigmas):\n",
    "                if uncert_pruning:\n",
    "                    # TODO apply Z mask\n",
    "                    snr = tf.abs(v) / tf.exp(0.5*s)\n",
    "                    mask = tf.greater_equal(snr, cutoff)\n",
    "                else:\n",
    "                    mask = tf.greater_equal(tf.abs(v), cutoff)\n",
    "                self.sess.run(tf.assign(v, tf.multiply(v, tf.cast(mask, v.dtype))))\n",
    "                self.sess.run(tf.assign(s, tf.multiply(s, tf.cast(mask, s.dtype))))  # also apply zero std to weight!!!\n",
    "            \n",
    "            # Biases\n",
    "            for v, s in zip(b_weights, b_sigmas):\n",
    "                if uncert_pruning:\n",
    "                    # TODO apply Z mask\n",
    "                    snr = tf.abs(v) / tf.exp(0.5*s)\n",
    "                    mask = tf.greater_equal(snr, cutoff)\n",
    "                else:\n",
    "                    mask = tf.greater_equal(tf.abs(v), cutoff)\n",
    "                self.sess.run(tf.assign(v, tf.multiply(v, tf.cast(mask, v.dtype))))\n",
    "                self.sess.run(tf.assign(s, tf.multiply(s, tf.cast(mask, s.dtype))))  # also apply zero std to weight!!!\n",
    "            \n",
    "            accs = []\n",
    "            for _ in range(10):\n",
    "                accs.append(self.sess.run(self.acc, {self.x: X_test,\n",
    "                                                     self.y: Y_test,\n",
    "                                                     self.task_idx: task_id,\n",
    "                                                     self.temp: self.lambda_1,\n",
    "                                                     self.training: False}))\n",
    "            print(\"%.2f, %s\" % (np.sum(sorted_STN < cutoff) / len(sorted_STN), np.mean(accs)))\n",
    "            return np.mean(accs)\n",
    "\n",
    "        # get weights\n",
    "        weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n",
    "        \n",
    "        # Get weights from network\n",
    "        mus_w = []\n",
    "        mus_b = []\n",
    "        sigmas_w = []\n",
    "        sigmas_b = []\n",
    "        for v in weights:\n",
    "            if re.match(\"^([w])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_w.append(v)\n",
    "            elif re.match(\"^([w])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_w.append(v)\n",
    "            elif re.match(\"^([b])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_b.append(v)\n",
    "            elif re.match(\"^([b])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_b.append(v)\n",
    "            else:\n",
    "                print(\"Un-matched: {}\".format(v.name))\n",
    "        # Get mask\n",
    "        # Need to apply mask to weights and biases for pruning\n",
    "        # Do not apply to head weights though!\n",
    "        # Zs \\in [x_test.size, dout], just need one sample of Z!\n",
    "        Zs = self.sess.run(self.Z, {self.x: X_test,\n",
    "                                    self.y: Y_test,\n",
    "                                    self.task_idx: task_id,\n",
    "                                    self.temp: self.lambda_1,\n",
    "                                    self.training: False})[0] # z mask for each layer in a list, each Z \\in dout\n",
    "\n",
    "        # cache\n",
    "        _mus_w = [self.sess.run(w) for w in mus_w]\n",
    "        _sigmas_w = [self.sess.run(w) for w in sigmas_w]\n",
    "        _mus_b = [self.sess.run(w) for w in mus_b]\n",
    "        _sigmas_b = [self.sess.run(w) for w in sigmas_b]\n",
    "\n",
    "        # flatten values for cut-off finding\n",
    "        # Tile Zs [dout] --> [din, dout]\n",
    "        dins = [x.shape[0] for x in _mus_w]\n",
    "        print(\"dins: {}\".format(len(dins)))\n",
    "        print(\"_mus: {}\".format(len(_mus_w)))\n",
    "        Z_tiled = []\n",
    "        t_Z_tiled = []\n",
    "        for i in range(len(dins)):\n",
    "            Z_tiled.append(np.tile(Zs[i][0, :], (dins[i], 1))) \n",
    "            t_Z_tiled.append(tf.tile(tf.squeeze(self.Z[i], axis=0), [dins[i], 1]))\n",
    "        \n",
    "        # multiply Zs with \n",
    "        xx = []\n",
    "        yy = []\n",
    "        for i in range(len(mus_w)):\n",
    "            xx.append(np.multiply(self.sess.run(mus_w[i]), Z_tiled[i]).flatten())\n",
    "            yy.append(np.multiply(self.sess.run(mus_b[i]), Zs[i]).flatten())\n",
    "            \n",
    "        weightvalues_ibp = np.hstack(np.array(xx + yy))\n",
    "        weightvalues = np.hstack(np.array([self.sess.run(w).flatten() for w in mus_w + mus_b]))\n",
    "        sigmavalues_tr = np.hstack(np.array([self.sess.run(tf.exp(0.5*s)).flatten() for s in sigmas_w + sigmas_b]))\n",
    "    \n",
    "        xs = np.append(0.05 * np.array(range(20)), np.array([0.98, 0.99, 0.999]))\n",
    "        ya = []\n",
    "        for pct in xs:\n",
    "            ya.append(pruning(pct, weightvalues_ibp, weightvalues, sigmavalues_tr, self.Z, t_Z_tiled,\n",
    "                              mus_w, sigmas_w, mus_b, sigmas_b, uncert_pruning=False))\n",
    "\n",
    "        # reset etc.\n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        pdb.set_trace()\n",
    "        yb = []\n",
    "        for pct in xs:\n",
    "            yb.append(pruning(pct, weightvalues_ibp, weightvalues, sigmavalues_tr, self.Z, t_Z_tiled,\n",
    "                              mus_w, sigmas_w, mus_b, sigmas_b, uncert_pruning=True))\n",
    "            \n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        \n",
    "        return xs, ya, yb, weightvalues, sigmavalues_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 18:17:57.806393 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0816 18:17:57.811012 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:167: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0816 18:17:57.882209 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:61: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0816 18:17:57.986377 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:65: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.047497832\n",
      "Epoch: 0006 cost= 0.000949820\n",
      "Epoch: 0011 cost= 0.000255335\n",
      "Epoch: 0016 cost= 0.000100076\n",
      "Epoch: 0021 cost= 0.000052391\n",
      "Epoch: 0026 cost= 0.000029428\n",
      "Epoch: 0031 cost= 0.000018897\n",
      "Epoch: 0036 cost= 0.000012422\n",
      "Epoch: 0041 cost= 0.000008246\n",
      "Epoch: 0046 cost= 0.000005765\n",
      "Epoch: 0051 cost= 0.000004355\n",
      "Epoch: 0056 cost= 0.000002994\n",
      "Epoch: 0061 cost= 0.000002143\n",
      "Epoch: 0066 cost= 0.000001619\n",
      "Epoch: 0071 cost= 0.000001182\n",
      "Epoch: 0076 cost= 0.000000894\n",
      "Epoch: 0081 cost= 0.000000666\n",
      "Epoch: 0086 cost= 0.000000527\n",
      "Epoch: 0091 cost= 0.000000367\n",
      "Epoch: 0096 cost= 0.000000278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 18:18:12.404314 140197656663872 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:534: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 18:18:12.944829 140197656663872 deprecation.py:323] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:505: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n",
      "z_discrete: (1, ?, 100)\n",
      "biases: <unknown>\n",
      "pre: <unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 18:18:13.505430 140197656663872 deprecation.py:323] From /home/skessler/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0816 18:18:14.818208 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:512: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0816 18:18:14.865417 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:630: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0816 18:18:14.889670 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:639: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W0816 18:18:14.976837 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:646: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "W0816 18:18:14.981236 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:653: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Z: (1, ?, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 18:18:15.136775 140197656663872 deprecation_wrapper.py:119] From /home/skessler/Projects/IBP_BNN/ddm/alg/cla_models_multihead.py:921: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train cost= 19.118880744\n",
      "Epoch: 0006 train cost= 5.136846292\n",
      "Optimization Finished!\n",
      "Un-matched: w_0:0\n",
      "Un-matched: b_0:0\n",
      "Un-matched: w_h_0:0\n",
      "Un-matched: b_h_0:0\n",
      "Un-matched: beta_a_0:0\n",
      "Un-matched: beta_b_0:0\n",
      "Un-matched: w_mu_h_0:0\n",
      "Un-matched: b_mu_h_0:0\n",
      "Un-matched: w_sigma_h_0:0\n",
      "Un-matched: b_sigma_h_0:0\n",
      "dins: 1\n",
      "_mus: 1\n",
      "0.00, 0.9973711\n",
      "0.05, 0.9970071\n",
      "0.10, 0.9971911\n",
      "0.15, 0.99726856\n",
      "0.20, 0.99732906\n",
      "0.25, 0.9969385\n",
      "0.30, 0.997244\n",
      "0.35, 0.9972345\n",
      "0.40, 0.9970709\n",
      "0.45, 0.9973839\n",
      "0.50, 0.9971201\n",
      "0.55, 0.99734706\n",
      "0.60, 0.9973661\n",
      "0.65, 0.99765337\n",
      "0.70, 0.9973229\n",
      "0.75, 0.9972355\n",
      "0.80, 0.9971466\n",
      "0.85, 0.9971503\n",
      "0.90, 0.9968709\n",
      "0.95, 0.99729174\n",
      "0.98, 0.9965154\n",
      "0.99, 0.9927977\n",
      "1.00, 0.7786955\n",
      "> <ipython-input-2-1dd24ffc7e9a>(160)prune_weights()\n",
      "-> yb = []\n",
      "(Pdb) xx[0].shape\n",
      "(78400,)\n",
      "(Pdb) yy[0].shape\n",
      "(78400,)\n",
      "(Pdb) self.sess.run(mus_b[0])\n",
      "array([ 9.53524411e-01, -7.23002732e-01,  6.56470120e-01, -3.89161527e-01,\n",
      "        5.11614263e-01,  2.02938989e-01, -2.04024628e-01, -8.99157077e-02,\n",
      "       -1.73770413e-01, -1.02267534e-01, -2.28908397e-02,  5.27119823e-03,\n",
      "       -2.65560988e-02, -1.57761923e-03, -4.80975228e-04,  4.97299246e-04,\n",
      "       -1.12303038e-04,  1.28125044e-04,  1.19239456e-04,  1.81144951e-05,\n",
      "       -1.27571748e-05,  3.45415174e-06,  1.32285813e-05, -9.04272611e-07,\n",
      "        2.67348582e-07,  2.75940420e-06,  7.65714844e-07, -5.25465111e-08,\n",
      "        6.33724085e-07,  1.36082132e-08,  2.38733069e-08,  3.33946121e-10,\n",
      "        3.11310124e-08,  9.67551284e-09,  6.24102853e-11, -2.74269946e-15,\n",
      "       -1.00600283e-17,  3.61882683e-24,  1.19585002e-24,  3.80528334e-16,\n",
      "        9.34526832e-25,  6.37643922e-24,  1.26789393e-24,  1.35553529e-24,\n",
      "       -2.72936072e-24,  9.07790561e-26, -1.87177819e-24, -3.09811956e-25,\n",
      "        3.62208167e-24, -2.18991115e-25, -4.12635883e-26,  2.64314009e-25,\n",
      "       -1.10686296e-24,  5.11984414e-24,  6.51906132e-24,  3.86461656e-24,\n",
      "       -3.09506224e-25,  3.84577432e-25, -1.04232684e-25,  1.37229257e-24,\n",
      "        2.08865371e-25, -1.13567897e-24, -4.98584981e-24, -6.37973311e-25,\n",
      "        2.89291466e-24,  3.38368238e-24,  2.70816738e-25, -1.28338124e-24,\n",
      "        6.03932345e-24,  3.68234867e-24, -3.74414606e-24, -5.70692074e-24,\n",
      "        4.96994844e-25, -2.77923822e-24, -2.16621648e-25,  2.79843476e-27,\n",
      "        2.38236141e-25,  2.97148323e-25, -1.59659718e-24,  1.50541285e-24,\n",
      "       -2.20425526e-24,  3.71596992e-24,  2.83957938e-24,  1.51780497e-24,\n",
      "        1.61807224e-24, -4.73003089e-24,  2.01316751e-24,  5.27243617e-25,\n",
      "        5.53244127e-24, -3.45314802e-26,  5.39388624e-25, -6.38952642e-24,\n",
      "        2.48279652e-24, -4.42932481e-25, -5.47804389e-25,  2.38500755e-25,\n",
      "       -3.76290162e-24, -1.35606215e-24, -3.28698460e-24, -2.82677779e-25],\n",
      "      dtype=float32)\n",
      "(Pdb) self.sess.run(mus_b[i]).shape\n",
      "(100,)\n",
      "(Pdb) self.sess.run(mus_w[i]).shape\n",
      "(784, 100)\n",
      "(Pdb) Z_tiled[0].shape\n",
      "(784, 100)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 10\n",
    "alpha0 = 1.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = False\n",
    "data_gen = SplitMnistGenerator(val, num_tasks=1)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN_prune(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, lambda_1=tau0, lambda_2=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    \n",
    "    xs, ya, yb, _, _  = mf_model.prune_weights(x_test, y_test, head)\n",
    "    \n",
    "    mf_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ya, label='$|\\mu|$')\n",
    "plt.plot(xs, yb, label='$ \\\\frac{|\\mu|}{\\sigma}$')\n",
    "plt.xlabel('cut-off')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
