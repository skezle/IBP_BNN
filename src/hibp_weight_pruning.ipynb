{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "import pdb\n",
    "import re\n",
    "from run_split import SplitMnistGenerator\n",
    "from alg.cla_models_multihead import MFVI_NN, Vanilla_NN\n",
    "from alg.HIBP_BNN_multihead import HIBP_BNN\n",
    "from alg.IBP_BNN_multihead import IBP_BNN\n",
    "from hibp_weight_pruning import prune_weights, MnistGenerator\n",
    "from alg.utils import get_scores, concatenate_results\n",
    "from alg.vcl import run_vcl\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [200, 200]\n",
    "batch_size = 128\n",
    "no_epochs = 100\n",
    "runs = 1\n",
    "seeds = [1,2,3,4,5]\n",
    "np.random.seed(1)\n",
    "xs = np.append(0.1 * np.array(range(10)), [0.95, 0.96, 0.97, 0.98, 0.99, 0.992, 0.995, 0.997, 0.999])\n",
    "ya_ibp_all = np.zeros((runs, len(xs)))\n",
    "yb_ibp_all = np.zeros((runs, len(xs)))\n",
    "\n",
    "for i in range(runs):\n",
    "    tf.set_random_seed(seeds[i])\n",
    "    coreset_size = 0\n",
    "    data_gen = MnistGenerator(val=False)\n",
    "    single_head=False\n",
    "    in_dim, out_dim = data_gen.get_dims()\n",
    "    x_testsets, y_testsets = [], []\n",
    "    task_id=0\n",
    "\n",
    "    tf.reset_default_graph()  \n",
    "    x_train, y_train, x_test, y_test = data_gen.task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, 10, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    model = HIBP_BNN(alphas=[1.]*len(hidden_size), \n",
    "                     input_size=in_dim, \n",
    "                     hidden_size=hidden_size, \n",
    "                     output_size=out_dim, \n",
    "                     training_size=x_train.shape[0], \n",
    "                     no_pred_samples=10,\n",
    "                     num_ibp_samples=10, prev_means=mf_weights, \n",
    "                     prev_log_variances=mf_variances, \n",
    "                     prev_betas=mf_betas,\n",
    "                     learning_rate=0.001, learning_rate_decay=0.87,\n",
    "                     prior_mean=0.0, prior_var=0.7,\n",
    "                     alpha0=5.0, beta0=1.0,\n",
    "                     lambda_1=5.0, lambda_2=1.0,\n",
    "                     tensorboard_dir='logs_wp',\n",
    "                     name='hibp_anneal',\n",
    "                     use_local_reparam=True, implicit_beta=True,\n",
    "                     hard_Z=False, cutoff=0.1)\n",
    "    model.create_model()\n",
    "    #model.restore(os.path.join(\"logs_wp\", 'graph_{0}'.format('hibp_mnist_hZ_n_ibp_samples100')))\n",
    "    model.train(x_train, y_train, head, no_epochs, bsize)\n",
    "    acc, neg_elbo = model.prediction_acc(x_test, y_test, bsize, head)\n",
    "    print(acc)\n",
    "    print(neg_elbo)\n",
    "\n",
    "#     xs, ya_ibp, yb_ibp  = prune_weights(model, x_test, y_test, bsize, head, xs)\n",
    "#     ya_ibp_all[i, :] = ya_ibp\n",
    "#     yb_ibp_all[i, :] = yb_ibp\n",
    "\n",
    "    model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/weight_pruning_ibp_new_xs.pkl', 'wb') as input_file:\n",
    "    pickle.dump({'xs': xs,\n",
    "                 'ya_ibp': ya_ibp_all,\n",
    "                 'yb_ibp': yb_ibp_all}, input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No IBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [200, 200]\n",
    "batch_size = 128\n",
    "no_epochs = 100\n",
    "runs = 1\n",
    "seeds = [10,11,12,13,14]\n",
    "xs = np.append(0.05 * np.array(range(20)), np.array([0.98, 0.99, 0.999]))\n",
    "ya_all = np.zeros((runs, len(xs)))\n",
    "yb_all = np.zeros((runs, len(xs)))\n",
    "\n",
    "for i in range(runs):\n",
    "    tf.set_random_seed(seeds[i])\n",
    "    np.random.seed(1)\n",
    "    coreset_size = 0\n",
    "    data_gen = MnistGenerator()\n",
    "    single_head=False\n",
    "    in_dim, out_dim = data_gen.get_dims()\n",
    "    task_id=0\n",
    "\n",
    "    tf.reset_default_graph()  \n",
    "    x_train, y_train, x_test, y_test = data_gen.task()\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    model = MFVI_NN(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                    prev_log_variances=mf_variances, \n",
    "                    learning_rate=0.001, learning_rate_decay=0.87,\n",
    "                    prior_mean=0.0, prior_var=0.8,\n",
    "                    tensorboard_dir='logs_wp',\n",
    "                    name='mfvi_wp_mnist_newp_run{0}'.format(i),\n",
    "                    use_local_reparam=True)\n",
    "    #model.restore(os.path.join(\"logs_wp\", 'graph_{0}'.format('hibp_wp_mnist_new_run{0}'.format(i))))\n",
    "    model.train(x_train, y_train, head, no_epochs, bsize)\n",
    "\n",
    "    xs, ya_ibp, yb_ibp  = prune_weights(model, x_test, y_test, bsize, head, xs)\n",
    "    \n",
    "    ya_all[i, :] = ya\n",
    "    yb_all[i, :] = yb\n",
    "\n",
    "    mf_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ibp_ya_mean = np.mean(ya_ibp_all, axis=0)\n",
    "_ibp_ya_std = np.std(ya_ibp_all, axis=0)\n",
    "_ibp_yb_mean = np.mean(yb_ibp_all, axis=0)\n",
    "_ibp_yb_std = np.std(yb_ibp_all, axis=0)\n",
    "_ya_mean = np.mean(ya_all, axis=0)\n",
    "_ya_std = np.std(ya_all, axis=0)\n",
    "_yb_mean = np.mean(yb_all, axis=0)\n",
    "_yb_std = np.std(yb_all, axis=0)\n",
    "\n",
    "def test_error(ls):\n",
    "    return 100*(1-np.array(ls))\n",
    "\n",
    "fig_size = (6, 5)\n",
    "\n",
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 16\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "ax.plot(xs, _ibp_ya_mean,linewidth=lw, color='b')\n",
    "ax.fill_between(xs,\n",
    "               [x - y for x, y in zip(_ibp_ya_mean, _ibp_ya_std)],\n",
    "               [x + y for x, y in zip(_ibp_ya_mean, _ibp_ya_std)],\n",
    "               alpha=0.3, color='b')\n",
    "\n",
    "ax.plot(xs, _ibp_yb_mean, linewidth=lw, color='g')\n",
    "ax.fill_between(xs,\n",
    "               [x - y for x, y in zip(_ibp_yb_mean, _ibp_yb_std)],\n",
    "               [x + y for x, y in zip(_ibp_yb_mean, _ibp_yb_std)],\n",
    "               alpha=0.3, color='g')\n",
    "\n",
    "ax.plot(xs, _ya_mean,linewidth=lw, color='r')\n",
    "ax.fill_between(xs,\n",
    "               [x - y for x, y in zip(_ya_mean, _ya_std)],\n",
    "               [x + y for x, y in zip(_ya_mean, _ya_std)],\n",
    "               alpha=0.3, color='r')\n",
    "\n",
    "ax.plot(xs, _yb_mean, linewidth=lw, color='c')\n",
    "ax.fill_between(xs,\n",
    "               [x - y for x, y in zip(_yb_mean, _yb_std)],\n",
    "               [x + y for x, y in zip(_yb_mean, _yb_std)],\n",
    "               alpha=0.3, color='c')\n",
    "\n",
    "plt.grid(True, which=\"both\", color=grid_color, linewidth=0.1, alpha=0.1)\n",
    "ax.set_xlim(0.0, 0.5)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "x_ticks = np.arange(0.0, 1.5, step=0.5)\n",
    "plt.xticks(x_ticks, fontsize=tick_size)\n",
    "plt.ticklabel_format(axis='y', style='sci')\n",
    "ax = fig.gca()\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(-1, 1))\n",
    "#ax.set_yticklabels([])\n",
    "plt.yscale('log')\n",
    "plt.legend([\"H-IBP $|\\mu|$\", 'H-IBP snr', '$|\\mu|$', 'snr'], fontsize=legend_size, loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Cut off\", fontsize=legend_size)\n",
    "plt.ylabel(\"Test acc\", fontsize=legend_size)\n",
    "plt.xlim(0.0, 1.01)\n",
    "plt.savefig(\"plots/weight_pruning_new_hibp.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.append(0.05 * np.array(range(20)), np.array([0.98, 0.99, 0.999]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/weight_pruning_hibp.pkl', 'wb') as input_file:\n",
    "    pickle.dump({'xs': xs,\n",
    "                 'ya_nnvi': ya_all,\n",
    "                 'yb_nnvi': yb_all,\n",
    "                 'ya_ibp': ya_ibp_all,\n",
    "                 'yb_ibp': yb_ibp_all}, input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/weight_pruning_hibp_runs5.pkl', 'rb') as input_file:\n",
    "    d = pickle.load(input_file)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = d['xs']\n",
    "ya = d['ya_nnvi']\n",
    "yb = d['yb_nnvi']\n",
    "ya_hibp = d['ya_ibp']\n",
    "yb_hibp = d['yb_ibp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb_ibp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hibp_ya_mean = np.mean(ya_hibp, axis=0)\n",
    "_hibp_ya_std = np.std(ya_hibp, axis=0)\n",
    "_hibp_yb_mean = np.mean(yb_hibp, axis=0)\n",
    "_hibp_yb_std = np.std(yb_hibp, axis=0)\n",
    "_ya_mean = np.mean(ya, axis=0)\n",
    "_ya_std = np.std(ya, axis=0)\n",
    "_yb_mean = np.mean(yb, axis=0)\n",
    "_yb_std = np.std(yb, axis=0)\n",
    "\n",
    "def test_error(ls):\n",
    "    return 100*(1-np.array(ls))\n",
    "\n",
    "fig_size = (6, 5)\n",
    "\n",
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 16\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "ax.plot(xs, test_error(_hibp_ya_mean), linewidth=lw, color='b')\n",
    "ax.fill_between(xs,\n",
    "               test_error([x - y for x, y in zip(_hibp_ya_mean, _hibp_ya_std)]),\n",
    "               test_error([x + y for x, y in zip(_hibp_ya_mean, _hibp_ya_std)]),\n",
    "               alpha=0.3, color='b')\n",
    "\n",
    "ax.plot(xs, test_error(_hibp_yb_mean), linewidth=lw, color='g')\n",
    "ax.fill_between(xs,\n",
    "               test_error([x - y for x, y in zip(_hibp_yb_mean, _hibp_yb_std)]),\n",
    "               test_error([x + y for x, y in zip(_hibp_yb_mean, _hibp_yb_std)]),\n",
    "               alpha=0.3, color='g')\n",
    "\n",
    "ax.plot(xs, test_error(_ya_mean), linewidth=lw, color='r')\n",
    "ax.fill_between(xs,\n",
    "               test_error([x - y for x, y in zip(_ya_mean, _ya_std)]),\n",
    "               test_error([x + y for x, y in zip(_ya_mean, _ya_std)]),\n",
    "               alpha=0.3, color='r')\n",
    "\n",
    "ax.plot(xs, test_error(_yb_mean), linewidth=lw, color='c')\n",
    "ax.fill_between(xs,\n",
    "               test_error([x - y for x, y in zip(_yb_mean, _yb_std)]),\n",
    "               test_error([x + y for x, y in zip(_yb_mean, _yb_std)]),\n",
    "               alpha=0.3, color='c')\n",
    "\n",
    "plt.grid(True, which=\"both\", color=grid_color, linewidth=0.1, alpha=0.1)\n",
    "ax.set_xlim(0.0, 0.5)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "x_ticks = np.arange(0.0, 1.5, step=0.5)\n",
    "plt.xticks(x_ticks, fontsize=tick_size)\n",
    "plt.ticklabel_format(axis='y')\n",
    "ax = fig.gca()\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(-1, 1))\n",
    "#ax.set_yticklabels([])\n",
    "#plt.yscale('log')\n",
    "plt.legend([\"H-IBP $|\\mu|$\", 'H-IBP snr', '$|\\mu|$', 'snr'], fontsize=legend_size, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Cut off\", fontsize=legend_size)\n",
    "plt.ylabel(\"Test error\", fontsize=legend_size)\n",
    "plt.xlim(0.4, 1.0)\n",
    "plt.savefig(\"plots/weight_pruning_new_hibp_x5.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_hibp_wp_hibp_wp_lr_run0\n",
    "hidden_size = [200, 200]\n",
    "batch_size = 512\n",
    "no_epochs = 200\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "Zs = []\n",
    "coreset_size = 0\n",
    "data_gen = MnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "task_id=0\n",
    "runs=5\n",
    "tag = 'hibp'\n",
    "for run in range(runs):\n",
    "    name='ibp_wp_{0}_run{1}'.format(tag, run)\n",
    "\n",
    "    tf.reset_default_graph()  \n",
    "    x_train, y_train, x_test, y_test = data_gen.task()\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, 10, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    model = HIBP_BNN(alphas=[1.]*len(hidden_size), \n",
    "                           input_size=in_dim, \n",
    "                           hidden_size=hidden_size, \n",
    "                           output_size=out_dim, \n",
    "                           training_size=x_train.shape[0], \n",
    "                           no_pred_samples=100,\n",
    "                           num_ibp_samples=10, prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, \n",
    "                           prev_betas=mf_betas,\n",
    "                           learning_rate=0.001, learning_rate_decay=0.87,\n",
    "                           prior_mean=0.0, prior_var=0.1,\n",
    "                           alpha0=5.0, beta0=1.0,\n",
    "                           lambda_1=1.0, lambda_2=1.0,\n",
    "                           tensorboard_dir='logs_wp',\n",
    "                           name='hibp_wp_hibp_wp_lr_run{0}'.format(run),\n",
    "                           use_local_reparam=True, implicit_beta=True)\n",
    "\n",
    "    model.create_model()\n",
    "    model.restore(os.path.join(\"logs_wp\", 'graph_{0}'.format('hibp_wp_hibp_wp_lr_run{0}').format(run)))\n",
    "#     xs, ya, yb = model.prune_weights(x_test, y_test, head)\n",
    "#     ya_all[run, :] = ya\n",
    "#     yb_all[run, :] = yb\n",
    "\n",
    "    Zs.append(model.sess.run(model.Z, feed_dict={model.x: x_test, model.task_idx: task_id, model.training: False}))\n",
    "\n",
    "    model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers=2\n",
    "num_tasks=1\n",
    "num_runs=5\n",
    "_Z_hibp = []\n",
    "for j in range(num_tasks*num_layers):\n",
    "    tmp = []\n",
    "    for i in range(num_runs):\n",
    "        tmp.append(np.squeeze(Zs[i][j]))\n",
    "    _Z_hibp.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__Z_hibp = [np.concatenate(_Z_hibp[i], axis=0) for i in range(num_tasks*num_layers)]\n",
    "__Z_hibp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_Z_hibp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.280326372\n",
      "Epoch: 0006 cost= 0.028386525\n",
      "Z: (1, ?, 200)\n",
      "Z: (1, ?, 200)\n",
      "INFO:tensorflow:Restoring parameters from logs_wp/graph_ibp_wp_ibp_l2_wp_n_run0/model.ckpt\n",
      "Epoch: 0001 cost= 0.279246731\n",
      "Epoch: 0006 cost= 0.027772299\n",
      "Z: (1, ?, 200)\n",
      "Z: (1, ?, 200)\n",
      "INFO:tensorflow:Restoring parameters from logs_wp/graph_ibp_wp_ibp_l2_wp_n_run1/model.ckpt\n",
      "Epoch: 0001 cost= 0.281405082\n",
      "Epoch: 0006 cost= 0.026062425\n",
      "Z: (1, ?, 200)\n",
      "Z: (1, ?, 200)\n",
      "INFO:tensorflow:Restoring parameters from logs_wp/graph_ibp_wp_ibp_l2_wp_n_run2/model.ckpt\n",
      "Epoch: 0001 cost= 0.279183709\n",
      "Epoch: 0006 cost= 0.027951502\n",
      "Z: (1, ?, 200)\n",
      "Z: (1, ?, 200)\n",
      "INFO:tensorflow:Restoring parameters from logs_wp/graph_ibp_wp_ibp_l2_wp_n_run3/model.ckpt\n",
      "Epoch: 0001 cost= 0.290161883\n",
      "Epoch: 0006 cost= 0.029862621\n",
      "Z: (1, ?, 200)\n",
      "Z: (1, ?, 200)\n",
      "INFO:tensorflow:Restoring parameters from logs_wp/graph_ibp_wp_ibp_l2_wp_n_run4/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [200, 200]\n",
    "batch_size = 128\n",
    "no_epochs = 100\n",
    "Zs_ibp = []\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "for run in range(runs):\n",
    "    data_gen = MnistGenerator()\n",
    "    single_head=True\n",
    "    in_dim, out_dim = data_gen.get_dims()\n",
    "    task_id=0\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    x_train, y_train, x_test, y_test = data_gen.task()\n",
    "\n",
    "    tag = 'ibp_l2_wp_n'\n",
    "    name='ibp_wp_{0}_run{1}'.format(tag, run)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, 10, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    model = IBP_BNN(input_size=in_dim, \n",
    "                               hidden_size=hidden_size, \n",
    "                               output_size=out_dim, \n",
    "                               training_size=x_train.shape[0], \n",
    "                               no_pred_samples=100,\n",
    "                               num_ibp_samples=10, prev_means=mf_weights, \n",
    "                               prev_log_variances=mf_variances, \n",
    "                               prev_betas=mf_betas,\n",
    "                               learning_rate=0.001, learning_rate_decay=0.87,\n",
    "                               prior_mean=0.0, prior_var=0.1,\n",
    "                               alpha0=5.0, beta0=1.0,\n",
    "                               lambda_1=1.0, lambda_2=1.0,\n",
    "                               tensorboard_dir='logs_wp',\n",
    "                               name=name,\n",
    "                               use_local_reparam=True, implicit_beta=True)\n",
    "\n",
    "    model.create_model()\n",
    "    model.restore(model.log_folder)\n",
    "\n",
    "    Zs_ibp.append(model.sess.run(model.Z, feed_dict={model.x: x_test, model.task_idx: task_id,\n",
    "                                                     model.training: False}))\n",
    "\n",
    "    model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Zs_ibp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zs_ibp[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers=2\n",
    "num_tasks=1\n",
    "num_runs=5\n",
    "_Z_ibp = []\n",
    "for j in range(num_tasks*num_layers):\n",
    "    tmp = []\n",
    "    for i in range(num_runs):\n",
    "        tmp.append(np.squeeze(Zs_ibp[i][j]))\n",
    "    _Z_ibp.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__Z_ibp = [np.concatenate(_Z_ibp[i], axis=0) for i in range(num_tasks*num_layers)]\n",
    "__Z_ibp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(__Z_ibp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Box plot params\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 16\n",
    "label_size = 16\n",
    "tick_size = 14\n",
    "legend_size = 16\n",
    "\n",
    "num_layers = 2\n",
    "ticks = ['H-IBP', 'IBP']\n",
    "fig_size=(4, 3)\n",
    "\n",
    "def set_box_pairs(bp):\n",
    "    plt.setp(bp['boxes'], color='black', lw=1.5)\n",
    "    plt.setp(bp['whiskers'], color='black', lw=1.5)\n",
    "    \n",
    "    \n",
    "    plt.setp(bp['caps'][0], color='#D7191C', lw=2)\n",
    "    plt.setp(bp['caps'][1], color='#D7191C', lw=2)\n",
    "    plt.setp(bp['caps'][2], color='#2C7BB6', lw=2)\n",
    "    plt.setp(bp['caps'][3], color='#2C7BB6', lw=2)\n",
    "    plt.setp(bp['medians'][0], color='#D7191C', lw=2)\n",
    "    plt.setp(bp['medians'][1], color='#2C7BB6', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_layers = 2\n",
    "plt.figure(figsize=fig_size)\n",
    "bpl = plt.boxplot([np.sum(np.asarray(np.squeeze(__Z_hibp[i] > 0.1)).astype(int), axis=1).reshape(-1) for i in range(num_layers)],\n",
    "                  positions=[-0.4, 0.4], sym='', widths=0.6)\n",
    "bpr = plt.boxplot([np.sum(np.asarray(np.squeeze(__Z_ibp[i] > 0.1)).astype(int), axis=1).reshape(-1) for i in range(num_layers)],\n",
    "                  positions=[2. - 0.4, 2. + 0.4], sym='', widths=0.6)\n",
    "set_box_pairs(bpl) # colors are from http://colorbrewer2.org/\n",
    "set_box_pairs(bpr)\n",
    "\n",
    "# draw temporary red and blue lines and use them to create a legend\n",
    "hb, = plt.plot([], '#D7191C')\n",
    "hr, = plt.plot([], '#2C7BB6')\n",
    "plt.legend((hb, hr), ('Layer 1', 'Layer 2'), loc='lower left')\n",
    "hb.set_visible(False)\n",
    "hr.set_visible(False)\n",
    "\n",
    "plt.grid(True, which=\"both\", color=grid_color, linewidth=0.1, alpha=0.1)\n",
    "plt.xticks(range(0, len(ticks) * 2, 2), ticks, fontsize=tick_size)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "plt.xlim(-1, len(ticks)*2 - 1)\n",
    "plt.ylim(19, 35)\n",
    "plt.ylabel('# neurons', fontsize=legend_size)\n",
    "#plt.xlabel('Layer', fontsize=label_size)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/Zs_mnist_ibp_vs_hibp.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(Z[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 22\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "fig_size=(6, 5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "imgplot = ax.imshow(Z[0][0, :50,:60], cmap=plt.cm.Greys, vmin=0, vmax=1)\n",
    "x_ticks = np.arange(0.0, 100, step=50)\n",
    "plt.xticks(x_ticks, fontsize=tick_size)\n",
    "plt.xlabel('$k$', fontsize=legend_size)\n",
    "plt.yticks([], [])\n",
    "#cbar_ax = fig.add_axes([0.95, 0.21, 0.05, 0.59])\n",
    "#fig.colorbar(imgplot, cax=cbar_ax)\n",
    "#cbar_ax.tick_params(labelsize=legend_size) \n",
    "plt.savefig('plots/hibp_weight_pruning_Z1.pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 22\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "imgplot = ax.imshow(Z[1][0, :50,:60], cmap=plt.cm.Greys, vmin=0, vmax=1)\n",
    "x_ticks = np.arange(0.0, 100, step=50)\n",
    "plt.xticks(x_ticks, fontsize=tick_size)\n",
    "plt.xlabel('$k$', fontsize=legend_size)\n",
    "plt.yticks([], [])\n",
    "cbar_ax = fig.add_axes([0.95, 0.13, 0.05, 0.75])\n",
    "fig.colorbar(imgplot, cax=cbar_ax)\n",
    "cbar_ax.tick_params(labelsize=legend_size) \n",
    "plt.savefig('plots/hibp_weight_pruning_Z2.pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 22\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# imgplot = ax.imshow(np.squeeze(Z[0])[:50,:100], cmap=plt.cm.Greys, vmin=0, vmax=1)\n",
    "# x_ticks = np.arange(0.0, 100, step=50)\n",
    "# plt.xticks(x_ticks, fontsize=tick_size)\n",
    "# plt.xlabel('$k$', fontsize=legend_size)\n",
    "# plt.yticks([], [])\n",
    "# cbar_ax = fig.add_axes([0.95, 0.21, 0.05, 0.59])\n",
    "# fig.colorbar(imgplot, cax=cbar_ax)\n",
    "# cbar_ax.tick_params(labelsize=legend_size) \n",
    "# plt.savefig('plots/weight_pruning_Z.pdf', bbox_inches='tight')\n",
    "# fig.show()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(4, 4))\n",
    "for i in range(2):\n",
    "    imgplot = ax[i].imshow(Z[i][0, :20, :60], cmap=plt.cm.Greys, vmin=0, vmax=1)\n",
    "    ax[0].set_xticks(np.arange(0.0, 100, step=50))\n",
    "    ax[0].set_xticklabels([], fontsize=tick_size)\n",
    "    ax[i].set_yticks([], [])\n",
    "    ax[i].set_yticklabels([], fontsize=tick_size)\n",
    "    if i == 1:\n",
    "        ax[i].set_xlabel('$k$', fontsize=legend_size)\n",
    "        ax[i].set_xticklabels(np.arange(0, 100, step=50), fontsize=tick_size)\n",
    "        ax[i].set_xticks(np.arange(0.0, 100, step=50))\n",
    "        #ax[i].set_yticklabels([])\n",
    "        cbar_ax = fig.add_axes([1.0, 0.22, 0.05, 0.69])\n",
    "        fig.colorbar(imgplot, cax=cbar_ax, ticks = [0.0, 0.5, 1.0])\n",
    "        cbar_ax.tick_params(labelsize=legend_size) \n",
    "        # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "fig.tight_layout()\n",
    "plt.savefig('plots/hibp_weight_pruning_Zs.pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_pruning_runs5_ibp_l2_wp_n.pkl\n",
    "with open('results/weight_pruning_ibp_new_xs.pkl', 'rb') as input_file:\n",
    "    d = pickle.load(input_file)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_ibp = d['xs']\n",
    "ya_ibp = d['ya_ibp']\n",
    "yb_ibp = d['yb_ibp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_pruning_runs5_ibp_l2_wp_n.pkl\n",
    "with open('results/weight_pruning_hibp_new_xs.pkl', 'rb') as input_file:\n",
    "    d = pickle.load(input_file)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_hibp = d['xs']\n",
    "ya_hibp = d['ya_ibp']\n",
    "yb_hibp = d['yb_ibp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ibp_ya_mean = np.mean(ya_ibp, axis=0)\n",
    "_ibp_ya_std = np.std(ya_ibp, axis=0)\n",
    "_ibp_yb_mean = np.mean(yb_ibp, axis=0)\n",
    "_ibp_yb_std = np.std(yb_ibp, axis=0)\n",
    "_hibp_ya_mean = np.mean(ya_hibp, axis=0)\n",
    "_hibp_ya_std = np.std(ya_hibp, axis=0)\n",
    "_hibp_yb_mean = np.mean(yb_hibp, axis=0)\n",
    "_hibp_yb_std = np.std(yb_hibp, axis=0)\n",
    "\n",
    "def test_error(ls):\n",
    "    return 100*(1-np.array(ls))\n",
    "\n",
    "fig_size = (6, 5)\n",
    "\n",
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 16\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_ibp_ya_mean), linewidth=lw, color='b')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_ibp_ya_mean, _ibp_ya_std)]),\n",
    "               test_error([x + y for x, y in zip(_ibp_ya_mean, _ibp_ya_std)]),\n",
    "               alpha=0.2, color='b')\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_ibp_yb_mean), linewidth=lw, color='r')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_ibp_yb_mean, _ibp_yb_std)]),\n",
    "               test_error([x + y for x, y in zip(_ibp_yb_mean, _ibp_yb_std)]),\n",
    "               alpha=0.2, color='r')\n",
    "\n",
    "ax.plot(xs_hibp, test_error(_hibp_ya_mean), linewidth=lw, color='g')\n",
    "ax.fill_between(xs_hibp,\n",
    "               test_error([x - y for x, y in zip(_hibp_ya_mean, _hibp_ya_std)]),\n",
    "               test_error([x + y for x, y in zip(_hibp_ya_mean, _hibp_ya_std)]),\n",
    "               alpha=0.2, color='g')\n",
    "\n",
    "ax.plot(xs_hibp, test_error(_hibp_yb_mean), linewidth=lw, color='c')\n",
    "ax.fill_between(xs_hibp,\n",
    "               test_error([x - y for x, y in zip(_hibp_yb_mean, _hibp_yb_std)]),\n",
    "               test_error([x + y for x, y in zip(_hibp_yb_mean, _hibp_yb_std)]),\n",
    "               alpha=0.2, color='c')\n",
    "\n",
    "plt.grid(True, which=\"both\", color=grid_color, linewidth=0.1, alpha=0.1)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "x_ticks = np.arange(0.0, 1.5, step=0.02)\n",
    "plt.xticks(x_ticks, fontsize=tick_size)\n",
    "plt.ticklabel_format(axis='y', style='sci')\n",
    "ax = fig.gca()\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(-1, 1))\n",
    "#ax.set_yticklabels([])\n",
    "#plt.yscale('log')\n",
    "#plt.legend([\"IBP $|\\mu|$\", 'IBP snr'], fontsize=legend_size, loc='top left')\n",
    "plt.legend([\"IBP $|\\mu|$\", 'IBP snr',\n",
    "           \"H-IBP $|\\mu|$\", 'H-IBP snr',], fontsize=legend_size, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Cut off\", fontsize=legend_size)\n",
    "plt.ylabel(\"Test error\", fontsize=legend_size)\n",
    "plt.xlim(0.9, 1.0)\n",
    "plt.savefig(\"plots/weight_pruning_mnist_ibp_vs_hibp.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [200, 200]\n",
    "batch_size = 512\n",
    "no_epochs = 100\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "data_gen = MnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "task_id=0\n",
    "    \n",
    "tf.reset_default_graph()  \n",
    "x_train, y_train, x_test, y_test = data_gen.task()\n",
    "x_testsets.append(x_test)\n",
    "y_testsets.append(y_test)\n",
    "\n",
    "tag = 'ibp_l2_wp_n'\n",
    "run = 0\n",
    "name='ibp_wp_{0}_run{1}'.format(tag, run)\n",
    "\n",
    "# Set the readout head to train\n",
    "head = 0 if single_head else task_id\n",
    "bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "# Train network with maximum likelihood to initialize first model\n",
    "if task_id == 0:\n",
    "    ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "    ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "    mf_weights = ml_model.get_weights()\n",
    "    mf_variances = None\n",
    "    mf_betas = None\n",
    "    ml_model.close_session()\n",
    "\n",
    "# Train on non-coreset data\n",
    "model = IBP_BNN(input_size=in_dim, \n",
    "                           hidden_size=hidden_size, \n",
    "                           output_size=out_dim, \n",
    "                           training_size=x_train.shape[0], \n",
    "                           no_pred_samples=100,\n",
    "                           num_ibp_samples=10, prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, \n",
    "                           prev_betas=mf_betas,\n",
    "                           learning_rate=0.001, learning_rate_decay=0.87,\n",
    "                           prior_mean=0.0, prior_var=0.1,\n",
    "                           alpha0=5.0, beta0=1.0,\n",
    "                           lambda_1=1.0, lambda_2=1.0,\n",
    "                           tensorboard_dir='logs_wp',\n",
    "                           name=name,\n",
    "                           use_local_reparam=True, implicit_beta=True)\n",
    "\n",
    "model.create_model()\n",
    "model.restore(model.log_folder)\n",
    "\n",
    "Z_ibp = model.sess.run(model.Z, feed_dict={model.x: x_test, model.task_idx: task_id, model.training: False})\n",
    "\n",
    "model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.squeeze(Z_ibp[0]), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Box plot params\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 16\n",
    "label_size = 16\n",
    "tick_size = 14\n",
    "legend_size = 16\n",
    "\n",
    "num_layers = 2\n",
    "ticks = ['H-IBP', 'IBP']\n",
    "fig_size=(4, 3)\n",
    "\n",
    "def set_box_pairs(bp):\n",
    "    plt.setp(bp['boxes'], color='black', lw=1.5)\n",
    "    plt.setp(bp['whiskers'], color='black', lw=1.5)\n",
    "    \n",
    "    \n",
    "    plt.setp(bp['caps'][0], color='#D7191C', lw=2)\n",
    "    plt.setp(bp['caps'][1], color='#D7191C', lw=2)\n",
    "    plt.setp(bp['caps'][2], color='#2C7BB6', lw=2)\n",
    "    plt.setp(bp['caps'][3], color='#2C7BB6', lw=2)\n",
    "    plt.setp(bp['medians'][0], color='#D7191C', lw=2)\n",
    "    plt.setp(bp['medians'][1], color='#2C7BB6', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "plt.figure(figsize=fig_size)\n",
    "bpl = plt.boxplot([np.sum(np.asarray(np.squeeze(Z[i] > 0.1)).astype(int), axis=1).reshape(-1) for i in range(num_layers)],\n",
    "                  positions=[-0.4, 0.4], sym='', widths=0.6)\n",
    "bpr = plt.boxplot([np.sum(np.asarray(np.squeeze(Z_ibp[i] > 0.1)).astype(int), axis=1).reshape(-1) for i in range(num_layers)],\n",
    "                  positions=[2. - 0.4, 2. + 0.4], sym='', widths=0.6)\n",
    "set_box_pairs(bpl) # colors are from http://colorbrewer2.org/\n",
    "set_box_pairs(bpr)\n",
    "\n",
    "# draw temporary red and blue lines and use them to create a legend\n",
    "hb, = plt.plot([], '#D7191C')\n",
    "hr, = plt.plot([], '#2C7BB6')\n",
    "plt.legend((hb, hr), ('Layer 1', 'Layer 2'))\n",
    "hb.set_visible(False)\n",
    "hr.set_visible(False)\n",
    "\n",
    "plt.grid(True, which=\"both\", color=grid_color, linewidth=0.1, alpha=0.1)\n",
    "plt.xticks(range(0, len(ticks) * 2, 2), ticks, fontsize=tick_size)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "plt.xlim(-1, len(ticks)*2 - 1)\n",
    "#plt.xlabel('Layer', fontsize=label_size)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/Zs_mnist_ibp_vs_hibp.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Variational Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pick file\n",
    "with open('sparse_vd/weight_pruning_svd.pkl', 'rb') as input_file:\n",
    "    d_svd = pickle.load(input_file)\n",
    "d_svd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/weight_pruning_hibp_new_xs.pkl', 'rb') as input_file:\n",
    "    d = pickle.load(input_file)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_xs = d_svd['xs']\n",
    "svd_ya_mean = np.mean(d_svd['ya'], axis=0)\n",
    "svd_ya_std = np.std(d_svd['ya'], axis=0)\n",
    "svd_yb_mean = np.mean(d_svd['yb'], axis=0)\n",
    "svd_yb_std = np.std(d_svd['yb'], axis=0)\n",
    "hibp_xs = d['xs']\n",
    "hibp_ya_mean = np.mean(d['ya_ibp'], axis=0)\n",
    "hibp_ya_std = np.std(d['ya_ibp'], axis=0)\n",
    "hibp_yb_mean = np.mean(d['yb_ibp'], axis=0)\n",
    "hibp_yb_std = np.std(d['yb_ibp'], axis=0)\n",
    "\n",
    "def test_error(ls):\n",
    "    return 100*(1-np.array(ls))\n",
    "\n",
    "fig_size = (6, 5)\n",
    "\n",
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 16\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "ax.plot(hibp_xs, test_error(hibp_ya_mean), linewidth=lw, color='b')\n",
    "ax.fill_between(hibp_xs,\n",
    "               test_error([x - y for x, y in zip(hibp_ya_mean, hibp_ya_std)]),\n",
    "               test_error([x + y for x, y in zip(hibp_ya_mean, hibp_ya_std)]),\n",
    "               alpha=0.2, color='b')\n",
    "\n",
    "ax.plot(hibp_xs, test_error(hibp_yb_mean), linewidth=lw, color='r')\n",
    "ax.fill_between(hibp_xs,\n",
    "               test_error([x - y for x, y in zip(hibp_yb_mean, hibp_yb_std)]),\n",
    "               test_error([x + y for x, y in zip(hibp_yb_mean, hibp_yb_std)]),\n",
    "               alpha=0.2, color='r')\n",
    "\n",
    "ax.plot(svd_xs, test_error(svd_ya_mean), linewidth=lw, color='g')\n",
    "ax.fill_between(svd_xs,\n",
    "               test_error([x - y for x, y in zip(svd_ya_mean, svd_ya_std)]),\n",
    "               test_error([x + y for x, y in zip(svd_ya_mean, svd_ya_std)]),\n",
    "               alpha=0.2, color='g')\n",
    "\n",
    "ax.plot(svd_xs, test_error(svd_yb_mean), linewidth=lw, color='c')\n",
    "ax.fill_between(svd_xs,\n",
    "               test_error([x - y for x, y in zip(svd_yb_mean, svd_yb_std)]),\n",
    "               test_error([x + y for x, y in zip(svd_yb_mean, svd_yb_std)]),\n",
    "               alpha=0.2, color='c')\n",
    "\n",
    "plt.grid(True, which=\"both\", color=grid_color, linewidth=0.1, alpha=0.1)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "x_ticks = np.arange(0.0, 1.5, step=0.02)\n",
    "plt.xticks(x_ticks, fontsize=tick_size)\n",
    "plt.ticklabel_format(axis='y', style='sci')\n",
    "ax = fig.gca()\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(-1, 1))\n",
    "#ax.set_yticklabels([])\n",
    "#plt.yscale('log')\n",
    "#plt.legend([\"IBP $|\\mu|$\", 'IBP snr'], fontsize=legend_size, loc='top left')\n",
    "plt.legend([\"H-IBP $|\\mu|$\", 'H-IBP snr',\n",
    "           \"SVD $|\\mu|$\", 'SVD snr',], fontsize=legend_size, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Cut off\", fontsize=legend_size)\n",
    "plt.ylabel(\"Test error\", fontsize=legend_size)\n",
    "plt.xlim(0.9, 1.0)\n",
    "plt.savefig(\"plots/weight_pruning_mnist_hibp_vs_svd.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local reparam + improved MFVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_pruning_runs5_ibp_l2_wp_n.pkl\n",
    "with open('results/weight_pruning_runs5_ibp_wp_lr_n.pkl', 'rb') as input_file:\n",
    "    d = pickle.load(input_file)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_ibp = d['xs']\n",
    "ya = d['ya_nnvi']\n",
    "yb = d['yb_nnvi']\n",
    "ya_ibp = d['ya_ibp']\n",
    "yb_ibp = d['yb_ibp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_ibp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ya[:,0]) # no pruning mfvi acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_pruning_runs5_ibp_l2_wp_n.pkl\n",
    "with open('results/weight_pruning_hibp_new_xs.pkl', 'rb') as input_file:\n",
    "    d = pickle.load(input_file)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_hibp = d['xs']\n",
    "ya_hibp = d['ya_ibp']\n",
    "yb_hibp = d['yb_ibp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_hibp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ya_hibp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hibp_ya_mean = np.mean(ya_hibp, axis=0)\n",
    "_hibp_ya_std = np.std(ya_hibp, axis=0)\n",
    "_hibp_yb_mean = np.mean(yb_hibp, axis=0)\n",
    "_hibp_yb_std = np.std(yb_hibp, axis=0)\n",
    "_ya_mean = np.mean(ya, axis=0)\n",
    "_ya_std = np.std(ya, axis=0)\n",
    "_yb_mean = np.mean(yb, axis=0)\n",
    "_yb_std = np.std(yb, axis=0)\n",
    "\n",
    "def test_error(ls):\n",
    "    return 100*(1-np.array(ls))\n",
    "\n",
    "fig_size = (6, 5)\n",
    "\n",
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 16\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_hibp_ya_mean), linewidth=lw, color='b')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_hibp_ya_mean, _hibp_ya_std)]),\n",
    "               test_error([x + y for x, y in zip(_hibp_ya_mean, _hibp_ya_std)]),\n",
    "               alpha=0.3, color='b')\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_hibp_yb_mean), linewidth=lw, color='g')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_hibp_yb_mean, _hibp_yb_std)]),\n",
    "               test_error([x + y for x, y in zip(_hibp_yb_mean, _hibp_yb_std)]),\n",
    "               alpha=0.3, color='g')\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_ya_mean), linewidth=lw, color='r')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_ya_mean, _ya_std)]),\n",
    "               test_error([x + y for x, y in zip(_ya_mean, _ya_std)]),\n",
    "               alpha=0.3, color='r')\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_yb_mean), linewidth=lw, color='c')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_yb_mean, _yb_std)]),\n",
    "               test_error([x + y for x, y in zip(_yb_mean, _yb_std)]),\n",
    "               alpha=0.3, color='c')\n",
    "\n",
    "plt.grid(True, which=\"both\", color=grid_color, linewidth=0.1, alpha=0.1)\n",
    "ax.set_xlim(0.0, 0.5)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "x_ticks = np.arange(0.0, 1.5, step=0.1)\n",
    "plt.xticks(x_ticks, fontsize=tick_size)\n",
    "plt.ticklabel_format(axis='y')\n",
    "ax = fig.gca()\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(-1, 1))\n",
    "#ax.set_yticklabels([])\n",
    "#plt.yscale('log')\n",
    "plt.legend([\"H-IBP $|\\mu|$\", 'H-IBP snr', '$|\\mu|$', 'snr'], fontsize=legend_size, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Cut off\", fontsize=legend_size)\n",
    "plt.ylabel(\"Test error\", fontsize=legend_size)\n",
    "plt.xlim(0.6, 1.0)\n",
    "plt.savefig(\"plots/weight_pruning_new_hibp_x5_better_mfvi.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ibp_ya_mean = np.mean(ya_ibp, axis=0)\n",
    "_ibp_ya_std = np.std(ya_ibp, axis=0)\n",
    "_ibp_yb_mean = np.mean(yb_ibp, axis=0)\n",
    "_ibp_yb_std = np.std(yb_ibp, axis=0)\n",
    "_hibp_ya_mean = np.mean(ya_hibp, axis=0)\n",
    "_hibp_ya_std = np.std(ya_hibp, axis=0)\n",
    "_hibp_yb_mean = np.mean(yb_hibp, axis=0)\n",
    "_hibp_yb_std = np.std(yb_hibp, axis=0)\n",
    "\n",
    "def test_error(ls):\n",
    "    return 100*(1-np.array(ls))\n",
    "\n",
    "fig_size = (6, 5)\n",
    "\n",
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 16\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_ibp_ya_mean), linewidth=lw, color='b')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_ibp_ya_mean, _ibp_ya_std)]),\n",
    "               test_error([x + y for x, y in zip(_ibp_ya_mean, _ibp_ya_std)]),\n",
    "               alpha=0.2, color='b')\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_ibp_yb_mean), linewidth=lw, color='r')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_ibp_yb_mean, _ibp_yb_std)]),\n",
    "               test_error([x + y for x, y in zip(_ibp_yb_mean, _ibp_yb_std)]),\n",
    "               alpha=0.2, color='r')\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_hibp_ya_mean), linewidth=lw, color='g')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_hibp_ya_mean, _hibp_ya_std)]),\n",
    "               test_error([x + y for x, y in zip(_hibp_ya_mean, _hibp_ya_std)]),\n",
    "               alpha=0.2, color='g')\n",
    "\n",
    "ax.plot(xs_ibp, test_error(_hibp_yb_mean), linewidth=lw, color='c')\n",
    "ax.fill_between(xs_ibp,\n",
    "               test_error([x - y for x, y in zip(_hibp_yb_mean, _hibp_yb_std)]),\n",
    "               test_error([x + y for x, y in zip(_hibp_yb_mean, _hibp_yb_std)]),\n",
    "               alpha=0.2, color='c')\n",
    "\n",
    "plt.grid(True, which=\"both\", color=grid_color, linewidth=0.1, alpha=0.1)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "x_ticks = np.arange(0.0, 1.5, step=0.02)\n",
    "plt.xticks(x_ticks, fontsize=tick_size)\n",
    "plt.ticklabel_format(axis='y', style='sci')\n",
    "ax = fig.gca()\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(-1, 1))\n",
    "#ax.set_yticklabels([])\n",
    "#plt.yscale('log')\n",
    "#plt.legend([\"IBP $|\\mu|$\", 'IBP snr'], fontsize=legend_size, loc='top left')\n",
    "plt.legend([\"IBP $|\\mu|$\", 'IBP snr',\n",
    "           \"H-IBP $|\\mu|$\", 'H-IBP snr',], fontsize=legend_size, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Cut off\", fontsize=legend_size)\n",
    "plt.ylabel(\"Test error\", fontsize=legend_size)\n",
    "plt.xlim(0.9, 1.0)\n",
    "#plt.savefig(\"plots/weight_pruning_mnist_ibp_vs_hibp.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu-cuda8] *",
   "language": "python",
   "name": "conda-env-tf-gpu-cuda8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
