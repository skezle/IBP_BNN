{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From alg/cla_models_multihead.py:9: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "import pdb\n",
    "import re\n",
    "from run_split import SplitMnistGenerator\n",
    "from alg.cla_models_multihead import MFVI_IBP_NN, MFVI_NN, Vanilla_NN\n",
    "from alg.utils import get_scores, concatenate_results\n",
    "from alg.vcl import run_vcl\n",
    "from copy import deepcopy\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFVI_IBP_NN_prune(MFVI_IBP_NN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, training_size, num_ibp_samples=10,\n",
    "                 no_train_samples=10, no_pred_samples=100, prev_means=None, prev_log_variances=None,\n",
    "                 prev_betas=None, learning_rate=0.0001,\n",
    "                 prior_mean=0, prior_var=1, alpha0=5., beta0=1., lambda_1=1., lambda_2=1.,\n",
    "                 tensorboard_dir='logs', name='ibp_wp', min_temp=0.5, tb_logging=True,\n",
    "                output_tb_gradients=True):\n",
    "\n",
    "        super(MFVI_IBP_NN_prune, self).__init__(input_size, hidden_size, output_size, training_size,\n",
    "                 no_train_samples, no_pred_samples, num_ibp_samples, prev_means, prev_log_variances,\n",
    "                 prev_betas, learning_rate,\n",
    "                 prior_mean, prior_var, alpha0, beta0, lambda_1, lambda_2,\n",
    "                 tensorboard_dir, name, min_temp, tb_logging, output_tb_gradients=True)\n",
    "\n",
    "\n",
    "    def prune_weights(self, X_test, Y_test, task_id):\n",
    "        \"\"\" Performs weight pruning.\n",
    "        \n",
    "        Z is at a data level doesn't make sense to introduce this intot he mask over weights which get zeroed \n",
    "        out. Simlpy running the accuracy over the graph will entail that Z is incorporated into the \n",
    "        matrix math for the prediction calculations.\n",
    "        \n",
    "        Args:\n",
    "            X_test: numpy array\n",
    "            Y_test: numpy array\n",
    "            task_id: int\n",
    "        :return: cutoffs, accs via naive pruning, accs via snr pruning,\n",
    "        weight values, sigma values of network\n",
    "        \"\"\"\n",
    "\n",
    "        def reset_weights(pr_mus, pr_sigmas, _mus, _sigmas):\n",
    "            \"\"\" Reset weights of graph to original values\n",
    "            Args:\n",
    "                pr_mus: list of tf variables which have been pruned\n",
    "                pr_sigmas: list of tf variables which have been pruned\n",
    "                _mus: list of cached mus in numpy\n",
    "                _sigmas: list of cached sigmas in numpy\n",
    "            \"\"\"\n",
    "\n",
    "            for v, _v in zip(pr_mus, _mus):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "            for v, _v in zip(pr_sigmas, _sigmas):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "        def pruning(remove_pct, weightvalues, sigmavalues,\n",
    "                    weights, sigmas, uncert_pruning=True):\n",
    "            \"\"\" Performs weight pruning experiment\n",
    "            Args:\n",
    "                weightvalues: np array of weights\n",
    "                sigmavalues: np array of sigmas\n",
    "                weights: list of tf weight variable\n",
    "                new_weights: list of new tf weight variables which wil\n",
    "                sigmas: list of tf sigma variables\n",
    "                uncert_pruning: bool pruning by snr\n",
    "            \"\"\"\n",
    "            if uncert_pruning:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues) / sigmavalues)\n",
    "                \n",
    "            else:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues))\n",
    "            cutoff = sorted_STN[int(remove_pct * len(sorted_STN))]\n",
    "            \n",
    "            # Weights, biases and head weights\n",
    "            for v, s in zip(weights, sigmas):\n",
    "                if uncert_pruning:\n",
    "                    snr = tf.abs(v) / tf.exp(0.5*s)\n",
    "                    mask = tf.greater_equal(snr, cutoff)\n",
    "                else:\n",
    "                    mask = tf.greater_equal(tf.abs(v), cutoff)\n",
    "                self.sess.run(tf.assign(v, tf.multiply(v, tf.cast(mask, v.dtype))))\n",
    "                #self.sess.run(tf.assign(s, np.multiply(self.sess.run(s), mask)))  # also apply zero std to weight!!!\n",
    "                \n",
    "            accs = []\n",
    "            for _ in range(10):\n",
    "                accs.append(self.sess.run(self.acc, {self.x: X_test,\n",
    "                                                     self.y: Y_test,\n",
    "                                                     self.task_idx: task_id,\n",
    "                                                     self.temp: self.lambda_1,\n",
    "                                                     self.training: False}))\n",
    "            print(\"%.2f, %s\" % (np.sum(sorted_STN < cutoff) / len(sorted_STN), np.mean(accs)))\n",
    "            return np.mean(accs)\n",
    "\n",
    "        # get weights\n",
    "        weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n",
    "        \n",
    "        # Get weights from network\n",
    "        # TODO get head weights and biases\n",
    "        mus_w = []\n",
    "        mus_b = []\n",
    "        sigmas_w = []\n",
    "        sigmas_b = []\n",
    "        mus_h = [] # weights and biases\n",
    "        sigmas_h = [] # weights and biases\n",
    "        for v in weights:\n",
    "            if re.match(\"^([w])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_w.append(v)\n",
    "            elif re.match(\"^([w])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_w.append(v)\n",
    "            elif re.match(\"^([b])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_b.append(v)\n",
    "            elif re.match(\"^([b])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_b.append(v)\n",
    "            elif re.match(\"^([wb])(_mu_h_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_h.append(v)\n",
    "            elif re.match(\"^([wb])(_sigma_h_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_h.append(v)\n",
    "            else:\n",
    "                print(\"Un-matched: {}\".format(v.name))\n",
    "                \n",
    "        acc = self.sess.run(self.acc, {self.x: X_test,\n",
    "                                    self.y: Y_test,\n",
    "                                    self.task_idx: task_id,\n",
    "                                    self.temp: self.lambda_1,\n",
    "                                    self.training: False}) # z mask for each layer in a list, each Z \\in dout\n",
    "        print(\"test acc: {}\".format(acc))\n",
    "        # cache network weights of resetting the network\n",
    "        _mus_w = [self.sess.run(w) for w in mus_w]\n",
    "        _sigmas_w = [self.sess.run(w) for w in sigmas_w]\n",
    "        _mus_b = [self.sess.run(w) for w in mus_b]\n",
    "        _sigmas_b = [self.sess.run(w) for w in sigmas_b]\n",
    "        _mus_h = [self.sess.run(w) for w in mus_h]\n",
    "        _sigmas_h = [self.sess.run(w) for w in sigmas_h]\n",
    "        \n",
    "        weightvalues = np.hstack(np.array([self.sess.run(w).flatten() for w in mus_w + mus_b + mus_h]))\n",
    "        sigmavalues = np.hstack(np.array([self.sess.run(tf.exp(0.5*s)).flatten() for s in sigmas_w + sigmas_b + sigmas_h]))\n",
    "    \n",
    "        xs = np.append(0.05 * np.array(range(20)), np.array([0.98, 0.99, 0.999]))\n",
    "        ya_ibp = []\n",
    "        for pct in xs:\n",
    "            ya_ibp.append(pruning(pct, weightvalues, sigmavalues, mus_w + mus_b + mus_h,\n",
    "                              sigmas_w + sigmas_b + sigmas_h, uncert_pruning=False))\n",
    "\n",
    "        # reset etc.\n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        yb_ibp = []\n",
    "        for pct in xs:\n",
    "            yb_ibp.append(pruning(pct, weightvalues, sigmavalues, mus_w + mus_b + mus_h,\n",
    "                                  sigmas_w + sigmas_b + sigmas_h, uncert_pruning=True))\n",
    "            \n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        \n",
    "        #return xs, ya, yb, ya_ibp, yb_ibp\n",
    "        return xs, ya_ibp, yb_ibp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass CLF Weight Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistGenerator():\n",
    "    def __init__(self, max_iter=10):\n",
    "        with gzip.open('data/mnist.pkl.gz', 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "        self.X_train = np.vstack((train_set[0], valid_set[0]))\n",
    "        self.Y_train = np.hstack((train_set[1], valid_set[1]))\n",
    "        self.X_test = test_set[0]\n",
    "        self.Y_test = test_set[1]\n",
    "        self.max_iter = max_iter\n",
    "        self.cur_iter = 0\n",
    "\n",
    "    def get_dims(self):\n",
    "        # Get data input and output dimensions\n",
    "        return self.X_train.shape[1], 10\n",
    "\n",
    "    def task(self):\n",
    "        # Retrieve train data\n",
    "        x_train = deepcopy(self.X_train)\n",
    "        y_train = np.eye(10)[self.Y_train]\n",
    "\n",
    "        # Retrieve test data\n",
    "        x_test = deepcopy(self.X_test)\n",
    "        y_test = np.eye(10)[self.Y_test]\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:174: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:58: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:62: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:65: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch: 0001 cost= 0.422611864\n",
      "Epoch: 0006 cost= 0.074313991\n",
      "Epoch: 0011 cost= 0.034981721\n",
      "Epoch: 0016 cost= 0.016915208\n",
      "Epoch: 0021 cost= 0.008358707\n",
      "Epoch: 0026 cost= 0.003985137\n",
      "Epoch: 0031 cost= 0.004512234\n",
      "Epoch: 0036 cost= 0.000792724\n",
      "Epoch: 0041 cost= 0.000497444\n",
      "Epoch: 0046 cost= 0.002953243\n",
      "Epoch: 0051 cost= 0.000263361\n",
      "Epoch: 0056 cost= 0.000187214\n",
      "Epoch: 0061 cost= 0.000178619\n",
      "Epoch: 0066 cost= 0.000103740\n",
      "Epoch: 0071 cost= 0.007201072\n",
      "Epoch: 0076 cost= 0.000103962\n",
      "Epoch: 0081 cost= 0.000059043\n",
      "Epoch: 0086 cost= 0.000037740\n",
      "Epoch: 0091 cost= 0.000166265\n",
      "Epoch: 0096 cost= 0.000040990\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:643: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:597: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "(1, ?, 100)\n",
      "<unknown>\n",
      "WARNING:tensorflow:From /Users/samuelKessler/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:623: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:604: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:738: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "_Z: (1, ?, 100)\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:755: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:762: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/samuelKessler/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py:1034: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch: 0001 train cost= 4.902848297\n",
      "Epoch: 0006 train cost= 4.092381529\n",
      "Epoch: 0011 train cost= 3.792935074\n",
      "Epoch: 0016 train cost= 3.614384398\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5a8e10f482df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#mf_model.restore(os.path.join(\"logs\", \"graph_{}_task{}\".format('ibp_wp_mnist', 0)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m mf_model.train(x_train, y_train, head, no_epochs, bsize,\n\u001b[0;32m---> 46\u001b[0;31m               anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mya_ibp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_ibp\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Oxford/IBP_BNN/ddm/alg/cla_models_multihead.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, task_idx, no_epochs, batch_size, display_epoch, anneal_rate, min_temp, verbose)\u001b[0m\n\u001b[1;32m   1052\u001b[0m                 _, c = sess.run(\n\u001b[1;32m   1053\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                     feed_dict={self.x: batch_x, self.y: batch_y, self.task_idx: task_idx, self.training: True, self.temp: temp})\n\u001b[0m\u001b[1;32m   1055\u001b[0m                 \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 600\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "data_gen = MnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "task_id=0\n",
    "    \n",
    "tf.reset_default_graph()  \n",
    "x_train, y_train, x_test, y_test = data_gen.task()\n",
    "x_testsets.append(x_test)\n",
    "y_testsets.append(y_test)\n",
    "\n",
    "# Set the readout head to train\n",
    "head = 0 if single_head else task_id\n",
    "bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "# Train network with maximum likelihood to initialize first model\n",
    "if task_id == 0:\n",
    "    ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "    ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "    mf_weights = ml_model.get_weights()\n",
    "    mf_variances = None\n",
    "    mf_betas = None\n",
    "    ml_model.close_session()\n",
    "\n",
    "# Train on non-coreset data\n",
    "mf_model = MFVI_IBP_NN_prune(in_dim, hidden_size, out_dim, x_train.shape[0], \n",
    "                             num_ibp_samples=10, prev_means=mf_weights, \n",
    "                       prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=5.0, beta0=1.0,\n",
    "                       learning_rate=0.00005, lambda_1=1.0, lambda_2=1.0, no_pred_samples=100,\n",
    "                       name='ibp_wp_mnist_new')\n",
    "\n",
    "#mf_model.restore(os.path.join(\"logs\", \"graph_{}_task{}\".format('ibp_wp_mnist', 0)))\n",
    "mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "              anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "\n",
    "xs, ya_ibp, yb_ibp  = mf_model.prune_weights(x_test, y_test, head)\n",
    "\n",
    "mf_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(xs, ya, label='$|\\mu|$')\n",
    "#plt.plot(xs, yb, label='snr')\n",
    "plt.plot(xs, ya_ibp, label='ibp $|\\mu|$')\n",
    "plt.plot(xs, yb_ibp, label='ibp snr')\n",
    "plt.xlabel('cut-off')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No IBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFVI_NN_prune(MFVI_NN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, training_size, \n",
    "        no_train_samples=10, no_pred_samples=100, prev_means=None, prev_log_variances=None, learning_rate=0.001, \n",
    "        prior_mean=0, prior_var=1):\n",
    "\n",
    "        super(MFVI_NN_prune, self).__init__(input_size, hidden_size, output_size, training_size, \n",
    "        no_train_samples, no_pred_samples, prev_means, prev_log_variances, learning_rate, \n",
    "        prior_mean, prior_var)\n",
    "\n",
    "\n",
    "    def prune_weights(self, X_test, Y_test, task_id):\n",
    "        \"\"\" Performs weight pruning.\n",
    "        \n",
    "        Z is at a data level doesn't make sense to introduce this intot he mask over weights which get zeroed \n",
    "        out. Simlpy running the accuracy over the graph will entail that Z is incorporated into the \n",
    "        matrix math for the prediction calculations.\n",
    "        \n",
    "        Args:\n",
    "            X_test: numpy array\n",
    "            Y_test: numpy array\n",
    "            task_id: int\n",
    "        :return: cutoffs, accs via naive pruning, accs via snr pruning,\n",
    "        weight values, sigma values of network\n",
    "        \"\"\"\n",
    "\n",
    "        def reset_weights(pr_mus, pr_sigmas, _mus, _sigmas):\n",
    "            \"\"\" Reset weights of graph to original values\n",
    "            Args:\n",
    "                pr_mus: list of tf variables which have been pruned\n",
    "                pr_sigmas: list of tf variables which have been pruned\n",
    "                _mus: list of cached mus in numpy\n",
    "                _sigmas: list of cached sigmas in numpy\n",
    "            \"\"\"\n",
    "\n",
    "            for v, _v in zip(pr_mus, _mus):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "            for v, _v in zip(pr_sigmas, _sigmas):\n",
    "                self.sess.run(tf.assign(v, tf.cast(_v, v.dtype)))\n",
    "\n",
    "        def pruning(remove_pct, weightvalues, sigmavalues,\n",
    "                    weights, sigmas, uncert_pruning=True):\n",
    "            \"\"\" Performs weight pruning experiment\n",
    "            Args:\n",
    "                weightvalues: np array of weights\n",
    "                sigmavalues: np array of sigmas\n",
    "                weights: list of tf weight variable\n",
    "                sigmas: list of tf sigma variables\n",
    "                uncert_pruning: bool pruning by snr\n",
    "            \"\"\"\n",
    "            if uncert_pruning:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues) / sigmavalues)\n",
    "                \n",
    "            else:\n",
    "                sorted_STN = np.sort(np.abs(weightvalues))\n",
    "            cutoff = sorted_STN[int(remove_pct * len(sorted_STN))]\n",
    "            \n",
    "            # Weights, biases and head weights\n",
    "            for v, s in zip(weights, sigmas):\n",
    "                if uncert_pruning:\n",
    "                    snr = tf.abs(v) / tf.exp(0.5*s)\n",
    "                    mask = tf.greater_equal(snr, cutoff)\n",
    "                else:\n",
    "                    mask = tf.greater_equal(tf.abs(v), cutoff)\n",
    "                self.sess.run(tf.assign(v, tf.multiply(v, tf.cast(mask, v.dtype))))\n",
    "                \n",
    "            accs = []\n",
    "            for _ in range(10):\n",
    "                accs.append(self.sess.run(self.acc, {self.x: X_test,\n",
    "                                                     self.y: Y_test,\n",
    "                                                     self.task_idx: task_id}))\n",
    "            print(\"%.2f, %s\" % (np.sum(sorted_STN < cutoff) / len(sorted_STN), np.mean(accs)))\n",
    "            return np.mean(accs)\n",
    "\n",
    "        # get weights\n",
    "        weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=tf.get_variable_scope().name)\n",
    "        \n",
    "        # Get weights from network\n",
    "        mus_w = []\n",
    "        mus_b = []\n",
    "        sigmas_w = []\n",
    "        sigmas_b = []\n",
    "        mus_h = [] # weights and biases\n",
    "        sigmas_h = [] # weights and biases\n",
    "        for v in weights:\n",
    "            if re.match(\"^([w])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_w.append(v)\n",
    "            elif re.match(\"^([w])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_w.append(v)\n",
    "            elif re.match(\"^([b])(_mu_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_b.append(v)\n",
    "            elif re.match(\"^([b])(_sigma_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_b.append(v)\n",
    "            elif re.match(\"^([wb])(_mu_h_)([0-9]+)(:0)$\", v.name):\n",
    "                mus_h.append(v)\n",
    "            elif re.match(\"^([wb])(_sigma_h_)([0-9]+)(:0)$\", v.name):\n",
    "                sigmas_h.append(v)\n",
    "            else:\n",
    "                print(\"Un-matched: {}\".format(v.name))\n",
    "                \n",
    "        # cache network weights of resetting the network\n",
    "        _mus_w = [self.sess.run(w) for w in mus_w]\n",
    "        _sigmas_w = [self.sess.run(w) for w in sigmas_w]\n",
    "        _mus_b = [self.sess.run(w) for w in mus_b]\n",
    "        _sigmas_b = [self.sess.run(w) for w in sigmas_b]\n",
    "        _mus_h = [self.sess.run(w) for w in mus_h]\n",
    "        _sigmas_h = [self.sess.run(w) for w in sigmas_h]\n",
    "        \n",
    "        weightvalues = np.hstack(np.array([self.sess.run(w).flatten() for w in mus_w + mus_b + mus_h]))\n",
    "        sigmavalues = np.hstack(np.array([self.sess.run(tf.exp(0.5*s)).flatten() for s in sigmas_w + sigmas_b + sigmas_h]))\n",
    "    \n",
    "        xs = np.append(0.05 * np.array(range(20)), np.array([0.98, 0.99, 0.999]))\n",
    "        # pruning\n",
    "        ya = []\n",
    "        for pct in xs:\n",
    "            ya_ibp.append(pruning(pct, weightvalues, sigmavalues, mus_w + mus_b + mus_h,\n",
    "                              sigmas_w + sigmas_b + sigmas_h, uncert_pruning=False))\n",
    "\n",
    "        # reset etc.\n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        yb = []\n",
    "        for pct in xs:\n",
    "            yb_ibp.append(pruning(pct, weightvalues, sigmavalues, mus_w + mus_b + mus_h,\n",
    "                                  sigmas_w + sigmas_b + sigmas_h, uncert_pruning=True))\n",
    "            \n",
    "        reset_weights(mus_w, sigmas_w, _mus_w, _sigmas_w)\n",
    "        reset_weights(mus_b, sigmas_b, _mus_b, _sigmas_b)\n",
    "        reset_weights(mus_h, sigmas_h, _mus_h, _sigmas_h)\n",
    "        \n",
    "        return xs, ya, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 600\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "coreset_size = 0\n",
    "data_gen = MnistGenerator()\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "task_id=0\n",
    "    \n",
    "tf.reset_default_graph()  \n",
    "x_train, y_train, x_test, y_test = data_gen.task()\n",
    "x_testsets.append(x_test)\n",
    "y_testsets.append(y_test)\n",
    "\n",
    "# Set the readout head to train\n",
    "head = 0 if single_head else task_id\n",
    "bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "\n",
    "# Train network with maximum likelihood to initialize first model\n",
    "if task_id == 0:\n",
    "    ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "    ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "    mf_weights = ml_model.get_weights()\n",
    "    mf_variances = None\n",
    "    ml_model.close_session()\n",
    "\n",
    "# Train on non-coreset data\n",
    "mf_model = MFVI_NN_prune(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                         prev_log_variances=mf_variances)\n",
    "\n",
    "mf_model.train(x_train, y_train, head, no_epochs, bsize)\n",
    "\n",
    "xs, ya, yb  = mf_model.prune_weights(x_test, y_test, head)\n",
    "\n",
    "mf_model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, ya, label='$|\\mu|$')\n",
    "plt.plot(xs, yb, label='snr')\n",
    "plt.plot(xs, ya_ibp, label='ibp $|\\mu|$')\n",
    "plt.plot(xs, yb_ibp, label='ibp snr')\n",
    "plt.xlabel('cut-off')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.savefig('plots/weight_pruning.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/weight_pruning.pkl', 'wb') as input_file:\n",
    "    pickle.dump({'ya_nnvi': ya,\n",
    "                 'yb_nnvi': yb,\n",
    "                 'ya_ibp': ya_ibp,\n",
    "                 'yb_ibp': yb_ibp}, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/weight_pruning.pkl', 'rb') as input_file:\n",
    "    d = pickle.load(input_file)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [100]\n",
    "batch_size = 128\n",
    "no_epochs = 200\n",
    "alpha0 = 1.0\n",
    "tau0=1.0 # initial temperature\n",
    "ANNEAL_RATE=0.000\n",
    "MIN_TEMP=0.1\n",
    "\n",
    "tf.set_random_seed(12)\n",
    "np.random.seed(1)\n",
    "\n",
    "ibp_acc = np.array([])\n",
    "\n",
    "coreset_size = 0\n",
    "val = False\n",
    "data_gen = SplitMnistGenerator(val, num_tasks=1)\n",
    "single_head=False\n",
    "in_dim, out_dim = data_gen.get_dims()\n",
    "x_testsets, y_testsets = [], []\n",
    "x_valsets, y_valsets = [], []\n",
    "for task_id in range(data_gen.max_iter):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    if val:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val = data_gen.next_task()\n",
    "        x_valsets.append(x_val)\n",
    "        y_valsets.append(y_val)\n",
    "    else:    \n",
    "        x_train, y_train, x_test, y_test = data_gen.next_task()\n",
    "    x_testsets.append(x_test)\n",
    "    y_testsets.append(y_test)\n",
    "\n",
    "    # Set the readout head to train\n",
    "    head = 0 if single_head else task_id\n",
    "    bsize = x_train.shape[0] if (batch_size is None) else batch_size\n",
    "    \n",
    "    # Train network with maximum likelihood to initialize first model\n",
    "    if task_id == 0:\n",
    "        ml_model = Vanilla_NN(in_dim, hidden_size, out_dim, x_train.shape[0])\n",
    "        ml_model.train(x_train, y_train, task_id, 100, bsize)\n",
    "        mf_weights = ml_model.get_weights()\n",
    "        mf_variances = None\n",
    "        mf_betas = None\n",
    "        ml_model.close_session()\n",
    "\n",
    "    # Train on non-coreset data\n",
    "    mf_model = MFVI_IBP_NN_prune(in_dim, hidden_size, out_dim, x_train.shape[0], prev_means=mf_weights, \n",
    "                           prev_log_variances=mf_variances, prev_betas=mf_betas,alpha0=alpha0,\n",
    "                           learning_rate=0.01, lambda_1=tau0, lambda_2=1.0, no_pred_samples=100)\n",
    "    mf_model.train(x_train, y_train, head, no_epochs, bsize,\n",
    "                   anneal_rate=ANNEAL_RATE, min_temp=MIN_TEMP)\n",
    "    \n",
    "    xs, ya, yb, ya_ibp, yb_ibp  = mf_model.prune_weights(x_test, y_test, head)\n",
    "    \n",
    "    mf_model.close_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
