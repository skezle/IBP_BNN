{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from local_logger import Logger\n",
    "from torch.nn import Parameter\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVDO(nn.Module):\n",
    "    def __init__(self, in_features, out_features, threshold, bias=True):\n",
    "        super(LinearSVDO, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.W = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.log_sigma = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = Parameter(torch.Tensor(1, out_features))\n",
    "        self.bias_log_sigma = Parameter(torch.Tensor(1, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.bias.data.zero_()\n",
    "        self.W.data.normal_(0, 0.01)\n",
    "        self.log_sigma.data.fill_(-6)\n",
    "        self.bias_log_sigma.data.fill_(-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.log_alpha = self.log_sigma * 2.0 - 2.0 * torch.log(1e-16 + torch.abs(self.W))\n",
    "        self.log_alpha = torch.clamp(self.log_alpha, -10, 10) \n",
    "        \n",
    "        if self.training:\n",
    "            lrt_mean =  F.linear(x, self.W) + self.bias\n",
    "            lrt_std = torch.sqrt(F.linear(x * x, torch.exp(self.log_sigma * 2.0)) + \\\n",
    "                                 torch.exp(self.bias_log_sigma * 2.0) + 1e-8)\n",
    "            eps = lrt_std.data.new(lrt_std.size()).normal_()\n",
    "            return lrt_mean + lrt_std * eps\n",
    "    \n",
    "        return F.linear(x, self.W * (self.log_alpha < 3).float()) + self.bias\n",
    "    \n",
    "    def prune(self, cut_off, baseline):\n",
    "        if baseline:\n",
    "            mask = torch.gt(torch.abs(self.W.data), cut_off)\n",
    "        else:\n",
    "            snr = torch.div(torch.abs(self.W.data), torch.sqrt(torch.exp(self.log_sigma.data * 2.0) + 1e-8))\n",
    "            mask = torch.gt(snr, cut_off)\n",
    "        self.W.data = torch.mul(mask, self.W.data)\n",
    "        self.log_sigma.data = torch.mul(mask, self.log_sigma.data)\n",
    "    \n",
    "    def reset_weights(self, w, log_sigma, zeros=False, weights_only=False):\n",
    "        if zeros:\n",
    "            self.W.data = torch.zeros_like(w)\n",
    "        else:\n",
    "            self.W.data = w.clone()\n",
    "        if not weights_only:\n",
    "            self.log_sigma.data = log_sigma.clone()\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.W.clone()\n",
    "        \n",
    "    def kl_reg(self):\n",
    "        # Return KL here -- a scalar \n",
    "        k1, k2, k3 = torch.Tensor([0.63576]).cuda(), torch.Tensor([1.8732]).cuda(), torch.Tensor([1.48695]).cuda()\n",
    "        kl = k1 * torch.sigmoid(k2 + k3 * self.log_alpha) - 0.5 * torch.log1p(torch.exp(-self.log_alpha))\n",
    "        a = - torch.sum(kl)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormalNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 200)\n",
    "        self.fc2 = nn.Linear(200,  200)\n",
    "        self.fc3 = nn.Linear(200,  10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Define a simple 2 layer Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = LinearSVDO(28*28, 200, threshold)\n",
    "        self.fc2 = LinearSVDO(200,  200, threshold)\n",
    "        self.fc3 = LinearSVDO(200,  10, threshold)\n",
    "        self.threshold=threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "def get_mnist(batch_size):\n",
    "    #trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    trsnform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define New Loss Function -- SGVLB \n",
    "class SGVLB(nn.Module):\n",
    "    def __init__(self, net, train_size):\n",
    "        super(SGVLB, self).__init__()\n",
    "        self.train_size = train_size\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, input, target, kl_weight=1.0):\n",
    "        assert not target.requires_grad\n",
    "        kl = 0.0\n",
    "        for module in self.net.children():\n",
    "            if hasattr(module, 'kl_reg'):\n",
    "                kl = kl + module.kl_reg()\n",
    "        return F.cross_entropy(input, target) * self.train_size + kl_weight * kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning(threshold, ws, log_sigmas, test_loader, baseline = False, use_cuda=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    threshold_t = torch.from_numpy(np.array(threshold)).float()\n",
    "    \n",
    "    weight_values_cat = torch.cat([torch.flatten(w) for w in ws])\n",
    "    sqrt_sigma_values_cat = torch.cat([torch.sqrt(torch.flatten(torch.exp(s * 2.0) + 1e-8)) for s in log_sigmas])\n",
    "\n",
    "    if use_cuda:\n",
    "        threshold_t = threshold_t.cuda()\n",
    "\n",
    "    if baseline:\n",
    "        sorted, _ = torch.sort(torch.abs(weight_values_cat))\n",
    "    else:\n",
    "        sorted, _ = torch.sort(torch.abs(weight_values_cat)/sqrt_sigma_values_cat)\n",
    "    cut_off = torch.narrow(sorted,\n",
    "                          0,\n",
    "                          start=min(int(torch.floor(torch.mul(threshold_t, torch.numel(sorted))).item()),\n",
    "                                    torch.numel(sorted)-1),\n",
    "                          length=1)\n",
    "    print(\"cut-off: {}\".format(cut_off))\n",
    "\n",
    "    # prune by snr\n",
    "    for c in model.children():\n",
    "        c.prune(cut_off, baseline)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        data = data.view(-1, 28*28)\n",
    "        output = model(data)\n",
    "        test_loss += float(sgvlb(output, target, kl_weight))\n",
    "        pred = output.data.max(1)[1] \n",
    "        test_acc += np.sum(pred.cpu().numpy() == target.cpu().data.numpy())\n",
    "        \n",
    "    logger.add_scalar(-1, 'te_loss', test_loss / len(test_loader.dataset))\n",
    "    logger.add_scalar(-1, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n",
    "    return test_acc / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/samuelk/anaconda2/envs/cl_baselines/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch       lr    tr_los    tr_acc    te_loss    te_acc    time\n",
      "-------  -------  --------  --------  ---------  --------  ------\n",
      "      1  1.0e-03   1.9e+02      90.6    8.2e+01      95.9     5.1\n",
      "      2  1.0e-03   7.1e+01      96.5    6.4e+01      96.7     4.6\n",
      "      3  1.0e-03   4.7e+01      97.6    4.5e+01      97.6     4.7\n",
      "      4  1.0e-03   3.4e+01      98.3    4.5e+01      97.8     4.7\n",
      "      5  1.0e-03   2.7e+01      98.6    4.5e+01      97.8     4.6\n",
      "      6  1.0e-03   2.0e+01      98.9    5.0e+01      97.6     4.7\n",
      "      7  1.0e-03   1.5e+01      99.2    4.5e+01      97.9     5.4\n",
      "      8  1.0e-03   1.4e+01      99.3    4.6e+01      97.9     5.5\n",
      "      9  1.0e-03   9.5e+00      99.5    5.6e+01      97.7     5.1\n",
      "     10  1.0e-03   1.1e+01      99.4    4.9e+01      98.0     5.4\n",
      "  epoch    kl       lr    tr_los    tr_acc    te_loss    te_acc    sp_0    sp_1    sp_2    time\n",
      "-------  ----  -------  --------  --------  ---------  --------  ------  ------  ------  ------\n",
      "      1     1  1.0e-03   1.1e+02      98.6   -6.9e+02      97.8   0.883   0.814     0.3    10.4\n",
      "      2     1  1.0e-03  -8.2e+02      98.3   -8.9e+02      97.8   0.887   0.833     0.4     9.2\n",
      "      3     1  1.0e-03  -9.5e+02      98.2   -9.9e+02      97.9   0.905   0.863     0.4    15.6\n",
      "      4     1  1.0e-03  -1.0e+03      98.2   -1.0e+03      97.8   0.900   0.863     0.5    14.8\n",
      "      5     1  1.0e-03  -1.1e+03      98.0   -1.1e+03      97.8   0.908   0.883     0.5    14.2\n",
      "      6     1  1.0e-03  -1.1e+03      97.8   -1.1e+03      98.0   0.906   0.894     0.5    16.2\n",
      "      7     1  1.0e-03  -1.1e+03      97.7   -1.1e+03      97.6   0.897   0.874     0.5    15.2\n",
      "      8     1  1.0e-03  -1.1e+03      97.7   -1.1e+03      97.7   0.901   0.879     0.5    15.9\n",
      "      9     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.7   0.898   0.872     0.5    15.4\n",
      "     10     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      98.0   0.913   0.903     0.5    13.9\n",
      "     11     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      97.7   0.908   0.899     0.5    10.6\n",
      "     12     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.902   0.897     0.5    10.5\n",
      "     13     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.8   0.905   0.904     0.6    11.1\n",
      "     14     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.914   0.908     0.6    10.2\n",
      "     15     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.906   0.898     0.6    10.7\n",
      "     16     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.917   0.909     0.6    10.5\n",
      "     17     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      97.9   0.908   0.908     0.6    10.4\n",
      "     18     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.912   0.908     0.6    12.0\n",
      "     19     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      98.0   0.915   0.913     0.6    13.1\n",
      "     20     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      98.1   0.915   0.911     0.6    14.5\n",
      "     21     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      98.0   0.915   0.914     0.6    15.2\n",
      "     22     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      98.0   0.921   0.918     0.6    15.5\n",
      "     23     1  1.0e-03  -1.1e+03      97.7   -1.2e+03      98.1   0.918   0.919     0.6    16.1\n",
      "     24     1  1.0e-03  -1.1e+03      97.7   -1.2e+03      98.0   0.916   0.917     0.6    15.8\n",
      "     25     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      98.0   0.920   0.923     0.6    16.2\n",
      "     26     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      97.9   0.914   0.915     0.6    16.8\n",
      "     27     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.921   0.925     0.6    16.7\n",
      "     28     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      97.9   0.918   0.920     0.6    16.1\n",
      "     29     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.915   0.922     0.6    15.0\n",
      "     30     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.921   0.929     0.7    17.1\n",
      "     31     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.923   0.926     0.7    16.1\n",
      "     32     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.919   0.928     0.7    12.1\n",
      "     33     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.920   0.928     0.7    11.8\n",
      "     34     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.8   0.924   0.935     0.7    11.4\n",
      "     35     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.923   0.927     0.7    11.1\n",
      "     36     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.929   0.936     0.7    11.3\n",
      "     37     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.923   0.931     0.7    10.9\n",
      "     38     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.923   0.931     0.7    11.4\n",
      "     39     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.923   0.931     0.7    11.2\n",
      "     40     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.926   0.934     0.7    11.3\n",
      "     41     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.928   0.936     0.7    10.5\n",
      "     42     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.924   0.933     0.7    11.7\n",
      "     43     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.929   0.940     0.7    10.5\n",
      "     44     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.928   0.934     0.7    10.9\n",
      "     45     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.1   0.926   0.937     0.7    10.7\n",
      "     46     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.932   0.940     0.7    11.9\n",
      "     47     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.0   0.931   0.939     0.7    10.9\n",
      "     48     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.930   0.939     0.7    11.3\n",
      "     49     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      97.9   0.925   0.938     0.7    10.6\n",
      "     50     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.955   0.962     0.8    11.7\n",
      "     51     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.961   0.967     0.8    11.6\n",
      "     52     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.1   0.963   0.968     0.8    11.7\n",
      "     53     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.963   0.968     0.8    10.8\n",
      "     54     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.1   0.964   0.969     0.8    11.0\n",
      "     55     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.965   0.969     0.8    11.9\n",
      "     56     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.965   0.969     0.8    11.6\n",
      "     57     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.964   0.969     0.8    11.5\n",
      "     58     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.965   0.970     0.8    11.8\n",
      "     59     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.1   0.965   0.970     0.8    12.0\n",
      "     60     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.2   0.967   0.972     0.8    11.8\n",
      "     61     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.968   0.973     0.8    11.7\n",
      "     62     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.1   0.968   0.974     0.8    11.2\n",
      "     63     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.1   0.969   0.974     0.8    11.2\n",
      "     64     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.2   0.969   0.974     0.8    11.3\n",
      "     65     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.1   0.970   0.975     0.8    11.5\n",
      "     66     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.975     0.8     8.9\n",
      "     67     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.975     0.8    10.7\n",
      "     68     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.975     0.8    10.6\n",
      "     69     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.975     0.8    12.3\n",
      "     70     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.976     0.8    11.9\n",
      "     71     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.970   0.976     0.8    11.6\n",
      "     72     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.970   0.976     0.8    12.2\n",
      "     73     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.970   0.976     0.8    11.6\n",
      "     74     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.970   0.976     0.8    12.1\n",
      "     75     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.976     0.8    11.3\n",
      "     76     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    12.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     77     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    11.5\n",
      "     78     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    11.2\n",
      "     79     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    11.3\n",
      "     80     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    11.8\n",
      "     81     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    11.9\n",
      "     82     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    12.2\n",
      "     83     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    11.8\n",
      "     84     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    11.2\n",
      "     85     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    12.2\n",
      "     86     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    11.5\n",
      "     87     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    11.4\n",
      "     88     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    12.0\n",
      "     89     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    11.5\n",
      "     90     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    11.4\n",
      "     91     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    10.4\n",
      "     92     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    12.2\n",
      "     93     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    11.1\n",
      "     94     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    12.5\n",
      "     95     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    11.7\n",
      "     96     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    12.6\n",
      "     97     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    11.8\n",
      "     98     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    12.2\n",
      "     99     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    11.1\n",
      "    100     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.976     0.8    12.3\n",
      "kept weight ratio = 33.46238007069517\n",
      "cut-off: tensor([1.5759e-09], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6.1846e-06], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1783e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.0423e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([4.0850e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6.4780e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([8.9661e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0003], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0003], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0005], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0007], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0012], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0023], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0065], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0090], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0149], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0366], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2584], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3570], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.5634], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.7462], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1311], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.3396], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([7.2367e-08], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0007], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0015], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0022], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0029], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0035], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0041], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0046], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0051], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0056], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0059], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0062], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0064], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0066], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0067], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0067], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0099], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0269], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0609], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.1329], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.1652], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2223], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3769], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.3872], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.1569], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([3.9905], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([5.7997], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([294.5091], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6349.4702], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Run: 1\n",
      "  epoch       lr    tr_los    tr_acc    te_loss    te_acc    time\n",
      "-------  -------  --------  --------  ---------  --------  ------\n",
      "      1  1.0e-03   1.9e+02      91.2    9.0e+01      95.5     6.7\n",
      "      2  1.0e-03   7.1e+01      96.4    5.5e+01      97.1     6.9\n",
      "      3  1.0e-03   4.6e+01      97.6    5.1e+01      97.2     6.4\n",
      "      4  1.0e-03   3.5e+01      98.2    4.3e+01      97.7     7.1\n",
      "      5  1.0e-03   2.7e+01      98.6    4.2e+01      97.8     6.3\n",
      "      6  1.0e-03   2.0e+01      98.9    4.0e+01      98.0     7.1\n",
      "      7  1.0e-03   1.7e+01      99.1    4.5e+01      97.9     5.9\n",
      "      8  1.0e-03   1.4e+01      99.2    4.0e+01      98.0     7.5\n",
      "      9  1.0e-03   1.2e+01      99.4    5.1e+01      97.8     6.6\n",
      "     10  1.0e-03   9.5e+00      99.5    4.5e+01      98.1     6.5\n",
      "  epoch    kl       lr    tr_los    tr_acc    te_loss    te_acc    sp_0    sp_1    sp_2    time\n",
      "-------  ----  -------  --------  --------  ---------  --------  ------  ------  ------  ------\n",
      "      1     1  1.0e-03   8.9e+01      98.4   -7.1e+02      97.9   0.886   0.796     0.3    11.3\n",
      "      2     1  1.0e-03  -8.3e+02      98.1   -9.1e+02      97.8   0.897   0.830     0.4     9.0\n",
      "      3     1  1.0e-03  -9.6e+02      98.1   -1.0e+03      97.9   0.894   0.847     0.4    12.3\n",
      "      4     1  1.0e-03  -1.0e+03      98.1   -1.0e+03      97.4   0.905   0.862     0.5    10.8\n",
      "      5     1  1.0e-03  -1.1e+03      97.9   -1.1e+03      97.7   0.908   0.875     0.5    12.0\n",
      "      6     1  1.0e-03  -1.1e+03      97.8   -1.1e+03      97.9   0.910   0.888     0.5    12.0\n",
      "      7     1  1.0e-03  -1.1e+03      97.7   -1.1e+03      97.9   0.915   0.905     0.5    11.6\n",
      "      8     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      98.0   0.910   0.893     0.5    12.8\n",
      "      9     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.920   0.912     0.6    11.5\n",
      "     10     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      97.9   0.922   0.913     0.6    12.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      98.1   0.919   0.901     0.6    12.3\n",
      "     12     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.8   0.919   0.908     0.6    11.9\n",
      "     13     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.8   0.921   0.924     0.6    12.7\n",
      "     14     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.924   0.915     0.6    12.1\n",
      "     15     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      98.0   0.923   0.918     0.6    12.0\n",
      "     16     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      97.9   0.919   0.917     0.6    12.4\n",
      "     17     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      97.9   0.923   0.918     0.6    12.9\n",
      "     18     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      97.8   0.925   0.922     0.6    12.0\n",
      "     19     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      98.0   0.920   0.920     0.6    11.5\n",
      "     20     1  1.0e-03  -1.1e+03      97.7   -1.2e+03      98.1   0.921   0.923     0.6    12.6\n",
      "     21     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      97.9   0.922   0.925     0.6    11.3\n",
      "     22     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      98.0   0.927   0.926     0.6    11.8\n",
      "     23     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      97.7   0.925   0.928     0.6    12.6\n",
      "     24     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      97.9   0.923   0.931     0.7    12.3\n",
      "     25     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.927   0.933     0.7    12.2\n",
      "     26     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      97.9   0.927   0.932     0.7    11.9\n",
      "     27     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.927   0.935     0.7    12.7\n",
      "     28     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.931   0.938     0.7    13.7\n",
      "     29     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.1   0.929   0.935     0.7    12.6\n",
      "     30     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.928   0.932     0.7    12.7\n",
      "     31     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.930   0.941     0.7    13.4\n",
      "     32     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.927   0.935     0.7    13.1\n",
      "     33     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.927   0.938     0.7    14.0\n",
      "     34     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.928   0.939     0.7    13.1\n",
      "     35     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.2   0.926   0.937     0.7    12.8\n",
      "     36     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.930   0.945     0.7    12.7\n",
      "     37     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.932   0.941     0.7    13.1\n",
      "     38     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.930   0.945     0.7    13.8\n",
      "     39     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.2   0.926   0.939     0.7    13.4\n",
      "     40     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.934   0.944     0.7    13.4\n",
      "     41     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.932   0.946     0.7    12.9\n",
      "     42     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.928   0.942     0.7    13.2\n",
      "     43     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.930   0.942     0.7    13.4\n",
      "     44     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      97.9   0.930   0.941     0.7    12.9\n",
      "     45     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      97.9   0.935   0.948     0.7    13.2\n",
      "     46     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.933   0.947     0.7    13.1\n",
      "     47     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.2   0.931   0.944     0.7    12.1\n",
      "     48     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.0   0.934   0.946     0.7    12.0\n",
      "     49     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.0   0.935   0.946     0.7    14.3\n",
      "     50     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.959   0.966     0.8    14.2\n",
      "     51     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.963   0.970     0.8    13.4\n",
      "     52     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.966   0.970     0.8    13.6\n",
      "     53     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.966   0.971     0.8    14.1\n",
      "     54     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.967   0.971     0.8    13.7\n",
      "     55     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.966   0.971     0.8    14.7\n",
      "     56     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.3   0.966   0.972     0.8    14.6\n",
      "     57     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.966   0.972     0.8    14.1\n",
      "     58     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.966   0.972     0.8    13.7\n",
      "     59     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.967   0.972     0.8    14.2\n",
      "     60     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.2   0.969   0.974     0.8    13.9\n",
      "     61     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.975     0.8    14.0\n",
      "     62     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.975     0.8    13.7\n",
      "     63     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.3   0.971   0.975     0.8    14.0\n",
      "     64     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.975     0.8    13.4\n",
      "     65     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    13.6\n",
      "     66     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    13.2\n",
      "     67     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.3   0.971   0.976     0.8    13.8\n",
      "     68     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.976     0.8    13.8\n",
      "     69     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.972   0.976     0.8    13.6\n",
      "     70     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.976     0.8    13.8\n",
      "     71     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.976     0.8    13.5\n",
      "     72     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.976     0.8    14.0\n",
      "     73     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.977     0.8    14.1\n",
      "     74     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.977     0.8    13.7\n",
      "     75     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.977     0.8    13.8\n",
      "     76     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.972   0.977     0.8    14.3\n",
      "     77     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.3   0.972   0.977     0.8    12.4\n",
      "     78     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.977     0.8    12.9\n",
      "     79     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.977     0.8    13.0\n",
      "     80     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.972   0.977     0.8    11.4\n",
      "     81     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.977     0.8    12.4\n",
      "     82     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.972   0.977     0.8    12.8\n",
      "     83     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.977     0.8    12.8\n",
      "     84     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.977     0.8    13.5\n",
      "     85     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.977     0.8    11.8\n",
      "     86     1  1.6e-06  -1.2e+03      98.6   -1.2e+03      98.3   0.972   0.977     0.8    12.9\n",
      "     87     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.977     0.8    13.4\n",
      "     88     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.977     0.8    13.0\n",
      "     89     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.972   0.977     0.8    12.8\n",
      "     90     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.972   0.977     0.8    10.2\n",
      "     91     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.972   0.977     0.8    12.5\n",
      "     92     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.972   0.977     0.8    11.8\n",
      "     93     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.977     0.8    12.5\n",
      "     94     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.977     0.8    12.4\n",
      "     95     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.3   0.972   0.977     0.8    13.4\n",
      "     96     1  1.6e-06  -1.2e+03      98.6   -1.2e+03      98.3   0.972   0.977     0.8    12.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     97     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.977     0.8    13.0\n",
      "     98     1  1.6e-06  -1.2e+03      98.6   -1.2e+03      98.3   0.972   0.977     0.8    13.0\n",
      "     99     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.972   0.977     0.8    13.1\n",
      "    100     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.972   0.977     0.8    12.8\n",
      "kept weight ratio = 35.37366548042704\n",
      "cut-off: tensor([1.3116e-10], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6.0637e-06], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1664e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.9751e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([3.8548e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6.0591e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([8.3297e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0003], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0004], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0006], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0010], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0021], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0059], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0083], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0135], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0313], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2255], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3389], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.5328], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.7280], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1405], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.4088], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([3.7475e-08], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0007], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0014], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0021], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0027], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0033], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0039], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0044], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0049], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0053], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0057], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0060], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0063], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0065], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0066], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0067], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0072], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0200], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0534], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.1259], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.1571], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2091], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3440], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.2484], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.0546], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([4.1449], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6.2620], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([636.4385], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([7257.3306], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Run: 2\n",
      "  epoch       lr    tr_los    tr_acc    te_loss    te_acc    time\n",
      "-------  -------  --------  --------  ---------  --------  ------\n",
      "      1  1.0e-03   1.9e+02      90.8    9.4e+01      95.2     7.3\n",
      "      2  1.0e-03   7.4e+01      96.3    6.3e+01      96.8     8.1\n",
      "      3  1.0e-03   4.9e+01      97.5    4.8e+01      97.5     7.3\n",
      "      4  1.0e-03   3.6e+01      98.1    4.5e+01      97.6     7.9\n",
      "      5  1.0e-03   2.6e+01      98.6    4.3e+01      97.9     6.6\n",
      "      6  1.0e-03   2.1e+01      98.9    4.3e+01      97.9     7.7\n",
      "      7  1.0e-03   1.8e+01      99.0    6.1e+01      97.3     7.5\n",
      "      8  1.0e-03   1.3e+01      99.3    4.5e+01      97.9     7.9\n",
      "      9  1.0e-03   1.1e+01      99.4    5.3e+01      97.6     7.3\n",
      "     10  1.0e-03   1.0e+01      99.4    5.4e+01      97.6     8.7\n",
      "  epoch    kl       lr    tr_los    tr_acc    te_loss    te_acc    sp_0    sp_1    sp_2    time\n",
      "-------  ----  -------  --------  --------  ---------  --------  ------  ------  ------  ------\n",
      "      1     1  1.0e-03   1.6e+02      98.6   -6.7e+02      97.6   0.864   0.771     0.3    13.3\n",
      "      2     1  1.0e-03  -8.1e+02      98.3   -8.9e+02      97.4   0.886   0.835     0.4    46.5\n",
      "      3     1  1.0e-03  -9.5e+02      98.1   -9.9e+02      97.7   0.914   0.880     0.4    48.3\n",
      "      4     1  1.0e-03  -1.0e+03      98.0   -1.0e+03      97.5   0.899   0.848     0.5    77.2\n",
      "      5     1  1.0e-03  -1.1e+03      98.0   -1.1e+03      97.8   0.910   0.868     0.5    53.0\n",
      "      6     1  1.0e-03  -1.1e+03      97.8   -1.1e+03      97.7   0.908   0.892     0.5    43.2\n",
      "      7     1  1.0e-03  -1.1e+03      97.7   -1.1e+03      97.6   0.913   0.899     0.5    51.2\n",
      "      8     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      97.9   0.916   0.894     0.5    50.7\n",
      "      9     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.8   0.909   0.892     0.5    43.6\n",
      "     10     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      98.0   0.910   0.899     0.5    52.6\n",
      "     11     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.918   0.912     0.6    46.3\n",
      "     12     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      97.4   0.915   0.895     0.5    52.1\n",
      "     13     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.8   0.915   0.908     0.6    89.2\n",
      "     14     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.6   0.917   0.917     0.6    96.9\n",
      "     15     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.6   0.909   0.908     0.6    93.8\n",
      "     16     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.915   0.919     0.6    89.1\n",
      "     17     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      98.0   0.915   0.916     0.6    84.2\n",
      "     18     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      97.6   0.919   0.921     0.6    77.1\n",
      "     19     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      97.9   0.924   0.925     0.6   103.9\n",
      "     20     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      98.0   0.915   0.914     0.6    19.8\n",
      "     21     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      97.9   0.919   0.925     0.6    20.9\n",
      "     22     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.918   0.923     0.6    18.9\n",
      "     23     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      97.8   0.922   0.922     0.6    19.7\n",
      "     24     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      98.0   0.922   0.920     0.6    22.3\n",
      "     25     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      97.8   0.927   0.930     0.6    21.8\n",
      "     26     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      97.7   0.925   0.931     0.6    21.6\n",
      "     27     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      97.9   0.928   0.931     0.7    21.8\n",
      "     28     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.7   0.924   0.929     0.6    14.6\n",
      "     29     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.923   0.928     0.6    12.9\n",
      "     30     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      97.9   0.926   0.935     0.7    12.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     31     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.926   0.930     0.6    13.7\n",
      "     32     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.8   0.923   0.930     0.7    22.0\n",
      "     33     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.1   0.927   0.934     0.7    22.9\n",
      "     34     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.927   0.933     0.7    21.1\n",
      "     35     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.928   0.933     0.7    22.9\n",
      "     36     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.8   0.927   0.934     0.7    21.1\n",
      "     37     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.925   0.934     0.7    21.8\n",
      "     38     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.8   0.927   0.936     0.7    22.5\n",
      "     39     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.8   0.926   0.936     0.7    22.3\n",
      "     40     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.929   0.938     0.7    22.6\n",
      "     41     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.929   0.938     0.7    22.1\n",
      "     42     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.931   0.940     0.7    23.7\n",
      "     43     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      97.8   0.929   0.940     0.7    22.5\n",
      "     44     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      97.9   0.931   0.943     0.7    17.3\n",
      "     45     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.1   0.931   0.939     0.7    20.1\n",
      "     46     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.931   0.940     0.7    18.5\n",
      "     47     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.0   0.928   0.941     0.7    19.2\n",
      "     48     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.930   0.941     0.7    20.7\n",
      "     49     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.1   0.930   0.943     0.7    20.4\n",
      "     50     1  2.0e-04  -1.2e+03      98.2   -1.2e+03      98.2   0.956   0.964     0.8    17.0\n",
      "     51     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.961   0.968     0.8    16.5\n",
      "     52     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.962   0.970     0.8    16.3\n",
      "     53     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.964   0.970     0.8    15.9\n",
      "     54     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.3   0.964   0.971     0.8    16.7\n",
      "     55     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.964   0.971     0.8    16.5\n",
      "     56     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.3   0.964   0.971     0.8    16.8\n",
      "     57     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.965   0.971     0.8    21.2\n",
      "     58     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.965   0.972     0.8    20.1\n",
      "     59     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.965   0.971     0.8    20.0\n",
      "     60     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.968   0.973     0.8    15.6\n",
      "     61     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.969   0.974     0.8    14.3\n",
      "     62     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.969   0.975     0.8    20.1\n",
      "     63     1  4.0e-05  -1.2e+03      98.3   -1.2e+03      98.2   0.969   0.975     0.8    23.3\n",
      "     64     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.969   0.975     0.8    22.4\n",
      "     65     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.969   0.975     0.8    22.2\n",
      "     66     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.976     0.8    22.2\n",
      "     67     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.976     0.8    22.8\n",
      "     68     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.976     0.8    23.1\n",
      "     69     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.2   0.970   0.976     0.8    24.1\n",
      "     70     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    22.3\n",
      "     71     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    18.1\n",
      "     72     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.3   0.971   0.976     0.8    22.0\n",
      "     73     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    22.7\n",
      "     74     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    24.2\n",
      "     75     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    22.0\n",
      "     76     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    22.4\n",
      "     77     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    24.3\n",
      "     78     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    22.2\n",
      "     79     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    23.3\n",
      "     80     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    21.8\n",
      "     81     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    18.5\n",
      "     82     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    16.8\n",
      "     83     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    17.0\n",
      "     84     1  1.6e-06  -1.2e+03      98.3   -1.2e+03      98.2   0.971   0.976     0.8    16.1\n",
      "     85     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    17.0\n",
      "     86     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    16.6\n",
      "     87     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    16.6\n",
      "     88     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    16.6\n",
      "     89     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    17.2\n",
      "     90     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    15.7\n",
      "     91     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    16.6\n",
      "     92     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    16.7\n",
      "     93     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    19.6\n",
      "     94     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    19.5\n",
      "     95     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    20.1\n",
      "     96     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    21.0\n",
      "     97     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    21.2\n",
      "     98     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    22.0\n",
      "     99     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.971   0.976     0.8    22.9\n",
      "    100     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.2   0.971   0.976     0.8    22.2\n",
      "kept weight ratio = 33.82102756039469\n",
      "cut-off: tensor([4.5724e-11], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([5.8453e-06], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1370e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.8817e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([3.6943e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([5.9326e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([8.2837e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0003], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0004], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0006], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0010], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0022], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0060], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0084], device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut-off: tensor([0.0139], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0343], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2405], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3466], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.5316], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.7339], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1440], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.8196], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([3.6763e-09], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0007], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0014], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0021], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0027], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0033], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0039], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0044], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0049], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0054], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0058], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0061], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0063], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0065], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0066], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0067], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0076], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0226], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0561], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.1303], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.1635], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2195], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3723], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.3021], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.9900], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([3.9442], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([5.7910], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([28.3186], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6459.7065], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Run: 3\n",
      "  epoch       lr    tr_los    tr_acc    te_loss    te_acc    time\n",
      "-------  -------  --------  --------  ---------  --------  ------\n",
      "      1  1.0e-03   1.9e+02      91.0    9.3e+01      95.3    15.4\n",
      "      2  1.0e-03   7.2e+01      96.5    7.0e+01      96.6    15.3\n",
      "      3  1.0e-03   4.7e+01      97.6    5.2e+01      97.0    16.3\n",
      "      4  1.0e-03   3.6e+01      98.2    6.0e+01      97.0    14.9\n",
      "      5  1.0e-03   2.7e+01      98.6    4.3e+01      97.7    14.7\n",
      "      6  1.0e-03   2.0e+01      98.9    4.4e+01      97.8    16.2\n",
      "      7  1.0e-03   1.8e+01      99.0    4.4e+01      97.8     8.8\n",
      "      8  1.0e-03   1.3e+01      99.3    4.6e+01      97.9     8.5\n",
      "      9  1.0e-03   1.3e+01      99.3    4.7e+01      98.0     8.5\n",
      "     10  1.0e-03   8.5e+00      99.6    5.4e+01      97.6     8.5\n",
      "  epoch    kl       lr    tr_los    tr_acc    te_loss    te_acc    sp_0    sp_1    sp_2    time\n",
      "-------  ----  -------  --------  --------  ---------  --------  ------  ------  ------  ------\n",
      "      1     1  1.0e-03   1.2e+02      98.5   -6.9e+02      97.7   0.881   0.814     0.3    11.3\n",
      "      2     1  1.0e-03  -8.2e+02      98.2   -9.0e+02      97.9   0.891   0.825     0.4    11.6\n",
      "      3     1  1.0e-03  -9.6e+02      98.2   -9.9e+02      97.8   0.891   0.845     0.4    11.6\n",
      "      4     1  1.0e-03  -1.0e+03      98.2   -1.0e+03      97.7   0.908   0.859     0.5    11.4\n",
      "      5     1  1.0e-03  -1.1e+03      98.0   -1.1e+03      97.6   0.893   0.845     0.5    11.6\n",
      "      6     1  1.0e-03  -1.1e+03      97.9   -1.1e+03      97.9   0.907   0.878     0.5    11.4\n",
      "      7     1  1.0e-03  -1.1e+03      97.7   -1.1e+03      97.8   0.904   0.848     0.5    11.2\n",
      "      8     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      97.9   0.903   0.871     0.5    11.4\n",
      "      9     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      97.7   0.906   0.878     0.5    11.2\n",
      "     10     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      97.9   0.909   0.892     0.5    11.3\n",
      "     11     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      97.7   0.911   0.896     0.6    11.4\n",
      "     12     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      97.6   0.909   0.900     0.6    11.5\n",
      "     13     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.919   0.915     0.6    11.7\n",
      "     14     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      98.0   0.915   0.906     0.6    11.6\n",
      "     15     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      98.0   0.912   0.909     0.6    11.2\n",
      "     16     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      98.2   0.916   0.915     0.6    11.5\n",
      "     17     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.8   0.918   0.910     0.6    11.9\n",
      "     18     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      97.9   0.914   0.909     0.6    11.4\n",
      "     19     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      97.7   0.920   0.917     0.6    10.9\n",
      "     20     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      97.8   0.916   0.916     0.6    11.2\n",
      "     21     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      97.9   0.916   0.911     0.6    10.2\n",
      "     22     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      97.9   0.919   0.917     0.6    10.2\n",
      "     23     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      97.8   0.919   0.919     0.6     9.3\n",
      "     24     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.921   0.923     0.6     9.5\n",
      "     25     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.921   0.922     0.7     9.5\n",
      "     26     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      98.1   0.922   0.921     0.6     9.4\n",
      "     27     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.921   0.927     0.6     9.7\n",
      "     28     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.8   0.924   0.925     0.7     9.6\n",
      "     29     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.920   0.925     0.7     9.8\n",
      "     30     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.924   0.932     0.7     9.7\n",
      "     31     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.925   0.930     0.7     9.9\n",
      "     32     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.924   0.926     0.7     9.6\n",
      "     33     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.1   0.922   0.925     0.7    10.2\n",
      "     34     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.922   0.930     0.7    11.4\n",
      "     35     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      97.9   0.926   0.933     0.7    13.9\n",
      "     36     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.928   0.937     0.7    12.0\n",
      "     37     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.924   0.933     0.7    10.6\n",
      "     38     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.927   0.936     0.7    13.5\n",
      "     39     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.926   0.934     0.7    14.3\n",
      "     40     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.923   0.932     0.7    14.3\n",
      "     41     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.930   0.936     0.7    15.3\n",
      "     42     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.924   0.933     0.7    14.5\n",
      "     43     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.2   0.928   0.938     0.7    15.3\n",
      "     44     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.925   0.936     0.7    14.9\n",
      "     45     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      97.9   0.927   0.937     0.7    15.2\n",
      "     46     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.2   0.925   0.937     0.7    18.8\n",
      "     47     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.0   0.926   0.939     0.7    17.1\n",
      "     48     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.926   0.937     0.7    22.8\n",
      "     49     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.2   0.927   0.940     0.7    23.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     50     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.3   0.955   0.961     0.8    23.3\n",
      "     51     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.3   0.961   0.965     0.8    17.2\n",
      "     52     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.4   0.963   0.968     0.8    18.3\n",
      "     53     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.963   0.967     0.8    20.5\n",
      "     54     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.964   0.968     0.8    16.8\n",
      "     55     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.2   0.964   0.969     0.8    23.1\n",
      "     56     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.3   0.964   0.968     0.8    21.3\n",
      "     57     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.3   0.965   0.970     0.8    23.5\n",
      "     58     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.3   0.964   0.969     0.8    23.2\n",
      "     59     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.3   0.964   0.968     0.8    24.4\n",
      "     60     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.3   0.966   0.970     0.8    23.4\n",
      "     61     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.3   0.967   0.971     0.8    21.8\n",
      "     62     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.4   0.968   0.972     0.8    21.9\n",
      "     63     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.3   0.968   0.973     0.8    23.4\n",
      "     64     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.3   0.969   0.973     0.8    22.5\n",
      "     65     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.3   0.969   0.973     0.8    24.6\n",
      "     66     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.3   0.969   0.973     0.8    23.4\n",
      "     67     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.3   0.969   0.974     0.8    22.3\n",
      "     68     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.3   0.969   0.974     0.8    22.9\n",
      "     69     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.3   0.969   0.974     0.8    24.2\n",
      "     70     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.974     0.8    22.2\n",
      "     71     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.974     0.8    23.6\n",
      "     72     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.974     0.8    16.9\n",
      "     73     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.974     0.8    19.8\n",
      "     74     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.974     0.8    20.2\n",
      "     75     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.3   0.970   0.975     0.8    20.2\n",
      "     76     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.3   0.970   0.975     0.8    20.1\n",
      "     77     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    19.8\n",
      "     78     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.974     0.8    15.3\n",
      "     79     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    14.2\n",
      "     80     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    14.5\n",
      "     81     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    14.3\n",
      "     82     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.3   0.970   0.975     0.8    15.7\n",
      "     83     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.3   0.970   0.975     0.8    15.6\n",
      "     84     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    14.9\n",
      "     85     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    14.8\n",
      "     86     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    14.6\n",
      "     87     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    14.7\n",
      "     88     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.975     0.8    14.6\n",
      "     89     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.975     0.8    14.8\n",
      "     90     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    14.3\n",
      "     91     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.975     0.8    14.9\n",
      "     92     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.975     0.8    13.7\n",
      "     93     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    14.3\n",
      "     94     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.975     0.8    14.4\n",
      "     95     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.975     0.8    15.2\n",
      "     96     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.975     0.8    15.2\n",
      "     97     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.2   0.970   0.975     0.8    23.0\n",
      "     98     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    22.3\n",
      "     99     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.3   0.970   0.975     0.8    22.3\n",
      "    100     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.3   0.970   0.975     0.8    23.2\n",
      "kept weight ratio = 32.97395919721347\n",
      "cut-off: tensor([7.8254e-10], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6.1437e-06], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1772e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.0798e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([4.1040e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6.4166e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([8.7783e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0003], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0003], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0005], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0007], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0012], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0025], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0066], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0093], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0151], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0372], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2445], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3555], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.5433], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.7394], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1431], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.3984], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.1225e-08], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0007], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0015], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0022], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0029], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0035], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0040], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0046], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0051], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0055], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0059], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0062], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0064], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0065], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0066], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0067], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0096], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0277], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0615], device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut-off: tensor([0.1355], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.1689], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2263], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3739], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.3340], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.1412], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([4.1411], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([5.9740], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([508.4951], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6837.2002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Run: 4\n",
      "  epoch       lr    tr_los    tr_acc    te_loss    te_acc    time\n",
      "-------  -------  --------  --------  ---------  --------  ------\n",
      "      1  1.0e-03   1.9e+02      90.6    8.8e+01      95.7    17.7\n",
      "      2  1.0e-03   7.3e+01      96.3    5.7e+01      97.1    19.0\n",
      "      3  1.0e-03   4.8e+01      97.5    5.3e+01      97.2    18.4\n",
      "      4  1.0e-03   3.5e+01      98.2    5.0e+01      97.4    17.1\n",
      "      5  1.0e-03   2.7e+01      98.6    4.8e+01      97.5    19.0\n",
      "      6  1.0e-03   2.1e+01      98.8    4.4e+01      97.8    18.3\n",
      "      7  1.0e-03   1.7e+01      99.1    4.4e+01      97.9    18.3\n",
      "      8  1.0e-03   1.3e+01      99.3    4.8e+01      97.8    18.8\n",
      "      9  1.0e-03   1.1e+01      99.4    4.8e+01      97.9    19.2\n",
      "     10  1.0e-03   9.2e+00      99.5    5.5e+01      97.7    18.6\n",
      "  epoch    kl       lr    tr_los    tr_acc    te_loss    te_acc    sp_0    sp_1    sp_2    time\n",
      "-------  ----  -------  --------  --------  ---------  --------  ------  ------  ------  ------\n",
      "      1     1  1.0e-03   8.8e+01      98.5   -6.9e+02      97.9   0.889   0.827     0.3    35.2\n",
      "      2     1  1.0e-03  -8.2e+02      98.2   -9.0e+02      97.8   0.898   0.846     0.4    36.4\n",
      "      3     1  1.0e-03  -9.6e+02      98.2   -9.9e+02      97.8   0.902   0.856     0.4    35.4\n",
      "      4     1  1.0e-03  -1.0e+03      98.1   -1.0e+03      97.7   0.908   0.869     0.4    34.6\n",
      "      5     1  1.0e-03  -1.1e+03      98.0   -1.1e+03      97.6   0.909   0.855     0.5    36.3\n",
      "      6     1  1.0e-03  -1.1e+03      97.7   -1.1e+03      97.7   0.912   0.864     0.5    27.3\n",
      "      7     1  1.0e-03  -1.1e+03      97.7   -1.1e+03      97.5   0.907   0.862     0.5    24.7\n",
      "      8     1  1.0e-03  -1.1e+03      97.6   -1.1e+03      97.8   0.917   0.896     0.5    24.2\n",
      "      9     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.8   0.906   0.888     0.5    24.8\n",
      "     10     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.5   0.912   0.893     0.5    25.7\n",
      "     11     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.8   0.914   0.911     0.5    24.9\n",
      "     12     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.8   0.913   0.907     0.5    24.3\n",
      "     13     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      97.8   0.916   0.903     0.5    25.3\n",
      "     14     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      98.0   0.914   0.913     0.6    27.0\n",
      "     15     1  1.0e-03  -1.1e+03      97.4   -1.1e+03      97.9   0.916   0.913     0.5    27.1\n",
      "     16     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.7   0.917   0.911     0.6    25.8\n",
      "     17     1  1.0e-03  -1.1e+03      97.5   -1.1e+03      97.9   0.919   0.917     0.6    26.4\n",
      "     18     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      98.0   0.918   0.914     0.6    26.6\n",
      "     19     1  1.0e-03  -1.1e+03      97.5   -1.2e+03      97.9   0.916   0.915     0.6    21.7\n",
      "     20     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      97.8   0.917   0.914     0.6    16.3\n",
      "     21     1  1.0e-03  -1.1e+03      97.6   -1.2e+03      97.9   0.917   0.921     0.6    16.0\n",
      "     22     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      98.0   0.919   0.922     0.6    16.6\n",
      "     23     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      98.0   0.926   0.928     0.6    16.8\n",
      "     24     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      97.8   0.920   0.920     0.6    14.3\n",
      "     25     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      98.0   0.919   0.921     0.6    14.8\n",
      "     26     1  1.0e-03  -1.2e+03      97.5   -1.2e+03      98.0   0.922   0.927     0.6    15.3\n",
      "     27     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.1   0.921   0.927     0.7    14.2\n",
      "     28     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.926   0.929     0.6    14.3\n",
      "     29     1  1.0e-03  -1.2e+03      97.6   -1.2e+03      98.0   0.923   0.932     0.6    15.0\n",
      "     30     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.926   0.931     0.7    14.6\n",
      "     31     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.924   0.930     0.7    14.5\n",
      "     32     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.921   0.928     0.7    14.8\n",
      "     33     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.928   0.935     0.7    14.7\n",
      "     34     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.927   0.932     0.7    14.1\n",
      "     35     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.925   0.933     0.7    14.6\n",
      "     36     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.924   0.936     0.7    14.8\n",
      "     37     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.929   0.938     0.7    14.8\n",
      "     38     1  1.0e-03  -1.2e+03      97.7   -1.2e+03      98.0   0.927   0.937     0.7    14.0\n",
      "     39     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.0   0.928   0.938     0.7    14.7\n",
      "     40     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.932   0.940     0.7    14.4\n",
      "     41     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.931   0.940     0.7    13.6\n",
      "     42     1  1.0e-03  -1.2e+03      97.8   -1.2e+03      98.1   0.931   0.941     0.7    14.4\n",
      "     43     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.1   0.927   0.940     0.7    14.7\n",
      "     44     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      97.7   0.929   0.940     0.7    15.0\n",
      "     45     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      97.9   0.934   0.944     0.7    14.5\n",
      "     46     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.931   0.941     0.7    14.3\n",
      "     47     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.0   0.931   0.938     0.7    14.9\n",
      "     48     1  1.0e-03  -1.2e+03      97.9   -1.2e+03      98.2   0.933   0.945     0.7    14.5\n",
      "     49     1  1.0e-03  -1.2e+03      98.0   -1.2e+03      98.0   0.934   0.946     0.7    14.2\n",
      "     50     1  2.0e-04  -1.2e+03      98.2   -1.2e+03      98.1   0.958   0.965     0.8    15.3\n",
      "     51     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.1   0.964   0.970     0.8    14.9\n",
      "     52     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.0   0.965   0.971     0.8    15.0\n",
      "     53     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.1   0.965   0.972     0.8    14.0\n",
      "     54     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.1   0.966   0.972     0.8    15.0\n",
      "     55     1  2.0e-04  -1.2e+03      98.2   -1.2e+03      98.1   0.966   0.971     0.8    14.8\n",
      "     56     1  2.0e-04  -1.2e+03      98.4   -1.2e+03      98.1   0.966   0.973     0.8    14.2\n",
      "     57     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.1   0.966   0.972     0.8    14.5\n",
      "     58     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.2   0.966   0.972     0.8    14.7\n",
      "     59     1  2.0e-04  -1.2e+03      98.3   -1.2e+03      98.1   0.966   0.972     0.8    14.7\n",
      "     60     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.1   0.968   0.974     0.8    14.9\n",
      "     61     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.1   0.969   0.975     0.8    15.6\n",
      "     62     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.1   0.970   0.976     0.8    15.0\n",
      "     63     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.1   0.970   0.976     0.8    15.3\n",
      "     64     1  4.0e-05  -1.2e+03      98.5   -1.2e+03      98.1   0.970   0.976     0.8    14.3\n",
      "     65     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.1   0.970   0.976     0.8    14.5\n",
      "     66     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.1   0.970   0.976     0.8    10.2\n",
      "     67     1  4.0e-05  -1.2e+03      98.3   -1.2e+03      98.1   0.970   0.976     0.8     9.8\n",
      "     68     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8     9.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     69     1  4.0e-05  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8     9.9\n",
      "     70     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    10.0\n",
      "     71     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.7\n",
      "     72     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.976     0.8    10.0\n",
      "     73     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.8\n",
      "     74     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.7\n",
      "     75     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.6\n",
      "     76     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.1\n",
      "     77     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.7\n",
      "     78     1  8.0e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.7\n",
      "     79     1  8.0e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.977     0.8     9.3\n",
      "     80     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.4\n",
      "     81     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.5\n",
      "     82     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.1\n",
      "     83     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.4\n",
      "     84     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.2\n",
      "     85     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.977     0.8     9.7\n",
      "     86     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.6\n",
      "     87     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.4\n",
      "     88     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.3\n",
      "     89     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.4\n",
      "     90     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.3\n",
      "     91     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.977     0.8     9.6\n",
      "     92     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.4\n",
      "     93     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.4\n",
      "     94     1  1.6e-06  -1.2e+03      98.5   -1.2e+03      98.1   0.971   0.977     0.8     9.2\n",
      "     95     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.3\n",
      "     96     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.6\n",
      "     97     1  1.6e-06  -1.2e+03      98.3   -1.2e+03      98.1   0.971   0.977     0.8     9.4\n",
      "     98     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.2\n",
      "     99     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.2\n",
      "    100     1  1.6e-06  -1.2e+03      98.4   -1.2e+03      98.1   0.971   0.977     0.8     9.4\n",
      "kept weight ratio = 34.489937543372655\n",
      "cut-off: tensor([7.3572e-10], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6.1047e-06], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1706e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.9921e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([3.9593e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6.3145e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([8.5823e-05], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0001], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0002], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0003], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0004], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0007], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0011], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0022], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0062], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0085], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0136], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0337], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2351], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3477], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.5413], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.7292], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1141], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([2.8232], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.1802e-07], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0007], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0015], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0022], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0028], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0034], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0040], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0045], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0050], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0055], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0058], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0061], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0063], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0065], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0066], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0067], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0077], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0234], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.0574], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.1302], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.1611], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.2149], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([0.3620], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.3009], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([1.9795], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([3.9323], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([5.9008], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([250.4341], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cut-off: tensor([6311.8262], device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "epochs_ml = 10\n",
    "epochs = 100\n",
    "reps = 5\n",
    "xs = np.append(0.05 * np.array(range(20)), [0.96, 0.97, 0.98, 0.99, 0.992, 0.995, 0.997, 0.999, 1.0])\n",
    "ya = np.zeros((reps, len(xs)))\n",
    "yb = np.zeros((reps, len(xs)))\n",
    "for i in range(reps):\n",
    "    print(\"Run: {}\".format(i))\n",
    "    ###################\n",
    "    ## Init ML model ##\n",
    "    ###################\n",
    "    \n",
    "    init_model = NormalNet().cuda()\n",
    "    optimizer = optim.Adam(init_model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,60,70,80], gamma=0.2)\n",
    "    train_loader, test_loader = get_mnist(batch_size=100)\n",
    "    sgvlb = SGVLB(init_model, len(train_loader.dataset)).cuda()\n",
    "    fmt = {'tr_los': '3.1e', 'te_loss': '3.1e', 'sp_0': '.3f', 'sp_1': '.3f', 'lr': '3.1e', 'kl': '.2f'}\n",
    "    logger = Logger('init_ml', fmt=fmt)\n",
    "    \n",
    "    #################\n",
    "    ## Train model ##\n",
    "    #################\n",
    "    for epoch in range(1, epochs_ml + 1):\n",
    "        start = time.time()\n",
    "        scheduler.step()\n",
    "        init_model.train()\n",
    "        train_loss, train_acc = 0, 0 \n",
    "        logger.add_scalar(epoch, 'lr', scheduler.get_lr()[0])\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            data = data.view(-1, 28*28)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = init_model(data)\n",
    "            pred = output.data.max(1)[1] \n",
    "            loss = sgvlb(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss \n",
    "            train_acc += np.sum(pred.cpu().numpy() == target.cpu().data.numpy())\n",
    "\n",
    "        logger.add_scalar(epoch, 'tr_los', train_loss / len(train_loader.dataset))\n",
    "        logger.add_scalar(epoch, 'tr_acc', train_acc / len(train_loader.dataset) * 100)\n",
    "\n",
    "\n",
    "        init_model.eval()\n",
    "        test_loss, test_acc = 0, 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            data = data.view(-1, 28*28)\n",
    "            output = init_model(data)\n",
    "            test_loss += float(sgvlb(output, target))\n",
    "            pred = output.data.max(1)[1] \n",
    "            test_acc += np.sum(pred.cpu().numpy() == target.cpu().data.numpy())\n",
    "\n",
    "        logger.add_scalar(epoch, 'te_loss', test_loss / len(test_loader.dataset))\n",
    "        logger.add_scalar(epoch, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n",
    "\n",
    "        for j, c in enumerate(init_model.children()):\n",
    "            if hasattr(c, 'kl_reg'):\n",
    "                logger.add_scalar(epoch, 'sp_%s' % j, (c.log_alpha.cpu().data.numpy() > init_model.threshold).mean())\n",
    "\n",
    "        end = time.time()  \n",
    "        logger.add_scalar(epoch, 'time', end - start)\n",
    "\n",
    "        logger.iter_info()\n",
    "        \n",
    "    ##############\n",
    "    ## Init SVD ##\n",
    "    ##############\n",
    "    # get weights\n",
    "    weights = []\n",
    "    for c in init_model.children():\n",
    "        weights.append(c.weight.data)\n",
    "        #model.fc2.weight.data\n",
    "    \n",
    "    kl_weight = 1.0\n",
    "    model = Net(threshold=3).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,60,70,80], gamma=0.2)\n",
    "    \n",
    "    # initialise with ML weights\n",
    "    for j, c in enumerate(model.children()):\n",
    "        c.reset_weights(weights[j], None, zeros=False, weights_only=True)\n",
    "\n",
    "    fmt = {'tr_los': '3.1e', 'te_loss': '3.1e', 'sp_0': '.3f', 'sp_1': '.3f', 'lr': '3.1e', 'kl': '.2f'}\n",
    "    logger = Logger('sparse_vd', fmt=fmt)\n",
    "\n",
    "    train_loader, test_loader = get_mnist(batch_size=100)\n",
    "    sgvlb = SGVLB(model, len(train_loader.dataset)).cuda()\n",
    "    #################\n",
    "    ## Train model ##\n",
    "    #################\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start = time.time()\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0, 0 \n",
    "        kl_weight = min(kl_weight+0.02, 1)\n",
    "        logger.add_scalar(epoch, 'kl', kl_weight)\n",
    "        logger.add_scalar(epoch, 'lr', scheduler.get_lr()[0])\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            data = data.view(-1, 28*28)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1)[1] \n",
    "            loss = sgvlb(output, target, kl_weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss \n",
    "            train_acc += np.sum(pred.cpu().numpy() == target.cpu().data.numpy())\n",
    "\n",
    "        logger.add_scalar(epoch, 'tr_los', train_loss / len(train_loader.dataset))\n",
    "        logger.add_scalar(epoch, 'tr_acc', train_acc / len(train_loader.dataset) * 100)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        test_loss, test_acc = 0, 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            data = data.view(-1, 28*28)\n",
    "            output = model(data)\n",
    "            test_loss += float(sgvlb(output, target, kl_weight))\n",
    "            pred = output.data.max(1)[1] \n",
    "            test_acc += np.sum(pred.cpu().numpy() == target.cpu().data.numpy())\n",
    "\n",
    "        logger.add_scalar(epoch, 'te_loss', test_loss / len(test_loader.dataset))\n",
    "        logger.add_scalar(epoch, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n",
    "\n",
    "        for j, c in enumerate(model.children()):\n",
    "            if hasattr(c, 'kl_reg'):\n",
    "                logger.add_scalar(epoch, 'sp_%s' % j, (c.log_alpha.cpu().data.numpy() > model.threshold).mean())\n",
    "\n",
    "        end = time.time()  \n",
    "        logger.add_scalar(epoch, 'time', end - start)\n",
    "\n",
    "        logger.iter_info()\n",
    "        \n",
    "    all_w, kep_w = 0, 0\n",
    "\n",
    "    for c in model.children():\n",
    "        kep_w += (c.log_alpha.cpu().data.numpy() < model.threshold).sum()\n",
    "        all_w += c.log_alpha.cpu().data.numpy().size\n",
    "\n",
    "    print('kept weight ratio =', all_w/kep_w)\n",
    "\n",
    "    #############\n",
    "    ## Pruning ##\n",
    "    #############\n",
    "    weights = []\n",
    "    log_vars = []\n",
    "    for c in model.children():\n",
    "        weights.append(c.W.clone())\n",
    "        log_vars.append(c.log_sigma.clone())\n",
    "\n",
    "    for j, threshold in enumerate(xs):\n",
    "        ya[i, j] = pruning(threshold, weights, log_vars, test_loader, baseline=True)\n",
    "\n",
    "    for j, c in enumerate(model.children()):\n",
    "        c.reset_weights(weights[j], log_vars[j], zeros=False)\n",
    "\n",
    "    for j, threshold in enumerate(xs):\n",
    "        yb[i, j] = pruning(threshold, weights, log_vars, test_loader, baseline=False)\n",
    "\n",
    "    for j, c in enumerate(model.children()):\n",
    "        c.reset_weights(weights[j], log_vars[j], zeros=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813,\n",
       "        0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813,\n",
       "        0.9813, 0.9813, 0.9813, 0.9811, 0.9812, 0.9814, 0.9806, 0.9653,\n",
       "        0.8835, 0.3671, 0.1003, 0.0974, 0.0974],\n",
       "       [0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825,\n",
       "        0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825,\n",
       "        0.9825, 0.9825, 0.9825, 0.9825, 0.9823, 0.9819, 0.982 , 0.9714,\n",
       "        0.9579, 0.5059, 0.1819, 0.0974, 0.0974],\n",
       "       [0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823,\n",
       "        0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823,\n",
       "        0.9823, 0.9823, 0.9823, 0.9822, 0.9819, 0.9811, 0.9806, 0.9705,\n",
       "        0.9414, 0.6047, 0.1406, 0.0974, 0.0974],\n",
       "       [0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828,\n",
       "        0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828,\n",
       "        0.9828, 0.9828, 0.9828, 0.9826, 0.9827, 0.983 , 0.9814, 0.9699,\n",
       "        0.935 , 0.4506, 0.1684, 0.1295, 0.0974],\n",
       "       [0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 ,\n",
       "        0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 ,\n",
       "        0.981 , 0.981 , 0.981 , 0.9809, 0.9807, 0.9805, 0.9802, 0.9689,\n",
       "        0.9252, 0.4324, 0.233 , 0.124 , 0.0974]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813,\n",
       "        0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813,\n",
       "        0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9813, 0.9763,\n",
       "        0.9645, 0.8219, 0.4088, 0.0974, 0.0974],\n",
       "       [0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825,\n",
       "        0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825,\n",
       "        0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.9825, 0.982 , 0.9766,\n",
       "        0.9667, 0.8144, 0.3791, 0.1016, 0.0974],\n",
       "       [0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823,\n",
       "        0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823,\n",
       "        0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9823, 0.9813, 0.9764,\n",
       "        0.9665, 0.792 , 0.4874, 0.0974, 0.0974],\n",
       "       [0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828,\n",
       "        0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9828,\n",
       "        0.9828, 0.9828, 0.9828, 0.9828, 0.9828, 0.9825, 0.9823, 0.9755,\n",
       "        0.968 , 0.7873, 0.2814, 0.0982, 0.0974],\n",
       "       [0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 ,\n",
       "        0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 ,\n",
       "        0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.981 , 0.9802, 0.9743,\n",
       "        0.9665, 0.8536, 0.4836, 0.1569, 0.0974]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "_ya_mean = np.mean(ya, axis=0)\n",
    "_ya_std = np.std(ya, axis=0)\n",
    "_yb_mean = np.mean(yb, axis=0)\n",
    "_yb_std = np.std(yb, axis=0)\n",
    "\n",
    "def test_error(ls):\n",
    "    return 100*(1-np.array(ls))\n",
    "\n",
    "fig_size = (6, 5)\n",
    "\n",
    "set_y_axis = False\n",
    "\n",
    "lw = 2\n",
    "\n",
    "grid_color = '0.1'\n",
    "grid_lw = 0.2\n",
    "\n",
    "title_size = 16\n",
    "label_size = 22\n",
    "tick_size = 20\n",
    "legend_size = 22\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "\n",
    "ax.plot(xs, _ya_mean, linewidth=lw, color='r')\n",
    "ax.fill_between(xs,\n",
    "               [x - y for x, y in zip(_ya_mean, _ya_std)],\n",
    "               [x + y for x, y in zip(_ya_mean, _ya_std)],\n",
    "               alpha=0.3, color='r')\n",
    "\n",
    "ax.plot(xs, _yb_mean, linewidth=lw, color='c')\n",
    "ax.fill_between(xs,\n",
    "               [x - y for x, y in zip(_yb_mean, _yb_std)],\n",
    "               [x + y for x, y in zip(_yb_mean, _yb_std)],\n",
    "               alpha=0.3, color='c')\n",
    "\n",
    "plt.grid(True, which=\"both\", color=grid_color, linewidth=0.1, alpha=0.1)\n",
    "ax.set_xlim(0.0, 0.5)\n",
    "plt.yticks(fontsize=tick_size)\n",
    "x_ticks = np.arange(0.0, 1.5, step=0.5)\n",
    "plt.xticks(x_ticks, fontsize=tick_size)\n",
    "plt.ticklabel_format(axis='y', style='sci')\n",
    "ax = fig.gca()\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(-1, 1))\n",
    "#ax.set_yticklabels([])\n",
    "plt.yscale('log')\n",
    "plt.legend(['$|\\mu|$', 'snr'], fontsize=legend_size, loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Cut off\", fontsize=legend_size)\n",
    "plt.ylabel(\"Test acc\", fontsize=legend_size)\n",
    "plt.xlim(0.9, 1.01)\n",
    "#plt.savefig(\"plots/weight_pruning_new_hibp_x5.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('weight_pruning_svd_nokl_annealing.pkl', 'wb') as input_file:\n",
    "    pickle.dump({'xs': xs,\n",
    "                 'ya': ya,\n",
    "                 'yb': yb}, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 16, 4\n",
    "rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "log_alpha = (model.fc1.log_alpha.cpu().detach().numpy() < 3).astype(np.float)\n",
    "W = model.fc1.W.cpu().detach().numpy()\n",
    "\n",
    "# Normalize color map\n",
    "max_val = np.max(np.abs(log_alpha * W))\n",
    "norm = mpl.colors.Normalize(vmin=-max_val,vmax=max_val)\n",
    "\n",
    "plt.imshow(log_alpha * W, cmap='RdBu', interpolation=None, norm=norm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "\n",
    "z = np.zeros((28*15, 28*15))\n",
    "\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        s += 1\n",
    "        z[i*28:(i+1)*28, j*28:(j+1)*28] =  np.abs((log_alpha * W)[s].reshape(28, 28))\n",
    "        \n",
    "plt.imshow(z, cmap='hot_r')\n",
    "plt.colorbar()\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, csc_matrix, coo_matrix, dok_matrix\n",
    "\n",
    "row, col, data = [], [], []\n",
    "M = list(model.children())[0].W.data.numpy()\n",
    "LA = list(model.children())[0].log_alpha.data.numpy()\n",
    "\n",
    "for i in range(300):\n",
    "    for j in range(28*28):\n",
    "        if LA[i, j] < 3:\n",
    "            row += [i]\n",
    "            col += [j]\n",
    "            data += [M[i, j]]\n",
    "\n",
    "Mcsr = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcsc = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcoo = coo_matrix((data, (row, col)), shape=(300, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('M_w', M)\n",
    "scipy.sparse.save_npz('Mcsr_w', Mcsr)\n",
    "scipy.sparse.save_npz('Mcsc_w', Mcsc)\n",
    "scipy.sparse.save_npz('Mcoo_w', Mcoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cl_baselines]",
   "language": "python",
   "name": "conda-env-cl_baselines-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
